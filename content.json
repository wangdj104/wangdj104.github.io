[{"title":"创建SSL证书","date":"2023-05-31T09:03:40.000Z","path":"2023/05/31/自签nginx的https访问证书/","text":"主要介绍如何自签https访问需要的ssl证书 环境准备 openssl-utils centos7 创建SSL证书 TLS &#x2F; SSL通过使用公共证书和私钥的组合来工作。SSL密钥在服务器上保密。它用于加密发送给客户端的内容。SSL证书与请求内容的任何人公开共享。它可用于解密由关联的SSL密钥签名的内容。 我们可以在一个命令中使用OpenSSL创建自签名密钥和证书对： 1`openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /etc/ssl/private/nginx-selfsigned.key -out /etc/ssl/certs/nginx-selfsigned.crt` openssl：这是用于创建和管理OpenSSL证书，密钥和其他文件的基本命令行工具。 req：此子命令指定我们要使用X.509证书签名请求（CSR）管理。“X.509”是SSL和TLS为其密钥和证书管理所遵循的公钥基础结构标准。我们想要创建一个新的X.509证书，所以我们使用这个子命令。 -x509：通过告诉实用程序我们要创建自签名证书而不是生成证书签名请求（通常会发生）来进一步修改上一个子命令。 -nodes：这告诉OpenSSL跳过用密码保护我们的证书的选项。当服务器启动时，我们需要Nginx能够在没有用户干预的情况下读取文件。密码短语会阻止这种情况发生，因为我们必须在每次重启后输入密码。 -days 365：此选项设置证书被视为有效的时间长度。我们在这里设置了一年。 -newkey rsa：2048：这指定我们要同时生成新证书和新密钥。我们没有创建在上一步中签署证书所需的密钥，因此我们需要将其与证书一起创建。该rsa:2048部分告诉它制作一个2048位长的RSA密钥。 -keyout：这一行告诉OpenSSL在哪里放置我们正在创建的生成的私钥文件。 -out：这告诉OpenSSL在哪里放置我们正在创建的证书。 如上所述，这些选项将创建密钥文件和证书。我们将询问有关我们服务器的一些问题，以便将信息正确地嵌入到证书中。 配置nginx1` vim /data/nginx_anzhuang/nginx/conf/nginx.conf`","tags":[]},{"title":"JAVA基础","date":"2021-11-05T01:43:21.000Z","path":"2021/11/05/Java基础/","text":"Java中的自增是线程安全的吗，如何实现线程安全的自增 自增会带来线程安全问题吗？为什么？是线程不安全的1.i++在字节码层面分为三步：保存当前值，执行添加操作，更新新值2.多线程操作时，可能会同时获取到旧值（假设为1），添加操作后为2，第一个线程刷新新值为3，第二个刷新还是3。 volatile可以保证线程安全吗？不可以！volatile只能保证可见性，以及顺序性但是不能保证多个线程同时操作 如何保证线程安全？1.增加synchronized进行线程同步2.使用lock、unlock处理Reetrantent 锁进行锁定3.使用JVM封装类AtomicIntegerAtomicInteger &gt;&gt;&gt; Unsafe &gt;&gt;&gt; cas &gt;&gt;&gt; aba 首先说明，此处 AtomicInteger，一个提供原子操作的 Integer 的类，常见的还有AtomicBoolean、AtomicInteger、AtomicLong、AtomicReference 等，他们的实现原理相同，区别在与运算对象类型的不同。令人兴奋地，还可以通过 AtomicReference将一个对象的所有操作转化成原子操作。 我们知道，在多线程程序中，诸如+i 或 i++等运算不具有原子性，是不安全的线程操作之一。通常我们会使用 synchronized 将该操作变成一个原子操作，但 JVM 为此类操作特意提供了一些同步类，使得使用更方便，且使程序运行效率变得更高。通过相关资料显示，通常AtomicInteger 的性能是 ReentantLock 的好几倍。 Jdk1.8中的stream有用过吗，详述一下stream的并行操作原理，stream并行的线程池是从哪里来的Stream作为Java8的一大亮点，它与java.io包里的InputStream和OutputStream是完全不同的概念。它是对容器对象功能的增强，它专注于对容器对象进行各种非常便利、高效的聚合操作或者大批量数据操作。 Stream API借助于同样新出现的Lambda表达式，极大的提高编程效率和程序可读性。同时，它提供串行和并行两种模式进行汇聚操作，并发模式能够充分利用多核处理器的优势，使用fork&#x2F;join并行方式来拆分任务和加速处理过程。所以说，Java8中首次出现的 java.util.stream是一个函数式语言+多核时代综合影响的产物。 Stream有如下三个操作步骤： 一、创建Stream：从一个数据源，如集合、数组中获取流。 二、中间操作：一个操作的中间链，对数据源的数据进行操作。 三、终止操作：一个终止操作，执行中间操作链，并产生结果。 Stream流的入门123456789101112131415161718192021222324public class StreamDemo1 &#123; public static void main(String[]args)&#123; //外部迭代 int [] nums = &#123;1,2,3&#125;; int sum = 0; for (int num : nums) &#123; sum += num; &#125; System.out.println(&quot;结果是:&quot; + sum); //使用Strem进行内部迭代 int sum2 = IntStream.of(nums).map( i -&gt; i *2).sum(); System.out.println(&quot;结果为:&quot; + sum2); System.out.println(&quot;惰性求值是终止没有调用的情况下,中间的操作不会执行&quot;); IntStream.of(nums).map(StreamDemo1::doubNum); &#125; public static int doubNum(int i)&#123; System.out.println(&quot;执行了乘以二&quot;); return i * 2; &#125;&#125; Stream流的创建1234567891011121314151617181920212223public class StremDemo2 &#123; public static void main(String[]args)&#123; List&lt;String&gt; list = new ArrayList&lt;&gt;(); //从集合中创建 list.stream(); list.parallelStream(); //从数组中创建 Arrays.stream(new int [] &#123;1,2,3&#125;); //使用rondom创建无线流 new Random().ints().limit(10); //创建数字流 IntStream.of(1,2,3); IntStream.rangeClosed(1,10); Random r = new Random(); //自己产生流 Stream.generate(()-&gt; r.nextInt()).limit(20).forEach(System.out::println); &#125;&#125; Strem流中常用的几个方法1234567891011121314151617public class StreamDemo3 &#123; public static void main(String[]args)&#123; String str = &quot;i want to be a software enginner&quot;; Stream.of(str.split(&quot; &quot;)).map( s -&gt; s.length()).forEach(System.out::println); Stream.of(str.split(&quot; &quot;)).filter(s -&gt; s.contains(&quot;a&quot;)).map(s -&gt; s.length()).forEach(System.out::println); Stream.of(str.split(&quot; &quot;)).flatMap(s -&gt; str.chars().boxed()).forEach(i -&gt; System.out.println((char)i.intValue())); Stream.of(str.split(&quot; &quot;)).peek(System.out::println).forEach(System.out::println); //limit 使用 ,主要用于无线流 new Random().ints().filter(i -&gt; i &gt; 100 &amp;&amp; i &lt; 1000).limit(10).forEach(System.out::println) ; &#125;&#125;&#125; 流的操作二123456789101112131415161718192021222324252627282930public class StreamDemo4 &#123; public static void main(String[]args)&#123; String str = &quot;hello lambda hello&quot;; //使用并行流 str.chars().parallel().forEach( s -&gt; System.out.println((char)s)); //保证顺序 str.chars().parallel().forEachOrdered(i -&gt; System.out.println((char)i)); //收集器 List&lt;String&gt; list = Stream.of(str.split(&quot; &quot;)).collect(Collectors.toList()); System.out.println(&quot;这是一个list:&quot; + list); //使用reduce拼接字符串 Optional&lt;String&gt; letters = Stream.of(str.split(&quot; &quot;)).reduce((s1,s2) -&gt; s1 + &quot;|&quot; + s2); System.out.println(letters.orElse(&quot;&quot;)); //计算所有单词总长度 Integer lent = Stream.of(str.split(&quot; &quot;)).map(s -&gt; s.length()).reduce(0,(s1,s2) -&gt; s1 + s2); System.out.println(&quot;单词的总长度是:&quot; + lent); //max Optional&lt;String&gt; max = Stream.of(str.split(&quot; &quot;)).max((s1,s2) -&gt; s1.length() - s2.length()); System.out.println(&quot;单词最长的是:&quot; + max.get()); OptionalInt findFirst = new Random().ints().findFirst(); System.out.println(&quot;短路操作&quot; + findFirst.getAsInt()); &#125; 串行流和并行流以及线程池123456789101112131415161718192021222324252627282930313233343536373839404142 public class StreamDemo5 &#123; public static void main(String[]args)&#123; IntStream.range(1,100).parallel().peek(StreamDemo5::debug).count(); //parallel 并行流 IntStream.range(1,100).parallel().peek(StreamDemo5::debug). //sequential串行流 sequential().peek(StreamDemo5::debug2).count(); ForkJoinPool pool = new ForkJoinPool(20); pool.submit(() -&gt; IntStream.range(1,100).parallel().peek(StreamDemo5::debug).count()); pool.shutdown(); synchronized (pool)&#123; try &#123; pool.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static void debug(int i)&#123; System.out.println(&quot;debug&quot; + i); System.out.println(&quot;线程&quot; + Thread.currentThread().getName() + &quot;：&quot; + i); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; public static void debug2(int i)&#123; System.err.println(&quot;debug&quot; + i); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 请聊一下java的集合类,以及在实际项目中你是如何用的参照java集合一章 注意说出集合体系,常用类 接口 实现类 加上你所知道的高并发集合类,JUC 参照集合增强内容 在实际项目中引用, 照实说就好了 问集合的引子… … 集合类型主要有3种：set(集）、list(列表）和map(映射)。 集合接口分为：Collection和Map，list、set实现了Collection接口 List总结： 可以重复，通过索引取出加入数据，顺序与插入顺序一致，可以含有null元素 ArrayList:底层数据结构是数组结构array，查询速度快，增删改慢，因为是一种类似数组的形式进行存储，因此它的随机访问速度极快； Vector：底层是数组结构array，与ArrayList相同，查询速度快，增删改慢； LinkedList:底层使用链表结构，增删速度快，查询稍慢； ArrayList与Vector的区别： 1.如果集合中的元素数量大于当前集合数组的长度时，Vector的增长率是目前数组长度的100%，而ArryaList增长率为目前数组长度的50%。所以，如果集合中使用数据量比较大的数据，用Vector有一定优势 2.线程同步ArrayList是线程不同步，所以Vector线程安全，但是因为每个方法都加上了synchronized，所以在效率上小于ArrayList Set总结：数据无序且唯一,实现类都不是线程安全的类，解决方案：Set set &#x3D; Collections.sysnchronizedSet(Set对象); HashSet：是Set接口（Set接口是继承了Collection接口的）最常用的实现类，顾名思义，底层是用了哈希表（散列&#x2F;hash）算法。其底层其实也是一个数组，存在的意义是提供查询速度，插入的速度也是比较快，但是适用于少量数据的插入操作。 LinkedHashSet：继承了HashSet类，所以它的底层用的也是哈希表的数据结构，但因为保持数据的先后添加顺序，所以又加了链表结构，但因为多加了一种数据结构，所以效率较低，不建议使用，如果要求一个集合急要保证元素不重复，也需要记录元素的先后添加顺序，才选择使用LinkedHashSet TreeSet：Set接口的实现类，也拥有set接口的一般特性，但是不同的是他也实现了SortSet接口，它底层采用的是红黑树算法（红黑树就是满足一下红黑性质的二叉搜索树：①每个节点是黑色或者红色②根节点是黑色的③每个叶子结点是黑色的④如果一个节点是红色的，那么他的两个子节点是黑色的⑤对每个节点，从该节点到其所有的后代叶子结点的简单路径上，仅包含相同数目的黑色结点，红黑树是许多“平衡”搜索树的一种，可以保证在最坏情况下的基本操作集合的时间复杂度为O(lgn)。 Map总结：java的Map(映射)是一种把键对象和值对象进行映射的集合，其中每一个元素都包含了键对象和值对象，其中值对象也可以是Map类型的数据，因此，Map支持多级映射，Map中的键是唯一的，但值可以不唯一，Map集合有两种实现，一种是利用哈希表来完成的叫做HashMap，它和HashSet都是利用哈希表来完成的，区别其实就是在哈希表的每个桶中，HashSet只有key，而HashMap在每个key上挂了一个value；另一种就是TreeMap，它实现了SortMap接口，也就是使用了红黑树的数据结构，和TreeSet一样也能实现自然排序和客户化排序两种排序方式，而哈希表不提供排序。 HashMap：哈希表的实现原理中，先采用一个数组表示位桶，每个位桶的实现在1.8之前都是使用链表，但当每个位桶的数据较多的时候，链表查询的效率就会不高，因此在1.8之后，当位桶的数据超过阈值（8）的时候，就会采用红黑树来存储该位桶的数据（在阈值之前还是使用链表来进行存储），所以，哈希表的实现包括数组+链表+红黑树，在使用哈希表的集合中我们都认为他们的增删改查操作的时间复杂度都是O(1)的，不过常数项很大，因为哈希函数在进行计算的代价比较高,HashMap和Hashtable类似，不同之处在于HashMap是非同步的，并且允许null，即null value和null key。，但是将HashMap视为Collection时（values()方法可返回Collection），其迭代子操作时间开销和HashMap 的容量成比例。因此，如果迭代操作的性能相当重要的话，不要将HashMap的初始化容量设得过高，或者load factor过低。 TreeMap：TreeMap 是一个有序的key-value集合，它是通过红黑树实现的。TreeMap 继承于AbstractMap，所以它是一个Map，即一个key-value集合。TreeMap 实现了NavigableMap接口，意味着它支持一系列的导航方法。比如返回有序的key集合。TreeMap 实现了Cloneable接口，意味着它能被克隆。TreeMap 实现了java.io.Serializable接口，意味着它支持序列化。 TreeMap基于红黑树（Red-Black tree）实现。该映射根据其键的自然顺序进行排序，或者根据创建映射时提供的 Comparator 进行排序，具体取决于使用的构造方法。TreeMap的基本操作 containsKey、get、put 和 remove 的时间复杂度是 log(n) 。另外，TreeMap是非同步的。 它的iterator 方法返回的迭代器是fail-fastl的。 HashTable:Hashtable继承Map接口，实现一个key-value映射的哈希表。任何非空（non-null）的对象都可作为key或者value，线程安全。 Hashmap为什么要使用红黑树？在jdk1.8版本后，java对HashMap做了改进，在链表长度大于8的时候，将后面的数据存在红黑树中，以加快检索速度 红黑树虽然本质上是一棵二叉查找树，但它在二叉查找树的基础上增加了着色和相关的性质使得红黑树相对平衡，从而保证了红黑树的查找、插入、删除的时间复杂度最坏为O(log n)。加快检索速率。 集合类是怎么解决高并发中的问题？思路 先说一下那些是非安全 ​ 普通的安全的集合类 ​ JUC中高并发的集合类 线程非安全的集合类 ArrayList LinkedList HashSet TreeSet HashMap TreeMap 实际开发中我们自己用这样的集合最多,因为一般我们自己写的业务代码中,不太涉及到多线程共享同一个集合的问题 线程安全的集合类 Vector HashTable 虽然效率没有JUC中的高性能集合高,但是也能够适应大部分环境 高性能线程安全的集合类  1.ConcurrentHashMap  2.ConcurrentHashMap和HashTable的区别  3.ConcurrentHashMap线程安全的具体实现方式&#x2F;底层具体实现  4.说说CopyOnWriteArrayList ConcurrentHashMap 在JDK1.7的时候，ConcurrentHashMap（分段锁）对整个桶数组进行了分割分段(Segment)，每一把锁只锁容器其中一部分数据，多线程访问容器里不同数据段的数据，就不会存在锁竞争 JDK1.8 ConcurrentHashMap取消了Segment分段锁，采用CAS和synchronized来保证并发安全。数据结构跟HashMap1.8的结构类似，数组+链表&#x2F;红黑二叉树。Java 8在链表长度超过一定阈值时将链表（寻址时间复杂度为O(N)）转换为红黑树（寻址时间复杂度为O(log(N))） synchronized只锁定当前链表或红黑二叉树的首节点，这样只 要hash不冲突，就不会产生并发，效率又提升N倍。 ConcurrentSkipListMap是线程安全的有序的哈希表(相当于线程安全的TreeMap); 它继承于AbstractMap类，并且实现ConcurrentNavigableMap接口。ConcurrentSkipListMap是通过“跳表”来实现的， ConcurrentSkipListSet是线程安全的有序的集合(相当于线程安全的TreeSet)；它继承于AbstractSet，并实现了NavigableSet接口。ConcurrentSkipListSet是通过ConcurrentSkipListMap实现的，它也支持并发。 CopyOnWriteArraySet addIfAbsent和 CopyOnWriteArrayList（写入并复制）也是juc里面的，它解决了并发修改异常，每当有写入的时候，就在底层重新复制一个新容器写入，最后把新容器的引用地址赋给旧的容器，在别人写入的时候，其他线程读数据，依然是旧容器的线程。这样是开销很大的，所以不适合频繁写入的操作。适合并发迭代操作多的场景。只能保证数据的最终一致性 简述一下自定义异常的应用场景？借助异常机制,我们可以省略很多业务逻辑上的判断处理,直接借助java的异常机制可以简化业务逻辑判断代码的编写 1当你不想把你的错误直接暴露给前端或者你想让前端从业务角度判断后台的异常，这个时候自定义异常类是你的不二选择 2 虽然JAVA给我们提供了丰富的异常类型,但是在实际的业务上,还有很多情况JAVA提供的异常类型不能准确的表述出我们业务上的含义 3 控制项目的后期服务 … … 描述一下Object类中常用的方法Object类中的toString()方法 1.object 默认方法toString方法，toString() 输出一个对象的地址字符串（哈希code码）！ 2.可以通过重写toString方法，获取对象的属性！ Object类中的equals()方法 1.Object类equals()比较的是对象的引用是否指向同一块内存地址！ 2.重写equals()方法比较俩对象的属性值是否相同 Object()默认构造方法 clone()创建并返回此对象的一个副本。 finalize()当垃圾回收器确定不存在对该对象的更多引用时，由对象的垃圾回收器调用此方法。 getClass()返回一个对象的运行时类。 hashCode()返回该对象的哈希码值。 为什么wait notify会放在Object里边？wait(),notify(),notifyAll()用来操作线程为什么定义在Object类中？1、这些方法存在于同步中；2、使用这些方法必须标识同步所属的锁；3、锁可以是任意对象，所以任意对象调用方法一定定义在Object类中。 wait(),sleep()区别？wait():释放资源，释放锁sleep():释放资源，不释放锁 notify()唤醒在此对象监视器上等待的单个线程。 notifyAll()唤醒在此对象监视器上等待的所有线程。 wait()导致当前的线程等待，直到其他线程调用此对象的 notify() 方法或 notifyAll() 方法。 wait(long timeout)导致当前的线程等待，直到其他线程调用此对象的 notify() 方法或 notifyAll() 方法，或者超过指定的时间量。 wait(long timeout, int nanos)导致当前的线程等待，直到其他线程调用此对象的 notify() 方法或 notifyAll() 方法，或者其他某个线程中断当前线程，或者已超过某个实际时间量。 1.8的新特性有了解过吗？ (注意了解其他版本新特征) +JDK更新认识· Lambda表达式 · 函数式接口 函数式编程 · 方法引用和构造器调用 · Stream API · 接口中的默认方法和静态方法 · 新时间日期API 新的日期类 属性 含义 Instant 代表的是时间戳 LocalDate 代表日期，比如2020-01-14 LocalTime 代表时刻，比如12:59:59 LocalDateTime 代表具体时间 2020-01-12 12:22:26 ZonedDateTime 代表一个包含时区的完整的日期时间，偏移量是以UTC&#x2F; 格林威治时间为基准的 Period 代表时间段 ZoneOffset 代表时区偏移量，比如：+8:00 Clock 代表时钟，比如获取目前美国纽约的时间 简述一下Java面向对象的基本特征，继承、封装与多态，以及你自己的应用？知识参照面向对象章节 注意单独解释 继承 封装 多态的概念 继承 基本概念解释 后面多态的条件 封装 基本概念解释 隐藏实现细节,公开使用方式 多态 基本概念解释 就是处理参数 提接口 打破单继承 设计模式 设计原则 Java中重写和重载的区别？联系: 名字相似 都是多个同名方法 重载 在同一个类之中发生的 重写 继承中,子类重写父类方法 1 目的差别 2 语法差别 怎样声明一个类不会被继承，什么场景下会用？final修饰的类不能有子类 大部分都是出于安全考虑 String举例 什么是ForkJoin框架 适用场景虽然目前处理器核心数已经发展到很大数目，但是按任务并发处理并不能完全充分的利用处理器资源，因为一般的应用程序没有那么多的并发处理任务。基于这种现状，考虑把一个任务拆分成多个单元，每个单元分别得到执行，最后合并每个单元的结果。 Fork&#x2F;Join框架是JAVA7提供的一个用于并行执行任务的框架，是一个把大任务分割成若干小任务，最终汇总每个小任务结果得到大任务结果的框架。 2. 工作窃取算法（work-stealing） 一个大任务拆分成多个小任务，为了减少线程间的竞争，把这些子任务分别放到不同的队列中，并且每个队列都有单独的线程来执行队列里的任务，线程和队列一一对应。 但是会出现这样一种情况：A线程处理完了自己队列的任务，B线程的队列里还有很多任务要处理。 A是一个很热情的线程，想过去帮忙，但是如果两个线程访问同一个队列，会产生竞争，所以A想了一个办法，从双端队列的尾部拿任务执行。而B线程永远是从双端队列的头部拿任务执行。 注意：线程池中的每个线程都有自己的工作队列（PS，这一点和ThreadPoolExecutor不同，ThreadPoolExecutor是所有线程公用一个工作队列，所有线程都从这个工作队列中取任务），当自己队列中的任务都完成以后，会从其它线程的工作队列中偷一个任务执行，这样可以充分利用资源。 工作窃取算法的优点： ​ 利用了线程进行并行计算，减少了线程间的竞争。 工作窃取算法的缺点： ​ 任务争夺问题 Java种的代理有几种实现方式？动态代理 JDK &gt;&gt;&gt; Proxy ​ 1 面向接口的动态代理 代理一个对象去增强面向某个接口中定义的方法 ​ 2 没有接口不可用 ​ 3 只能读取到接口上的一些注解 MyBatis DeptMapper dm&#x3D;sqlSession.getMapper(DeptMapper.class) 第三方 CGlib ​ 1 面向父类的动态代理 ​ 2 有没有接口都可以使用 ​ 3 可以读取类上的注解 ​ AOP 日志 性能检测 事务 MyBatis 源码 spring源码 happens-before规则​ 先行发生原则（Happens-Before）是判断数据是否存在竞争、线程是否安全的主要依据。​ 先行发生是Java内存，模型中定义的两项操作之间的偏序关系，如果操作A先行发生于操作B，那么操作 A产生的影响能够被操作B观察到。口诀：如果两个操作之间具有happen-before关系，那么前一个操作的结果就会对后面的一个操作可见。是Java内存模型中定义的两个操作之间的偏序关系。 常见的happen-before规则：1.程序顺序规则：一个线程中的每个操作，happen-before在该线程中的任意后续操作。(注解：如果只有一个线程的操作，那么前一个操作的结果肯定会对后续的操作可见。)程序顺序规则中所说的每个操作happen-before于该线程中的任意后续操作并不是说前一个操作必须要在后一个操作之前执行，而是指前一个操作的执行结果必须对后一个操作可见，如果不满足这个要求那就不允许这两个操作进行重排序 2.锁规则：对一个锁的解锁，happen-before在随后对这个锁加锁。(注解：这个最常见的就是synchronized方法和syncronized块) 3.volatile变量规则：对一个volatile域的写，happen-before在任意后续对这个volatile域的读。该规则在CurrentHashMap的读操作中不需要加锁有很好的体现。 4.传递性：如果A happen-before B，且B happen-before C，那么A happen - before C. 5.线程启动规则：Thread对象的start()方法happen-before此线程的每一个动作。 6.线程终止规则：线程的所有操作都happen-before对此线程的终止检测，可以通过Thread.join()方法结束，Thread.isAlive()的返回值等手段检测到线程已经终止执行。 7.线程中断规则：对线程interrupt()方法的调用happen-before发生于被中断线程的代码检测到中断时事件的发生。 jvm监控系统是通过jmx做的么？​ 一般都是，但是要是记录比较详细的性能定位指标，都会导致进入 safepoint，从而降低了线上应用性能​ 例如 jstack，jmap打印堆栈，打印内存使用情况，都会让 jvm 进入safepoint，才能获取线程稳定状态从而采集信息。​ 同时，JMX暴露向外的接口采集信息，例如使用jvisualvm，还会涉及rpc和网络消耗，以及JVM忙时，无法采集到信息从而有指标断点。这些都是基于 JMX 的外部监控很难解决的问题。​ 所以，推荐使用JVM内部采集 JFR，这样即使在JVM很忙时，也能采集到有用的信息 jvm有哪些垃圾回收器 图中展示了7种作用于不同分代的收集器，如果两个收集器之间存在连线，则说明它们可以搭配使用。虚拟机所处的区域则表示它是属于新生代还是老年代收集器。新生代收集器（全部的都是复制算法）：Serial、ParNew、Parallel Scavenge 老年代收集器：CMS（标记-清理）、Serial Old（标记-整理）、Parallel Old（标记整理） 整堆收集器： G1（一个Region中是标记-清除算法，2个Region之间是复制算法）同时，先解释几个名词： 1，并行（Parallel）：多个垃圾收集线程并行工作，此时用户线程处于等待状态2，并发（Concurrent）：用户线程和垃圾收集线程同时执行3，吞吐量：运行用户代码时间／（运行用户代码时间＋垃圾回收时间）1.Serial收集器是最基本的、发展历史最悠久的收集器。特点：单线程、简单高效（与其他收集器的单线程相比），对于限定单个CPU的环境来说，Serial收集器由于没有线程交互的开销，专心做垃圾收集自然可以获得最高的单线程手机效率。收集器进行垃圾回收时，必须暂停其他所有的工作线程，直到它结束（Stop The World）。应用场景：适用于Client模式下的虚拟机。 Serial &#x2F; Serial Old收集器运行示意图 2.ParNew收集器其实就是Serial收集器的多线程版本。除了使用多线程外其余行为均和Serial收集器一模一样（参数控制、收集算法、Stop The World、对象分配规则、回收策略等）。 特点：多线程、ParNew收集器默认开启的收集线程数与CPU的数量相同，在CPU非常多的环境中，可以使用-XX:ParallelGCThreads参数来限制垃圾收集的线程数。 和Serial收集器一样存在Stop The World问题应用场景：ParNew收集器是许多运行在Server模式下的虚拟机中首选的新生代收集器，因为它是除了 Serial收集器外，唯一一个能与CMS收集器配合工作的。ParNew&#x2F;Serial Old组合收集器运行示意图如下： 3.Parallel Scavenge 收集器与吞吐量关系密切，故也称为吞吐量优先收集器。特点：属于新生代收集器也是采用复制算法的收集器，又是并行的多线程收集器（与ParNew收集器类似）。 该收集器的目标是达到一个可控制的吞吐量。还有一个值得关注的点是：GC自适应调节策略（与ParNew收集器最重要的一个区别） GC自适应调节策略：Parallel Scavenge收集器可设置-XX:+UseAdptiveSizePolicy参数。当开关打开时不需要手动指定新生代的大小（-Xmn）、Eden与Survivor区的比例（-XX:SurvivorRation）、晋升老年代的对象年龄（-XX:PretenureSizeThreshold）等，虚拟机会根据系统的运行状况收集性能监控信息，动态设置这些参数以提供最优的停顿时间和最高的吞吐量，这种调节方式称为GC的自适应调节策略。 Parallel Scavenge收集器使用两个参数控制吞吐量：​ XX:MaxGCPauseMillis 控制最大的垃圾收集停顿时间​ XX:GCRatio 直接设置吞吐量的大小。 4.Serial Old是Serial收集器的老年代版本。特点：同样是单线程收集器，采用标记-整理算法。应用场景：主要也是使用在Client模式下的虚拟机中。也可在Server模式下使用。Server模式下主要的两大用途（在后续中详细讲解···）： 在JDK1.5以及以前的版本中与Parallel Scavenge收集器搭配使用。 作为CMS收集器的后备方案，在并发收集Concurent Mode Failure时使用。Serial &#x2F; Serial Old收集器工作过程图（Serial收集器图示相同）： 5.Parallel Old是Parallel Scavenge收集器的老年代版本。特点：多线程，采用标记-整理算法。 应用场景：注重高吞吐量以及CPU资源敏感的场合，都可以优先考虑Parallel Scavenge+Parallel Old 收集器。 Parallel Scavenge&#x2F;Parallel Old收集器工作过程图： 6.CMS收集器是一种以获取最短回收停顿时间为目标的收集器。特点：基于标记-清除算法实现。并发收集、低停顿。应用场景：适用于注重服务的响应速度，希望系统停顿时间最短，给用户带来更好的体验等场景下。如 web程序、b&#x2F;s服务。CMS收集器的运行过程分为下列4步： 初始标记：标记GC Roots能直接到的对象。速度很快但是仍存在Stop The World问题。 并发标记：进行GC Roots Tracing 的过程，找出存活对象且用户线程可并发执行。 重新标记：为了修正并发标记期间因用户程序继续运行而导致标记产生变动的那一部分对象的标记记录。仍然存在Stop The World问题。并发清除：对标记的对象进行清除回收。CMS收集器的内存回收过程是与用户线程一起并发执行的。 CMS收集器的工作过程图： CMS收集器的缺点：​ 对CPU资源非常敏感。 ​ 无法处理浮动垃圾，可能出现Concurrent Model Failure失败而导致另一次Full GC的产生。​ 因为采用标记-清除算法所以会存在空间碎片的问题，导致大对象无法分配空间，不得不提前触发一次Full GC。 ​ 7.G1收集器一款面向服务端应用的垃圾收集器。特点如下： 并行与并发： ​ G1能充分利用多CPU、多核环境下的硬件优势，使用多个CPU来缩短Stop-The-World停顿时间。部分收集器原本需要停顿Java线程来执行GC动作，G1收集器仍然可以通过并发的方式让Java程序继续运行。 ​ 分代收集： ​ G1能够独自管理整个Java堆，并且采用不同的方式去处理新创建的对象和已经存活了一段时间、熬过多次GC的旧对象以获取更好的收集效果 ​ 空间整合： ​ G1运作期间不会产生空间碎片，收集后能提供规整的可用内存。 ​ 可预测的停顿： G1除了追求低停顿外，还能建立可预测的停顿时间模型。能让使用者明确指定在一个长 度为M毫秒的时间段内，消耗在垃圾收集上的时间不得超过N毫秒。 G1收集器运行示意图： ​ 关于gc的选择​ 除非应用程序有非常严格的暂停时间要求，否则请先运行应用程序并允许VM选择收集器（如果没有特别要求。使用VM提供给的默认GC就好）。​ 如有必要，调整堆大小以提高性能。 如果性能仍然不能满足目标，请使用以下准则作为选择收集器的起点： ​ 1234567891011121314 如果应用程序的数据集较小（最大约100 MB），则选择带有选项-XX：+ UseSerialGC的串行收集器。 如果应用程序将在单个处理器上运行，并且没有暂停时间要求，则选择带有选项-XX：+UseSerialGC的串行收集器。 如果（a）峰值应用程序性能是第一要务，并且（b）没有暂停时间要求或可接受一秒或更长时间的暂停，则让VM选择收集器或使用-XX：+ UseParallelGC选择并行收集器 。 如果响应时间比整体吞吐量更重要，并且垃圾收集暂停时间必须保持在大约一秒钟以内，则选择具有-XX：+ UseG1GC。（值得注意的是JDK9中CMS已经被Deprecated，不可使用！移除该选项） 如果使用的是jdk8，并且堆内存达到了16G，那么推荐使用G1收集器，来控制每次垃圾收集的时间。 如果响应时间是高优先级，或使用的堆非常大，请使用-XX：UseZGC选择完全并发的收集器。（值得注意的是JDK11开始可以启动ZGC，但是此时ZGC具有实验性质，在JDK15中[202009发布]才取消实验性质的标签，可以直接显示启用，但是JDK15默认GC仍然是G1） ​ 这些准则仅提供选择收集器的起点，因为性能取决于堆的大小，应用程序维护的实时数据量以及可用处理器的数量和速度。​ 如果推荐的收集器没有达到所需的性能，则首先尝试调整堆和新生代大小以达到所需的目标。 如果性能仍然不足，尝试使用其他收集器​ 总体原则：减少STOP THE WORD时间，使用并发收集器（比如CMS+ParNew，G1）来减少暂停时间，加快响应时间，并使用并行收集器来增加多处理器硬件上的总体吞吐量。 pc计数器​ pc计数器是一种指针，也叫bcp（bytecode pointer）字节码指针 ，当栈帧成立，读取字节码文件时，你需要知道它读取到哪里了，pc计数器就有着这么一个作用，用来标记读取字节码的当前位置。 介绍一下垃圾回收算法老年代和新生代内存溢出的原因内存溢出的原因java.lang.OutOfMemoryError: ……java heap space….. 堆栈溢出，代码问题的可能性极大java.lang.OutOfMemoryError: GC over head limit exceeded 系统处于高频的GC状态，而且回收的效果依然不佳的情况，就会开始报这个错误，这种情况一般是产生了很多不可以被释放的对象，有可能是引用使用不当导致，或申请大对象导致，但是java heap space的内存溢出有可能提前不会报这个错误，也就是可能内存就直接不够导致，而不是高频GC.java.lang.OutOfMemoryError: PermGen space jdk1.7之前才会出现的问题 ，原因是系统的代码非常多或引用的第三方包非常多、或代码中使用了大量的常量、或通过intern注入常量、或者通过动态代码加载等方法，导致常量池的膨胀java.lang.OutOfMemoryError: Direct buffer memory 直接内存不足，因为jvm垃圾回收不会回收掉直接内存这部分的内存，所以可能原因是直接或间接使用了ByteBuffer中的allocateDirect方法的时候，而没有做clearjava.lang.StackOverflowError - Xss设置的太小了java.lang.OutOfMemoryError: unable to create new native thread 堆外内存不足，无法为线程分配内存区域java.lang.OutOfMemoryError: request {} byte for {}out of swap 地址空间不够 线上Gc频繁 查看监控，以了解出现问题的时间点以及当前FGC的频率（可对比正常情况看频率是否正常） 了解该时间点之前有没有程序上线、基础组件升级等情况。 了解JVM的参数设置，包括：堆空间各个区域的大小设置，新生代和老年代分别采用了哪些垃圾收集器，然后分析JVM参数设置是否合理。 再对步骤1中列出的可能原因做排除法，其中元空间被打满、内存泄漏、代码显式调用gc方法比较容易排查。 针对大对象或者长生命周期对象导致的FGC，可通过 jmap -histo 命令并结合dump堆内存文件作进一步分析，需要先定位到可疑对象。 通过可疑对象定位到具体代码再次分析，这时候要结合GC原理和JVM参数设置，弄清楚可疑","tags":[{"name":"gitlab","slug":"gitlab","permalink":"https://wangdj104.github.io/tags/gitlab/"}]},{"title":"rpm包安装Idap","date":"2019-12-10T02:49:59.000Z","path":"2019/12/10/rpm安装ldap/","text":"安装openldaprpm包安装下载链接: https://pan.baidu.com/s/1bWPcquCyqA6iFgktPqE2fQ 提取码: 4834 1rpm -ivh *.rpm --nodeps --force 设置DB Cache123cp /usr/share/openldap-servers/DB_CONFIG.example /var/lib/ldap/DB_CONFIGsudo chown -R ldap.ldap /var/lib/ldap/sudo chown -R ldap.ldap /etc/openldap/slapd.d 创建配置文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677vim /etc/openldap/slapd.conf\\###### SAMPLE 1 - SIMPLE DIRECTORY ############\\#\\# NOTES: inetorgperson picks up attributes and objectclasses\\# from all three schemas\\#\\# NB: RH Linux schemas in /etc/openldapinclude /etc/openldap/schema/corba.schemainclude /etc/openldap/schema/core.schemainclude /etc/openldap/schema/cosine.schemainclude /etc/openldap/schema/duaconf.schemainclude /etc/openldap/schema/dyngroup.schemainclude /etc/openldap/schema/inetorgperson.schemainclude /etc/openldap/schema/java.schemainclude /etc/openldap/schema/misc.schemainclude /etc/openldap/schema/nis.schemainclude /etc/openldap/schema/openldap.schemainclude /etc/openldap/schema/ppolicy.schemainclude /etc/openldap/schema/collective.schema\\# NO SECURITY - no access clause\\# defaults to anonymous access for read\\# only rootdn can write\\# NO REFERRALS\\# DON&#x27;T bother with ARGS file unless you feel strongly\\# slapd scripts stop scripts need this to workpidfile /var/run/openldap/slapd.pid\\# enable a lot of logging - we might need it\\# but generates huge logsloglevel -1\\# MODULELOAD definitions\\# not required (comment out) before version 2.3moduleload back_bdb.la\\# NO TLS-enabled connections\\# backend definition not required\\####################################################################################################\\# bdb database definitions\\#\\# replace example and com below with a suitable domain\\#\\# If you don&#x27;t have a domain you can leave it since example.com\\# is reserved for experimentation or change them to my and inc\\#\\####################################################################################################database bdb**suffix** &quot;dc=devops-dev, dc=chinaunicom&quot;\\# root or superuser**rootdn** &quot;cn=Manager,dc=devops-dev, dc=chinaunicom&quot;**rootpw** &#123;SSHA&#125;obNNrW3soCjEh0QdDquGq4vHyPiyb2qN\\# The database directory MUST exist prior to running slapd AND\\# change path as necessarydirectory /var/lib/ldap\\# Indices to maintain for this directory\\# unique id so equality match onlyindex uid eq\\# allows general searching on commonname, givenname and emailindex cn,gn,mail eq,sub\\# allows multiple variants on surname searchingindex sn eq,sub\\# sub above includes subintial,subany,subfinal\\# optimise department searchesindex ou eq\\# if searches will include objectClass uncomment following\\# index objectClass eq\\# shows use of default index parameterindex default eq,sub\\# indices missing - uses default eq,subindex telephonenumber eq\\# other database parameters\\# read more in slapd.conf reference sectioncachesize 10000checkpoint 128 15 (1)修改管理员用户rootdn cn&#x3D;Manager,dc&#x3D;devops-dev,dc&#x3D;chinaunicom (2)修改管理员密码通过slappasswd命令生成{SSHA}加密密码rootpw {SSHA}obNNrW3soCjEh0QdDquGq4vHyPiyb2qN 检测配置有无问题1slaptest -f /etc/openldap/slapd.conf 报错的这个文件是在启动的时候进行初始化的。先忽视这个问题，不妨我们先启动它。 修改配置12345vim /etc/openldap/slapd.d/cn\\=config\\/olcDatabase\\=&#123;2&#125;hdb.ldif olcSuffix: dc=mypaas,dc=com olcRootDN: cn=Manager,dc=mypaas,dc=com \\#管理账号的用户名 olcRootPW: &#123;SSHA&#125;GPEzYwuXyEjXetnjC7uKXydXoERcF3HB \\#管理账号的密码 修改监控认证配置12345vim /etc/openldap/slapd.d/cn\\=config\\/olcDatabase\\=&#123;1&#125;monitor.ldif olcAccess: &#123;0&#125;to * by dn.base=&quot;gidNumber=0+uidNumber=0,cn=peercred,cn=extern al,cn=auth&quot; read by dn.base=&quot;cn=Manager,dc=devops-dev,dc=chinaunicom&quot; read by * none 注意 和 cn=config/olcDatabase={2}hdb.ldif 文件中的 olcRootDN 相同 启动OpenLDAP服务和开机启动12systemctl start slapd.servicesystemctl enable slapd.service 再次检查配置1slaptest -f /etc/openldap/slapd.conf 检查服务状态1systemctl status slapd 导入模板1ls /etc/openldap/schema/*.ldif | xargs -I &#123;&#125; sudo ldapadd -Y EXTERNAL -H ldapi:/// -f &#123;&#125; 安装phpldapadmin参考：https://www.cnblogs.com/linuxws/p/9084455.html rpm包安装链接: https://pan.baidu.com/s/1h2_vxYPVAYlKxYimr9oU4w 提取码: 2vum 1rpm -ivh *.rpm --nodeps --force 修改配置文件httpd.conf1vim /etc/httpd/conf/httpd.conf 找到AllowOverride一行，修改none为all 1234&lt;Directory /&gt;AllowOverride allRequire all denied&lt;/Directory&gt; 注意 如果想修改端口号，修改Listen 80一行 启动服务，测试页面12systemctl start httpd.servicesystemctl enable httpd.service curl http://127.0.0.1/ 修改配置文件1vim /etc/phpldapadmin/config.php 找到并取消下面几行的注释： 1234567891011121314$servers-&gt;setValue(‘server’,’host’,’127.0.0.1’);$servers-&gt;setValue(‘server’,’port’,389);$servers-&gt;setValue(‘server’,’base’,array(‘**dc=devops-dev,dc=chinaunicom**’));\\# array里加上openldap配置文件中设置的olcSuffix$servers-&gt;setValue(‘login’,’auth_type’,’session’);$servers-&gt;setValue(‘login’,’attr’,’dn’);把下边这一行注释掉0$servers-&gt;setValue(‘login’,’attr’,’uid’); 修改访问配置文件，允许任意ip访问1vim /etc/httpd/conf.d/phpldapadmin.conf 添加一行指令，允许这个IP段访问 Require ip 192.168.0 #指定可访问的ip段（不填不能访问到这个管理工具） 重启httpd服务 1systemctl restart httpd.service 创建基础目录在&#x2F;etc&#x2F;openldap目录下添加base.ldif文件 12345678cd /etc/openldap/vim base.ldifdn: dc=devops-dev,dc=chinaunicomo: ldapobjectclass: dcObjectobjectclass: organizationdc: devops-dev 导入基础目录12ldapadd -x -D &quot;cn=Manager,dc=devops-dev,dc=chinaunicom&quot; -W -f base.ldifldapsearch -x -b &#x27; dc=devops-dev,dc=chinaunicom &#x27; &#x27;(objectClass=*)&#x27; 其它问题问题1 错误原因：管理员密码错误这个需要自行判断，如需要修改rootdn的密码，在&#x2F;etc&#x2F;openldap&#x2F;slapd.conf中改掉roodn的rootpw项，然后执行如下操作 12345$ rm -fr /etc/openldap/slapd.d/*$ sudo slaptest -f /etc/openldap/slapd.conf -F /etc/openldap/slapd.d\\#测试配置文件语法是否有错误，如果提示testing succeeded则可以进入下一步$ sudo chown -R ldap.ldap /etc/openldap/slapd.d$ sudo systemctl restart slapd.service 提示： 删除&#x2F;etc&#x2F;openldap&#x2F;slapd.d&#x2F;目录下的内容，并不会导致ldap数据库的丢失，实际上，ldap数据库存储位置（通常位于&#x2F;var&#x2F;lib&#x2F;ldap目录下）由主配置文件里的directory项指定。 问题2Cannot load modules&#x2F;libphp5.so into server: &#x2F;lib64&#x2F;libcrypto.so.10: version &#96;OPENSSL_1.0.2’ not found (required by &#x2F;etc&#x2F;httpd&#x2F;modules&#x2F;libphp5.so) 利用rpm -qa|grep openssl查看安装的openssl的版本若为openssl-1.0.1,则需要安装openssl-1.0.2,如下rpm包： openssl-1.0.2k-16.el7_6.1.x86_64.rpmopenssl-libs-1.0.2k-16.el7_6.1.x86_64.rpm 问题3 1vim /etc/httpd/conf.d/phpldapadmin.conf 123456789101112131415&lt;Directory /usr/share/phpldapadmin/htdocs&gt; &lt;IfModule mod_authz_core.c&gt; \\# Apache 2.4 Require all granted &lt;/IfModule&gt; &lt;IfModule !mod_authz_core.c&gt; \\# Apache 2.2 Order Deny,Allow Allow from all Allow from 127.0.0.1 Allow from ::1 &lt;/IfModule&gt;&lt;/Directory&gt; 1systemctl restart httpd.service","tags":[{"name":"Idap","slug":"Idap","permalink":"https://wangdj104.github.io/tags/Idap/"}]},{"title":"持续构建持续部署文档","date":"2019-12-09T06:45:38.000Z","path":"2019/12/09/jenkins自动化构建部署/","text":"第一章 rke部署k8s集群 机器 192.168.17.129 192.168.17.130 192.168.17.131 192.168.17.132 参考博客：https://blog.csdn.net/godservant/article/details/80895970 安装docker 四台机器均安装docker,版本要求1.11.x 1.12.x 1.13.x 17.03.x 123456789101112chmod +x docker-composemv docker-compose /usr/bin/rpm -ivh *.rpm --nodeps --forcesystemctl start dockersystemctl enable dockervim /etc/docker/daemon.json&#123;&quot;insecure-registries&quot;:[&quot;0.0.0.0/0&quot;]&#125;systemctl restart docker 运行环境 软件 版本&#x2F;镜像 备注 OS RHEL 7.2 Docker 1.12.6 RKE v0.0.12-dev kubectl v1.8.4 Kubernetes rancher&#x2F;k8s:v1.8.3-rancher2 canal quay.io&#x2F;calico&#x2F;node:v2.6.2 quay.io&#x2F;calico&#x2F;cni:v1.11.0 quay.io&#x2F;coreos&#x2F;flannel:v0.9.1 etcd quay.io&#x2F;coreos&#x2F;etcd:latest alpine alpine:latest Nginx proxy rancher&#x2F;rke-nginx-proxy:v0.1.0 Cert downloader rancher&#x2F;rke-cert-deployer:v0.1.1 Service sidekick rancher&#x2F;rke-service-sidekick:v0.1.0 kubedns gcr.io&#x2F;google_containers&#x2F;k8s-dns-kube-dns-amd64:1.14.5 dnsmasq gcr.io&#x2F;google_containers&#x2F;k8s-dns-dnsmasq-nanny-amd64:1.14.5 Kubedns sidecar gcr.io&#x2F;google_containers&#x2F;k8s-dns-sidecar-amd64:1.14.5 Kubedns autoscaler gcr.io&#x2F;google_containers&#x2F;cluster-proportional-autoscaler-amd64:1.0.0 部署k8s集群创建rke用户并设置rke的ssh免密su - rke（只要是集群节点，就需要做免密，包括自己） 1、生成默认格式的密匙key，此过程会在&#x2F;root&#x2F;.ssh&#x2F;文件件夹下生成id_rsa(私钥）和id_rsa.pub(公钥）。 1ssh-keygen 查看&#x2F;root&#x2F;.ssh&#x2F;目录 2、将公钥id_rsa.pub复制到远程主机&#x2F;root&#x2F;.ssh&#x2F;文件中，并且重命名为authorized_keys。（此处以远程主机ip为192.168.17.130举例） 1ssh-copy-id -i ~/.ssh/id_rsa.pub rke@192.168.17.130 关闭swap分区Kubelet运行是需要worker节点关闭swap分区，执行以下命令关闭swap分区 1）永久禁用swap 可以直接修改&#x2F;etc&#x2F;fstab文件，注释掉swap项 1vi /etc/fstab 2）临时禁用 1swapoff -a 将rke用户加入到docker用户组123usermod -aG docker &lt;user_name&gt;eg: usermod -aG docker rke 关闭内核防火墙 1vim /etc/selinux/config 1reboot 下载rke和kubectl（1）rke最新版本下载: https://github.com/rancher/rke/releases/, 下载rke_linux-amd64 123 mv rke_linux-amd64 rke chmod +x rkemv rke /usr/bin/ （2）Kubectl下载：https://storage.googleapis.com/kubernetes-release/release/v1.7.0/bin/darwin/amd64/kubectl 12chmod +x kubectlmv kubectl /usr/bin/ 部署私有容器镜像仓库harbor首先将搭建集群所需的镜像上传到公有harbor上.在镜像所在机器保存特定标签的镜像 123docker images --format &quot;&#123;&#123;.Repository&#125;&#125;:&#123;&#123;.Tag&#125;&#125;&quot;\\|grep 192.168.17.130docker save -o rke.tar.gz $(docker images --format &quot;&#123;&#123;.Repository&#125;&#125;:&#123;&#123;.Tag&#125;&#125;&quot;|grep 192.168.17.130) 所有镜像下载链接: https://pan.baidu.com/s/1flpjyrVHX283f-t0b7RFtQ 提取码: ubct 修改hosts文件(非必须)123456vim /etc/hots192.168.17.129 rke192.168.17.130 node1192.168.17.131 node2192.168.17.132 node3 编辑集群配置文件cluster.yml可以直接在主机的rke用户下执行： .&#x2F;rke config 命令, 生成配置文件: cluster.yml 。本次为了方便，我直接上传cluster.yml，具体内容如下： 1su - rke 创建cluster.yml文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150# If you intened to deploy Kubernetes in an air-gapped environment,# please consult the documentation on how to configure custom RKE images.nodes:- address: &quot;192.168.17.129&quot; port: &quot;22&quot; internal_address: &quot;&quot; role: - controlplane - etcd - worker hostname_override: &quot;&quot; user: rke docker_socket: /var/run/docker.sock ssh_key: &quot;&quot; ssh_key_path: ~/.ssh/id_rsa labels: &#123;&#125;- address: &quot;192.168.17.130&quot; port: &quot;22&quot; internal_address: &quot;&quot; role: - etcd - worker hostname_override: &quot;&quot; user: rke docker_socket: /var/run/docker.sock ssh_key: &quot;&quot; ssh_key_path: ~/.ssh/id_rsa labels: &#123;&#125;- address: &quot;192.168.17.131&quot; port: &quot;22&quot; internal_address: &quot;&quot; role: - worker hostname_override: &quot;&quot; user: rke docker_socket: /var/run/docker.sock ssh_key: &quot;&quot; ssh_key_path: ~/.ssh/id_rsa labels: &#123;&#125;services: etcd: image: &quot;&quot; extra_args: &#123;&#125; extra_binds: [] extra_env: [] external_urls: [] ca_cert: &quot;&quot; cert: &quot;&quot; key: &quot;&quot; path: &quot;&quot; snapshot: false retention: &quot;&quot; creation: &quot;&quot; kube-api: image: &quot;&quot; extra_args: &#123;&#125; extra_binds: [] extra_env: [] service_cluster_ip_range: 10.43.0.0/16 service_node_port_range: &quot;&quot; pod_security_policy: false kube-controller: image: &quot;&quot; extra_args: &#123;&#125; extra_binds: [] extra_env: [] cluster_cidr: 10.42.0.0/16 service_cluster_ip_range: 10.43.0.0/16 scheduler: image: &quot;&quot; extra_args: &#123;&#125; extra_binds: [] extra_env: [] kubelet: image: &quot;&quot; extra_args: &#123;&#125; extra_binds: [] extra_env: [Environment=&quot;KUBELET_SYSTEM_PODS_ARGS=--pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true --fail-swap-on=false&quot;] cluster_domain: cluster.local infra_container_image: &quot;&quot; cluster_dns_server: 10.43.0.10 fail_swap_on: false kubeproxy: image: &quot;&quot; extra_args: &#123;&#125; extra_binds: [] extra_env: []network: plugin: canal options: &#123;&#125;authentication: strategy: x509 options: &#123;&#125; sans: []addons: &quot;&quot;addons_include: []system_images: etcd: 192.168.17.132/rke/quay.io/coreos/etcd:v3.1.12 alpine: 192.168.17.132/rke/rancher/rke-tools:v0.1.13 nginx_proxy: 192.168.17.132/rke/rancher/rke-tools:v0.1.13 cert_downloader: 192.168.17.132/rke/rancher/rke-tools:v0.1.13 kubernetes_services_sidecar: 192.168.17.132/rke/rancher/rke-tools:v0.1.13 kubedns: 192.168.17.132/rke/gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.7 dnsmasq: 192.168.17.132/rke/gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.7 kubedns_sidecar: 192.168.17.132/rke/gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.7 kubedns_autoscaler: 192.168.17.132/rke/gcr.io/google_containers/cluster-proportional-autoscaler-amd64:1.0.0 kubernetes: 192.168.17.132/rke/rancher/hyperkube:v1.9.7-rancher2 flannel: 192.168.17.132/rke/quay.io/coreos/flannel:v0.9.1 flannel_cni: 192.168.17.132/rke/quay.io/coreos/flannel-cni:v0.2.0 calico_node: 192.168.17.132/rke/quay.io/calico/node:v3.1.1 calico_cni: 192.168.17.132/rke/quay.io/calico/cni:v3.1.1 calico_controllers: &quot;&quot; calico_ctl: 192.168.17.132/rke/quay.io/calico/ctl:v2.0.0 canal_node: 192.168.17.132/rke/quay.io/calico/node:v3.1.1 canal_cni: 192.168.17.132/rke/quay.io/calico/cni:v3.1.1 canal_flannel: 192.168.17.132/rke/quay.io/coreos/flannel:v0.9.1 wave_node: 192.168.17.132/rke/weaveworks/weave-kube:2.1.2 weave_cni: 192.168.17.132/rke/weaveworks/weave-npc:2.1.2 pod_infra_container: 192.168.17.132/rke/gcr.io/google_containers/pause-amd64:3.0 ingress: 192.168.17.132/rke/rancher/nginx-ingress-controller:0.16.2-rancher1 ingress_backend: 192.168.17.132/rke/k8s.gcr.io/defaultbackend:1.4 metrics_server: 192.168.17.132/rke/metrics-server-amd64:v0.2.1ssh_key_path: ~/.ssh/id_rsassh_agent_auth: falseauthorization: mode: rbac options: &#123;&#125;ignore_docker_version: falsekubernetes_version: &quot;&quot;private_registries: []ingress: provider: &quot;&quot; options: &#123;&#125; node_selector: &#123;&#125; extra_args: &#123;&#125;cluster_name: &quot;&quot;cloud_provider: name: &quot;&quot;prefix_path: &quot;&quot;addon_job_timeout: 0bastion_host: address: &quot;&quot; port: &quot;&quot; user: &quot;&quot; ssh_key: &quot;&quot; ssh_key_path: &quot;&quot;monitoring: provider: &quot;&quot; options: &#123;&#125; 启动rke，部署集群执行 1rke -d up 或 1rke up -config cluster.yml 出现下图即为成功 移动集群配置文件运行rke成功部署集群后会生成.kube_config_cluster.yml文件 1cp kube_config_cluster.yml /home/rke/.kube/config 删除k8s集群运行 1rke -d remove 或 1rke remove -config cluster.yml 清理节点上的docker容器，在每个节点上执行如下命令： 1docker rm -fv $(docker ps -aq) 为集群节点创建标签为节点创建标签 1kubectl label node 192.168.17.131 ci.role=slave 获取节点信息(带节点的标签) 1kubectl get node --show-labels 查询带特定标签的节点 1kubectl get node -a -l &quot; ci.role=slave &quot; 删除一个Label，只需在命令行最后指定Label的key名并与一个减号相连即可 1kubectl label nodes 192.168.17.131 ci.role- 然后在pipeline中就可以加：nodeSelector: ‘ci.role&#x3D;slaver’ 语句了 为集群创建maven-setting创建configmap-setting.yaml文件： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465apiVersion: v1data: settings.xml: | &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;settings xmlns=&quot;http://maven.apache.org/SETTINGS/1.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd&quot;&gt; &lt;pluginGroups&gt; &lt;/pluginGroups&gt; &lt;proxies&gt; &lt;/proxies&gt; &lt;servers&gt; &lt;server&gt; &lt;id&gt;chinaunicom&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;11111111&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;maven-snapshot-manager&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;11111111&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;maven-release-manager&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;11111111&lt;/password&gt; &lt;/server&gt; &lt;/servers&gt; &lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;chinaunicom&lt;/id&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;name&gt;Human Readable Name for this Mirror.&lt;/name&gt; &lt;url&gt;http://192.168.17.132:8081/repository/maven-public/&lt;/url&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;maven-snapshot-manager&lt;/id&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;name&gt;Human Readable Name for this Mirror.&lt;/name&gt; &lt;url&gt;http://192.168.17.132:8081/repository/maven-snapshots/&lt;/url&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;maven-release-manager&lt;/id&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;name&gt;Human Readable Name for this Mirror.&lt;/name&gt; &lt;url&gt;http://192.168.17.132:8081/repository/maven-releases/&lt;/url&gt; &lt;/mirror&gt; &lt;/mirrors&gt; &lt;profiles&gt; &lt;/profiles&gt; &lt;/settings&gt;kind: ConfigMapmetadata: name: configmap-maven-settings-mirror-31010 namespace: jenkins 执行命令 1kubectl create -f configmap-setting.yaml 创建名为configmap-maven-settings-mirror-31010的configMap,然后pipeline中就可以如下配置： 查看configMap 1kubectl get configmap -n jenkins 为集群创建configMap创建kubeconfig-dev.yaml文件： 1234567891011121314151617181920212223242526apiVersion: v1data: config: | apiVersion: v1 kind: Config clusters: - cluster: api-version: v1 certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN3akNDQWFxZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFTTVJBd0RnWURWUVFERXdkcmRXSmwKTFdOaE1CNFhEVEU1TVRJd01URXlOREkwTTFvWERUSTVNVEV5T0RFeU5ESTBNMW93RWpFUU1BNEdBMVVFQXhNSAphM1ZpWlMxallUQ0NBU0l3RFFZSktvWklodmNOQVFFQkJRQURnZ0VQQURDQ0FRb0NnZ0VCQU53TVdZY1dheGk3ClA0VXNhOExCSW1NL3Q3bUlZMEVHTnZia2dGd3hpR2tBODhBMXJ3SjhzNFE3cFRmS3BzS3Zublp2QlY4aHdpNmQKKzZkK0xFMTlTRFNvL2owRHMzUDRJNFY5S3E1Mk9sRzN4U0pFTXY3VW9DM0VJOE9OM1pMZnIwSlpNRnpvaW1BbwpjUjFDTENlVVN0WDNudXk5K2Y0cjBuaEZGRDcyNkFJTmN3QlB6SjI5RHcwZ3grbmFuVnh6S3UzSjJOSm9hS05jCm41V0dMeUE4V2YxNUdUQVdGL29rcklkTHZFTUxQSUx5SG43elRHZm52TFNTRVhJWTFBZEl2bFRlcmI2UXdNUjIKcWFMSlg4WEo0WTlQR0UvdktUVGdrMHZGdm9ZQTcvbnZQdGRoUTF2aGJqcWpsL2NJM0JtYnhnNFZJL1RUQzl3SQppQ0IzeUdVeXpGRUNBd0VBQWFNak1DRXdEZ1lEVlIwUEFRSC9CQVFEQWdLa01BOEdBMVVkRXdFQi93UUZNQU1CCkFmOHdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBTTBJakVGSHM0TmNiT1FQNVdpd0tXZ0k0WDdMZy9Sa2VCK20KaWx3YzU4aU1XR3ZGT1hJc3ZBaFU1YVdUSHlIQUt2VGd2U2xyamlwSDJxeVZVQTlkSlNkcjBCL2RPdnBac1RIcAprakdFOGFTYkNnajJBQ2o1bEZjT21DSC9oeTRPVHlnSEl0amJDaVpjejMvOVVZVTlkczh4RWlLejlsUDRwelE5Cm5IRGxwemJyU09pbWsrQTFiZTRQWXNIL0ozUWJ6cGs1VERWbEp0eWlGbHhBU2RWdmJUUWtMMjRUMXZHWFp3K1YKcEE0S1h1OTlTaTZncVhHeFQrcElJUE9HSjZzcDM2eXh1RE5YSnhKdGpTUk9zTWhlMm9oT0pXencwYUpWYWtpSQpqbDNiSnlMcDlVd2ZrOEZFOUd2UlVBd1c4VitCYzk1MkhwR2wyaXVyazZoSTVqV2ZEUnc9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K server: &quot;https://192.168.17.129:6443&quot; name: &quot;local&quot; contexts: - context: cluster: &quot;local&quot; user: &quot;kube-admin-local&quot; name: &quot;local&quot; current-context: &quot;local&quot; users: - name: &quot;kube-admin-local&quot; user: client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM2VENDQWRHZ0F3SUJBZ0lJSXMrbG05aXN6RW93RFFZSktvWklodmNOQVFFTEJRQXdFakVRTUE0R0ExVUUKQXhNSGEzVmlaUzFqWVRBZUZ3MHhPVEV5TURFeE1qUXlORE5hRncweU1ERXhNekF4TWpReU5EWmFNQzR4RnpBVgpCZ05WQkFvVERuTjVjM1JsYlRwdFlYTjBaWEp6TVJNd0VRWURWUVFERXdwcmRXSmxMV0ZrYldsdU1JSUJJakFOCkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tDQVFFQXNLdGtaZWY4bzMzdzB0VmpmbFZITVMrVHpCM0QKV2NnM1o3L3JMZ0UzTS9ySlkvdUtJV2RiTmxYZG1ZeTBJakdQVEZqNFl4eHV4Ymo4cy9xVmdDR1dJQUk2dVAxcgpxYUlZdm5ud3dWZUVDcTZsZG9TM09vYTFGQzFlTFg3Z0o2OWE0UVprR2lYNHhZYkpkTUNTWm92N3JzamtIbUgvCmJ3Z010cVlzdk9sOTJ6S2NiaEtFNG5GejVlN3A2Qm5wL05BQXhvYnF2K0F4VHlrYUdBYWRna0trM3Jrd2VCUkIKODVQV0pSZDVITDBwOTUybzFwdkthbGx0MVh1a0ZPTHgzc3lJdTQrR1EwbWJWQkVmeVRYM2JoSFlpaG05NDlHdgo1c0MySVN4WUxLT0RSc2xuY1lVcGVYYVR5NldJRmFMODc1ZFVIaG9vRVZFTjJ5WnVYVlMvZmVMWWp3SURBUUFCCm95Y3dKVEFPQmdOVkhROEJBZjhFQkFNQ0JhQXdFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUhBd0l3RFFZSktvWkkKaHZjTkFRRUxCUUFEZ2dFQkFKQktqZjZ1Zk40ZjBmdTBHMFQxNHJZWS96VGRPSjNOeGxBMXp3aUJrQndNWHFXcApBQW5HOFp6bzlvYWE0MFNkR2FRUUVCSnR4ZHFBYWNWOW1kREFJM1lmdW1UQlVTdVZCSDdEYUx4RG9HU283NElDCkFpVkYzNWk1cGRkWjNZRCt4c2Q1Ni94QXBJdTFvOGpxVFFRejlQUDFIZFRMaTNZYm9OM0FOdFNKZnFlSlBVckcKYXJjTDdmdGRldEdvdWJUdlhPazIxOXk3YldpRE9xSlFqVkpVajVudkNXL25xRFBmbDZmbkZ1YStxMUg4bXZ4QwpzRDdlMEgra0F0RHN5TW1RbXRzd2l4alpkVU5vRzZYZUlMZ25aaEMrejVWdjlRd0hEMXd4YXdRc0pZK3YzU0drClAydWFMcXFXb1FwQklmZHZXaThzK29DdTA1NXlYNzZpODgxVTFsWT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo= client-key-data: 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kind: ConfigMapmetadata: name: configmap-kubeconfig-dev namespace: jenkins 执行命令 1kubectl create -f kubeconfig-dev.yaml 创建名为configmap-kubeconfig-dev的configMap,然后pipeline中就可以如下配置： 调试集群查看集群部署信息 1kubectl cluster-info 查看k8s的节点是否工作正常 1kubectl get nodes -o wide 查看所有命名空间下的deployment 1kubectl get deployment --all-namespaces 查看所有命名空间下的svc 1kubectl get svc --all-namespaces 查看所有命名空间下的pod 1kubectl get pod -o wide --all-namespaces 查看某个pod的运行日志 1kubectl logs -f kubernetes-dashboard-7f9f8cc4cf-tm2ld -n kube-system 删除某个pod 1kubectl delete pod ms-cloud-tenant-service-5dc69784b-f42vh -n cloud 查看某个失败的pod的明细 1kubectl describe pod metrics-server-6c84bc5674-tf4qw -n kube-system 查看镜像描述信息 1docker inspect 10.124.133.192/devops/jenkins-slaver-dind:v1.0.0 查看某个命名空间下pod的实时状态变化 1kubectl get pod -o wide -n jenkins -w 查看所有启动的容器（包括失败的） 1docker ps -a 部署k8s dashboard页面参考博客：https://www.kubernetes.org.cn/4004.html 部署dashboard(1)新建k8s-dashboard.yaml文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164# Copyright 2017 The Kubernetes Authors.## Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# ------------------- Dashboard Secret ------------------- #apiVersion: v1kind: Secretmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-certs namespace: kube-systemtype: Opaque---# ------------------- Dashboard Service Account ------------------- #apiVersion: v1kind: ServiceAccountmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system---# ------------------- Dashboard Role &amp; Role Binding ------------------- #kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: kubernetes-dashboard-minimal namespace: kube-systemrules: # Allow Dashboard to create &#x27;kubernetes-dashboard-key-holder&#x27; secret.- apiGroups: [&quot;&quot;] resources: [&quot;secrets&quot;] verbs: [&quot;create&quot;] # Allow Dashboard to create &#x27;kubernetes-dashboard-settings&#x27; config map.- apiGroups: [&quot;&quot;] resources: [&quot;configmaps&quot;] verbs: [&quot;create&quot;] # Allow Dashboard to get, update and delete Dashboard exclusive secrets.- apiGroups: [&quot;&quot;] resources: [&quot;secrets&quot;] resourceNames: [&quot;kubernetes-dashboard-key-holder&quot;, &quot;kubernetes-dashboard-certs&quot;] verbs: [&quot;get&quot;, &quot;update&quot;, &quot;delete&quot;] # Allow Dashboard to get and update &#x27;kubernetes-dashboard-settings&#x27; config map.- apiGroups: [&quot;&quot;] resources: [&quot;configmaps&quot;] resourceNames: [&quot;kubernetes-dashboard-settings&quot;] verbs: [&quot;get&quot;, &quot;update&quot;] # Allow Dashboard to get metrics from heapster.- apiGroups: [&quot;&quot;] resources: [&quot;services&quot;] resourceNames: [&quot;heapster&quot;] verbs: [&quot;proxy&quot;]- apiGroups: [&quot;&quot;] resources: [&quot;services/proxy&quot;] resourceNames: [&quot;heapster&quot;, &quot;http:heapster:&quot;, &quot;https:heapster:&quot;] verbs: [&quot;get&quot;]---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: kubernetes-dashboard-minimal namespace: kube-systemroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: kubernetes-dashboard-minimalsubjects:- kind: ServiceAccount name: kubernetes-dashboard namespace: kube-system---# ------------------- Dashboard Deployment ------------------- #kind: DeploymentapiVersion: apps/v1beta2metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemspec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: kubernetes-dashboard template: metadata: labels: k8s-app: kubernetes-dashboard spec: containers: - name: kubernetes-dashboard image: 192.168.17.132/rke/k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.3 ports: - containerPort: 8443 protocol: TCP args: - --auto-generate-certificates # Uncomment the following line to manually specify Kubernetes API server Host # If not specified, Dashboard will attempt to auto discover the API server and connect # to it. Uncomment only if the default does not work. # - --apiserver-host=http://my-address:port volumeMounts: - name: kubernetes-dashboard-certs mountPath: /certs # Create on-disk volume to store exec logs - mountPath: /tmp name: tmp-volume livenessProbe: httpGet: scheme: HTTPS path: / port: 8443 initialDelaySeconds: 30 timeoutSeconds: 30 volumes: - name: kubernetes-dashboard-certs secret: secretName: kubernetes-dashboard-certs - name: tmp-volume emptyDir: &#123;&#125; serviceAccountName: kubernetes-dashboard # Comment the following tolerations if Dashboard must not be deployed on master tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule---# ------------------- Dashboard Service ------------------- #kind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemspec: ports: - port: 443 targetPort: 8443 selector: k8s-app: kubernetes-dashboard 执行 1kubectl create -f k8s-dashboard.yaml (2)新建dashboard-usr.yaml文件 123456789101112131415161718apiVersion: v1kind: ServiceAccountmetadata: name: admin-user namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: admin-userroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: admin-user namespace: kube-system 执行 1kubectl create -f dashboard-usr.yml (3)若对yml修改后,更新部署,则执行下述命令 1kubectl apply -f dashboard-usr.yml 访问dashboard通过window配置负载(1)安装kubectl 1curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.7.0/bin/windows/amd64/kubectl.exe (2) 拷贝集群kube_config_cluster.ym文件到本地config.yml在cmd执行以下命令 1kubectl --kubeconfig=C:\\Users\\Administrator\\Desktop\\config.yml proxy (3)web访问 1http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/ (4) 生成web访问的token 12kubectl -n kube-system describe secret $(kubectl -n kube-system get secret |grep admin-user | awk &#x27;&#123;print $1&#125;&#x27;) 通过configMap配置负载(1)编辑configmap文件 1kubectl edit configmap tcp-services -n ingress-nginx 添加以下两行 12data: &quot;9090&quot;: kube-system/kubernetes-dashboard:443 (2)生成token 1kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk &#x27;&#123;print $1&#125;&#x27;) (3)第一次部署报错的情况下,删除token 1kubectl --kubeconfig ../kube_config_cluster.yml delete secret kubernetes-dashboard-key-holder -n kube-system 访问dashboard地址：https://192.168.17.131:9090 部署k8s dashboard页面图形化数据上传heapster-grafana.yaml heapster.yaml influxdb.yaml文件，创建(1)创建heapster-grafana.yaml文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465apiVersion: apps/v1kind: Deploymentmetadata: name: monitoring-grafana namespace: kube-systemspec: replicas: 1 selector: matchLabels: task: monitoring k8s-app: grafana template: metadata: labels: task: monitoring k8s-app: grafana spec: containers: - name: grafana image: 192.168.17.132/rke/heapster-grafana:v4.3.3 ports: - containerPort: 3000 protocol: TCP volumeMounts: - mountPath: /etc/ssl/certs name: ca-certificates readOnly: true - mountPath: /var name: grafana-storage env: - name: INFLUXDB_HOST value: monitoring-influxdb - name: GF_SERVER_HTTP_PORT value: &quot;3000&quot; - name: GF_AUTH_BASIC_ENABLED value: &quot;false&quot; - name: GF_AUTH_ANONYMOUS_ENABLED value: &quot;true&quot; - name: GF_AUTH_ANONYMOUS_ORG_ROLE value: Admin - name: GF_SERVER_ROOT_URL value: / volumes: - name: ca-certificates hostPath: path: /etc/ssl/certs - name: grafana-storage emptyDir: &#123;&#125;---apiVersion: v1kind: Servicemetadata: labels: kubernetes.io/cluster-service: &#x27;true&#x27; kubernetes.io/name: monitoring-grafana name: monitoring-grafana namespace: kube-systemspec: type: NodePort ports: - nodePort: 30108 port: 80 targetPort: 3000 selector: k8s-app: grafana (2)创建heapster.yaml文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263apiVersion: v1kind: ServiceAccountmetadata: name: heapster namespace: kube-system---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: heapster namespace: kube-systemspec: replicas: 1 selector: matchLabels: task: monitoring k8s-app: heapster template: metadata: labels: task: monitoring k8s-app: heapster spec: serviceAccountName: heapster containers: - name: heapster image: 192.168.17.132/rke/heapster:v1.4.0 imagePullPolicy: IfNotPresent command: - /heapster # - --source=kubernetes:https://$KUBERNETES_SERVICE_HOST:443?inClusterConfig=true&amp;useServiceAccount=true - --source=kubernetes:kubernetes:https://kubernetes.default?useServiceAccount=true&amp;kubeletHttps=true&amp;kubeletPort=10250&amp;insecure=true - --sink=influxdb:http://monitoring-influxdb.kube-system.svc.cluster.local:8086?retention=0s - --v=2 ---apiVersion: v1kind: Servicemetadata: labels: task: monitoring kubernetes.io/cluster-service: &#x27;true&#x27; kubernetes.io/name: Heapster name: heapster namespace: kube-systemspec: ports: - port: 80 targetPort: 8082 selector: k8s-app: heapster---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: heapsterroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: heapster namespace: kube-system (3)创建influxdb.yaml文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344apiVersion: apps/v1kind: Deploymentmetadata: name: monitoring-influxdb namespace: kube-systemspec: replicas: 1 selector: matchLabels: task: monitoring k8s-app: influxdb template: metadata: labels: task: monitoring k8s-app: influxdb spec: containers: - name: influxdb image: 192.168.17.132/rke/heapster-influxdb:v1.3.3 volumeMounts: - mountPath: /data name: influxdb-storage volumes: - name: influxdb-storage emptyDir: &#123;&#125;---apiVersion: v1kind: Servicemetadata: labels: task: monitoring kubernetes.io/cluster-service: &#x27;true&#x27; kubernetes.io/name: monitoring-influxdb name: monitoring-influxdb namespace: kube-systemspec: type: NodePort ports: - nodePort: 31001 port: 8086 targetPort: 8086 selector: k8s-app: influxdb (4)创建各个pod 123kubectl create -f heapster-grafana.yamlkubectl create -f heapster.yamlkubectl create -f influxdb.yaml –source: 指定连接的集群。inClusterConfig:Use kube config in service accounts associated with Heapster’s namespace.(default: true)kubeletPort: 指定kubelet的使用端口，默认10255kubeletHttps: 是否使用https去连接kubelets(默认：false)apiVersion: 指定K8S的apiversioninsecure: 是否使用安全证书(默认：false)auth: 安全认证useServiceAccount: 是否使用K8S的安全令牌 –sink: 指定后端数据存储。这里指定influxdb数据库。后缀参数：user: InfluxDB用户pw: InfluxDB密码db: 数据库名secure: 安全连接到InfluxDB(默认：false)withfields： 使用InfluxDB fields(默认：false)。 第二章 自动化构建部署流程搭建部署jenkins-master(1)下载镜像jenkins-master链接: https://pan.baidu.com/s/1lhllaOIsvDXJoiFMPx47Qw 提取码: z24p或者链接: https://pan.baidu.com/s/1hROj7nt_iw0Vf1tQTShfYA 提取码: iieu (2)在192.168.17.131 上建目录 1mkdir /jenkins-data (3)赋予权限 1chown -R 1000:1000 /jenkins-data (4)创建jenkins-k8s-resources.yml文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106apiVersion: apps/v1beta1kind: Deploymentmetadata: name: jenkins-wdj namespace: jenkins labels: k8s-app: jenkins-wdjspec: replicas: 1 selector: matchLabels: k8s-app: jenkins-wdj template: metadata: labels: k8s-app: jenkins-wdj spec: nodeSelector: kubernetes.io/hostname: 192.168.17.131 containers: - name: jenkins-wdj image: 192.168.17.132/rke/jenkinsci-blueocean:v1.0 env: - name: TZ value: Asia/Shanghai imagePullPolicy: IfNotPresent volumeMounts: - name: jenkins-home-test mountPath: /var/jenkins_home ports: - containerPort: 8080 name: web - containerPort: 50000 name: agent volumes: - name: jenkins-home-test hostPath: path: /jenkins-data type: DirectoryOrCreate---kind: ServiceapiVersion: v1metadata: labels: k8s-app: jenkins-wdj name: jenkins-wdj namespace: jenkinsspec: type: NodePort ports: - port: 8080 name: web targetPort: 8080 nodePort: 31666 - port: 50000 name: agent targetPort: 50000 selector: k8s-app: jenkins-wdj---apiVersion: v1kind: ServiceAccountmetadata: name: jenkins-wdj namespace: jenkins---kind: RoleapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: jenkins-wdj namespace: jenkinsrules: - apiGroups: [&quot;&quot;] resources: [&quot;pods&quot;] verbs: [&quot;create&quot;,&quot;delete&quot;,&quot;get&quot;,&quot;list&quot;,&quot;patch&quot;,&quot;update&quot;,&quot;watch&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;pods/exec&quot;] verbs: [&quot;create&quot;,&quot;delete&quot;,&quot;get&quot;,&quot;list&quot;,&quot;patch&quot;,&quot;update&quot;,&quot;watch&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;pods/log&quot;] verbs: [&quot;get&quot;,&quot;list&quot;,&quot;watch&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;secrets&quot;] verbs: [&quot;get&quot;]---apiVersion: rbac.authorization.k8s.io/v1beta1kind: RoleBindingmetadata: name: jenkins-wdj namespace: jenkinsroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: jenkins-wdjsubjects: - kind: ServiceAccount name: jenkins-wdj (5)启动Jenkins-master 1kubectl create -f jenkins-k8s-resources.yml (6)在jenkins中安装kubernetes插件 部署jenkins-slave构建jenkins-slave基础镜像基于openshift/jenkins-slave-base-centos7:latest所构建的包含了maven, nodejs,helm, go, beego等常用构建工具(1)下载jenkins-slave-base-centos7:latest镜像 下载链接: https://pan.baidu.com/s/1Wvs1pRotGWqTT_6BLoRrTg 提取码: xvde (2)编辑Dockerfile文件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879FROM openshift/jenkins-slave-base-centos7:latestLABEL maintainer=caiy17@chinaunicom.cnLABEL Description=&quot;基于`openshift/jenkins-slave-base-centos7:latest`所构建的包含了maven, nodejs, helm, go, beego等常用构建工具&quot;ENV TZ Asia/Shanghai################################################################################ 生成ssh密钥###############################################################################USER rootRUN ssh-keygen -f /root/.ssh/id_rsa################################################################################ 设置yum源 安装常用工具################################################################################ RUN curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo \\ # &amp;&amp; yum makecache \\RUN yum install -y vim telnet net-tools wget unzip################################################################################ 安装JDK###############################################################################ADD packages/jdk-8u181-linux-x64.tar.gz /usr/javaENV JAVA_HOME /usr/java/jdk1.8.0_181ENV PATH $JAVA_HOME/bin:$PATHRUN chown root:root /usr/java/jdk1.8.0_181 -R \\ &amp;&amp; chmod 755 /usr/java/jdk1.8.0_181 -R################################################################################ 安装maven###############################################################################ADD packages/apache-maven-3.6.0-bin.tar.gz /optRUN chown root:root /opt/apache-maven-3.6.0 -R \\ &amp;&amp; chmod 755 /opt/apache-maven-3.6.0 -R \\ &amp;&amp; ln -s /opt/apache-maven-3.6.0/ /opt/mavenENV M2_HOME /opt/mavenENV PATH $M2_HOME/bin:$PATH# 配置maven镜像源RUN sed -i &#x27;N;146 a \\ \\ \\ \\ &lt;mirror&gt;\\n\\ \\ \\ \\ \\ \\ &lt;id&gt;self-maven&lt;/id&gt;\\n\\ \\ \\ \\ \\ \\ &lt;mirrorOf&gt;self-maven&lt;/mirrorOf&gt;\\n\\ \\ \\ \\ \\ \\ &lt;name&gt;self-maven&lt;/name&gt;\\n\\ \\ \\ \\ \\ \\ &lt;url&gt;http://10.236.5.18:8088/repository/maven-public/&lt;/url&gt;\\n\\ \\ \\ \\ &lt;/mirror&gt;&#x27; /opt/maven/conf/settings.xml################################################################################ 安装nodejs###############################################################################ADD packages/node-v10.15.3-linux-x64.tar.xz /optRUN chown root:root /opt/node-v10.15.3-linux-x64 -R \\ &amp;&amp; chmod 755 /opt/node-v10.15.3-linux-x64 -R \\ &amp;&amp; ln -s /opt/node-v10.15.3-linux-x64/ /opt/nodeENV NODE_HOME /opt/nodeENV PATH $PATH:$NODE_HOME/bin################################################################################ 安装helm###############################################################################ADD packages/helm-v2.8.2-linux-amd64.tar.gz /opt/helmENV HELM_HOME /opt/helmENV PATH $PATH:$HELM_HOME/linux-amd64################################################################################ 安装go###############################################################################ADD packages/go1.12.3.linux-amd64.tar.gz /optENV GOROOT /opt/goENV GOPATH /root/goENV PATH $PATH:$GOROOT/bin:$GOPATH/bin################################################################################ 安装beego###############################################################################ADD packages/bee.tar.gz /root/go/bin (2)执行构建命令 1docker build -t jenkins-slave:2019-09-11-v1 . 注意最后有个点，代表使用当前路径的 Dockerfile 进行构建 生成的镜像下载链接: https://pan.baidu.com/s/1Wvs1pRotGWqTT_6BLoRrTg 提取码: xvde 生成连接k8s集群的客户端证书1)生成连接api-server的服务证书 key并进行连接测试 1cat kube_config_cluster.yml 分别执行 12345echo certificate-authority-data | base64 -d &gt; ca.crtecho client-certificate-data | base64 -d &gt; client.crtecho client-key-data | base64 -d &gt; client.key 然后根据如上内容生成客户端认证的证书cert.pfx 1openssl pkcs12 -export -out cert.pfx -inkey client.key -in client.crt -certfile ca.crt 参考http://www.mamicode.com/info-detail-2399348.html 配置jenkins-master（参考）打开 jenkins UI 界面依次点击系统管理 –&gt; 系统设置 –&gt; 新增一个云（在界面最下方） –&gt; Kubernetes配置如下： 配置完成后点击保存 1234567891011121314151617kubectl create serviceaccount jenkins-wdj -n jenkinskubectl get serviceaccount --all-namespaceskubectl describe serviceaccount/jenkins-wdj -n jenkinskubectl get secret -n jenkinskubectl get secret jenkin-wdj-token-cm449 -o yaml -n jenkinskubectl get secret jenkins-wdj-token-ld6dc -n jenkins -o jsonpath=&#123;&quot;.data.token&quot;&#125; | base64 -dkubectl delete serviceaccount/jenkins-xy -n jenkinskubectl get sa jenkins-wdj -n jenkins -o yamlkubectl get secret jenkins-wdj-token-cm449 -n jenkins -o jsonpath=&#123;&quot;.data.token&quot;&#125; | base64 -d https://www.iteye.com/blog/m635674608-2361440 配置jenkins-master（选用）（1）新建启动集群时生成的kube_config_cluster.yml文件； （2）系统管理-》插件管理-》可选插件-》搜索kubernetes插件-》直接安装； （3）系统管理-》系统设置-》新增一个云，配置如下： 新建pipeline流水线任务demo1(1)新建一个测试pipeline任务demo; (2) 脚本 12345678910111213141516171819202122def name = &quot;ci-demo-backend&quot;def label = &quot;$&#123;name&#125;-$&#123;UUID.randomUUID().toString()&#125;&quot;podTemplate( label: label, namespace: &#x27;jenkins&#x27;, cloud: &#x27;kubernetes&#x27;, containers: [ containerTemplate(name: &#x27;jnlp&#x27;, image: &#x27;192.168.17.132/rke/cicd/jenkins-slaver-dind:v1.1.5&#x27;) ], volumes: [ hostPathVolume(hostPath: &#x27;/var/run/docker.sock&#x27;, mountPath: &#x27;/var/run/docker.sock&#x27;) ]) &#123; node(label) &#123; stage(&#x27;test&#x27;) &#123; echo &quot;hello, world&quot; sleep 60 &#125; &#125; &#125; demo2（1）K8s集群中设置harbor仓库认证（参考） 使用kubectl命令生成secret(不同的namespace要分别生成secret，不共用*)*1234567891011kubectl create secret docker-registry harborsecret \\ --docker-server=192.168.17.132 \\ --docker-username=admin \\ --docker-password=Harbor12345 \\ --docker-email=admin@example.com \\ --namespace=jenkins 查看此secret的配置内容 kubectl get secret harborsecret –output&#x3D;yaml -n jenkins （2）添加gitlab和harbor仓库的认证(选用) （2）下载Git Parameter插件 （3）设置参数化构建过程 demo3123456789101112131415161718192021222324252627282930313233343536373839404142434445def label = &quot;ci-ms-cloud-tenant-service-$&#123;UUID.randomUUID().toString()&#125;&quot;def deployNamespace = &quot;cloud&quot;def deploymentName = &quot;ms-cloud-tenant-service&quot;podTemplate( label: label, cloud: &#x27;kubernetes&#x27;, namespace: &#x27;jenkins&#x27;, containers: [ containerTemplate(name: &#x27;jnlp&#x27;, image: &#x27;192.168.17.132/rke/cicd/jenkins-slaver-dind:v1.1.5&#x27;, envVars: [envVar(key: &#x27;LANG&#x27;, value: &#x27;en_US.UTF-8&#x27;)],) ], volumes: [ configMapVolume(configMapName: &#x27;configmap-maven-setting-wdj&#x27;, mountPath: &#x27;/home/jenkins/maven&#x27;), configMapVolume(configMapName: &#x27;configmap-kubeconfig-dev&#x27;, mountPath: &#x27;/home/jenkins/kube&#x27;), hostPathVolume(hostPath: &#x27;/var/run/docker.sock&#x27;, mountPath: &#x27;/var/run/docker.sock&#x27;) ]) &#123; node(label) &#123; stage(&#x27;拉取代码&#x27;) &#123; git credentialsId: &#x27;ab716d57-9399-4978-bfb4-82eaccaea9d2&#x27;, url: &#x27;http://192.168.17.132:8080/cloud/ms-cloud-tenant-service.git&#x27; &#125; stage(&#x27;打jar包&#x27;) &#123; sh &#x27;mvn clean package -U -Dmaven.test.skip=true --settings /home/jenkins/maven/settings.xml&#x27; &#125; def dockerTag=new Date().format(&quot;yyyyMMddHHmmss&quot;) def dockerImageName = &quot;192.168.17.132/devops/ms-cloud-tenant-service:v$&#123;dockerTag&#125;&quot; stage(&#x27;构建docker镜像&#x27;) &#123; pwd() sh &quot;docker build -f docker/Dockerfile --tag=\\&quot;$&#123;dockerImageName&#125;\\&quot; .&quot; echo &quot;===&gt; finish build docker image: $&#123;dockerImageName&#125;&quot; sh &#x27;docker images&#x27; &#125; stage(&#x27;发布docker镜像&#x27;) &#123; withDockerRegistry(credentialsId: &#x27;1049ee9b-99fd-42df-8c40-a818fe66ae5a&#x27;, url: &#x27;http://192.168.17.132/&#x27;)&#123; sh &quot;docker push $&#123;dockerImageName&#125;&quot; sh &quot;docker rmi $&#123;dockerImageName&#125;&quot; sh &#x27;docker images&#x27; &#125; &#125; stage(&#x27;部署&#x27;) &#123; sh &quot;kubectl --kubeconfig=/home/jenkins/kube/config -n $&#123;deployNamespace&#125; patch deployment $&#123;deploymentName&#125; -p &#x27;&#123;\\&quot;spec\\&quot;: &#123;\\&quot;template\\&quot;: &#123;\\&quot;spec\\&quot;: &#123;\\&quot;containers\\&quot;: [&#123;\\&quot;name\\&quot;:\\&quot;$&#123;deploymentName&#125;\\&quot;, \\&quot;image\\&quot;:\\&quot;$&#123;dockerImageName&#125;\\&quot;&#125;]&#125;&#125;&#125;&#125;&#x27;&quot; echo &quot;==&gt; deploy $&#123;dockerImageName&#125; successfully&quot; &#125; &#125;&#125; 参考文档https://www.cnblogs.com/miaocunf/p/11694943.html jenkins job迁移Job Import Plugin导入（http://10.244.6.41:31000到本机192.168.17.131:31666） (1)首先到新的Jenkins上,在插件管理里先安装下Job Import Plugin，如下所示： (2)安装完后进入“Manage Jenkins” -&gt; “Configure System”下，找到Job Import Pluguin配置的地方，进行如下设置： name: 这个可以任意命名，这里我命名成要拷贝的Jenkins的IPUrl: 指要从哪里拷贝的Jenkins的URL,现在我们要从192.168.9.10拷贝job,因此这里要设置成192.168.9.10的Jenkins的URLCredentials：需要添加一个旧Jenkins的账号（也就是192.168.9.10的账号），没有添加的时候点击Add手动添加下，就可以像上面的截图一样下拉选择到这个账号了 (3)设置完后点击保存下，回到Jenkins首页点击Job Import Plugin就可以进行Job的迁移了，如下所示： 在Job Import Plugin界面，下拉选择刚才添加的配置，然后点击Query按钮就可以搜索出配置的Jenkins下的job了，然后选择需要的job进行迁移导入即可： (4)因为有时候旧的Jenkins上的插件新Jenkins上未必有，因此可以根据实际情况勾选是否需要安装必要的插件，如上面的截图所示，需不需要覆盖已有的job也根据实际情况勾选下。导入成功会有如下的提示： (5)有了上面的提示后就可以会到新的Jenkins的首页，查看Job有没有成功进入，并进入导入的job查看设置有没有成功的复制过来，如下所示： 可以看到job及其设置成功的被导入到新的job了。Job Import Pugin也支持多个job同时拷贝，如果旧的Job里有多个job，如上面的步骤里所示，query出来就有很多job可供选择，只需要勾选多个即可同时进行多个job的导入了。 Jenkins CLI方式导入有时候在公司内部Jenkins部署到不同的网段里，不同网段间可能会限制无法相互访问，这种情况下通过Job Import Plugin进行job导入的方式就行不通了，这时候可以通过Jenkins CLI方式进行job配置导出，然后新Jenkins在根据导出的配置进行再导入操作，完成job的配置迁移。下面我们来具体讲解下。 点击进入Jenkins CLI，可以看到Jenkins命令行接口提供很多命令可以用来进行Jenkins的相关操作，可以看到有提供了get-job这样一个命令，这个命令可以将job的定义导出到xml的格式到输出流，这样我们可以通过这个命令将旧Jenkins上的job导出到外部文件，然后还可以看到有另外一个命令create-job，这个命令可以根据已有的xml配置文件进行job创建，那我们可以根据从旧job导出的job配置文件做为输入进行job的创建了。(1)首先在旧的Jenkins上的cli页面点击jenkins-cli.jar就可以下载这个jar到本地，如下所示： (2)接着点击下Jenkins右上角的账号，选择Configure，然后点击Show API Token，拷贝token，这个token可以用来进行配置导出的时候做为认证使用 (3)在jenkins-cli.jar下载的根目录下执行如下命令进行job导出，这里我新建了个job，命名为test4，现在执行下如下命令进行test4这个job配置的导出： 12java -jar jenkins-cli.jar -s http://192.168.9.10:8080/jenkins -authadmin:493375c06bc0006a455005804796c989 get-job &quot;test4&quot; &gt; test4.xml http://192.168.9.10:8080/jenkins 就Job的Jenkins地址 admin： 上面截图获取Show API Token下的User ID 493375c06bc0006a455005804796c989：上面截图获取API Token的值 test4: 需要导出配置的job名 test4.xml: 导出的文件的名称，可任意 根据实际情况替换下上面的四个值即可执行完上面的命令就可以看到test4.xml文件生成了 (4)接着在新的Jenkins下同样先下载下jenkins-cli.jar，然后将上面生成的test4.xml拷贝到新的Jenkins机器下，同样获取下新Jenkins登录账号的API Token和User ID，执行下如下命令就可以进行job导入了 12java -jar jenkins-cli.jar -s http://192.168.9.8:8080/jenkins -authadmin:51964e7b89a427be5dd2a28f38c86eff create-job &quot;test4&quot; &lt; test4.xml 记得将URL替换成新Jenkins的URL，User ID和token也替换下上面的命令执行完后，就可以看到在新的Jenkins下新job被成功导入了 参考自：https://cloud.tencent.com/developer/article/1470433 Job 批量删除点击系统管理-》脚本命令行-》脚本命令行-》输入代码，点击运行 12345678910111213def jobName = &quot;devops-ci-ms-cloud-tenant-service&quot;def maxNumber = 75Jenkins.instance.getItemByFullName(jobName).builds.findAll &#123;it.number &lt; maxNumber&#125;.each &#123;it.delete()&#125; 配置jenkins的credentialsId即生产连接gitlab、harbor的credentialsId(1)设置jenkins连接gitlab的git credentialsId 出错了 成功的图 可能原因分析：这是由于git客户端版本过低造成的！解决方案：https://blog.csdn.net/wudinaniya/article/details/97921383安装了2.16.5(2)生成jenkins连接harbor的credentialsId 利用GitLab webhook来触发Jenkins构建参考自：https://www.cnblogs.com/zblade/p/9480366.html本文针对如何设置GitLab以及Jenkins，实现每次GitLab上有合并事件的时候，都能触发Jenkins执行相应的操作，主要分为以下几个步骤： 新建GitLab测试项目进入个人GitLab账号，在右上角的加号中，选出GitLab 的 New Project，可以新建个人的GitLab工程： 其余都走默认的设置，填写好project的名字，就可以创建一个新的project，如图： 新建Jenkins的job（1）首先验证是否安装了GitLab plugin在“系统管理”-&gt;“插件管理”，查看已安装插件，输入 GitLab，看看是否已经安装，如果没有，则 查看 可选插件，搜索 GitLab，安装后重启即可。（2）新建一个jenkins测试job，如图：源码管理选择Git, 输入刚刚新建的GitLab的 URL以及个人的API_TOKEN:目前只有master分支，后续可以根据不同分支对应设置不同的url，监听不同分支的情况。在构建触发器选项中，勾选 Build when a change is pushed to GitLab，该选项最后的URL就是这个工程的URL路径，注意如果是本机，则会显示localhost，可以将localhost改为个人的ip。注意这个url，下一步会用到这个url。点击应用和保存。 设置GitLab的webhook在gitlab的当前工程-&gt;settings-&gt;integretions，使用当前工程owner角色的账号制作钩子。将上一步中的url和token填入，选择merge事件。当owner进行合并代码时自动触发jenkins构建操作。 点击Add webhook后,可能会报Url is blocked: Requests to the local network are not allowed 错误原因：gitlab 10.6版本以后为了安全，不允许向本地网络发送webhook请求 解决方案：最上面一排的扳手设置按钮—&gt;进入左侧 设置—-&gt;网络—-&gt;选择允许webhooks和本机网络交互 点击save changes后，再次回到当前工程-&gt;settings-&gt;integretions中重新制作钩子，然后点击Edit: 点击Test-&gt;Push events: 查看事件状态,若为200则钩子制作成功： 可能出现的其它问题(1)不允许匿名构建问题 出现上述问题，原因就是不支持匿名build，回到jenkins中，在 系统管理-&gt;全局安全管理中，勾选匿名用户具有可读权限 如图： 然后点击应用和保存， 回到GitLab，继续测试.如果继续报该错，则进入刚刚构建的工程，点击构建触发器中选中的Build When a change is pushed右下角的高级选项，有一个Secret token，点击Generate，会生成一个安全代码： 复制到webhook中的url下面： 然后保存，再测试，就可以通过，这时候会触发jenkins执行一次操作： 看看控制台输出： 参考自https://www.cnblogs.com/zblade/p/9480366.html 附录 遇到的问题(1)集群中的每个节点不能直接拉取公有harbor上的镜像 解决方案：必须把所有的镜像都拉到所有节点的本地？不是的，需要把harbor仓库的rke项目设为公有仓库。 (2)在129机器上运行.&#x2F;rke up 出现下图问题 解决方案：129机器没有自己对自己做免密 (3)dashboard页面没有图形化的统计数据 解决方案： 把–source改为图中红框内的配置。 （4）jenkins-master在192.168.17.131起不来 在131机器上创建目录mkdir &#x2F;jenkins-data 并修改权限chown -R 1000:1000 &#x2F;jenkins-data","tags":[{"name":"CI/CD","slug":"CI-CD","permalink":"https://wangdj104.github.io/tags/CI-CD/"}]},{"title":"配置本地yum源","date":"2019-09-03T07:43:05.000Z","path":"2019/09/03/配置本地yum源/","text":"本地yum源配置下载地址：https://pan.baidu.com/s/1Q0wIDlXXQu1-map1iZ_VYg 提取码: wsvq(1)准备ISO源，挂载CentOS-7-x86_64-Everything-1511.iso，把里面所有文件都拷贝到本地目录&#x2F;develop&#x2F;yum（自己创建）(2)再新建一个目录mkdir -p &#x2F;home&#x2F;mcloud&#x2F;centos-yum(3)将iso文件挂载到新建目录&#x2F;home&#x2F;mcloud&#x2F;centos-yum下（相当于解压iso文件）,挂载成功后如图所示。 1mount -o rw CentOS-7-x86_64-Everything-1511.iso /home/mcloud/centos-yum (4)将挂载到&#x2F;home&#x2F;mcloud&#x2F;centos-yum下的ISO文件解压后的所有文件拷贝到&#x2F;develop&#x2F;yum下。 1cp -r /home/mcloud/centos-yum/* /develop/yum (5)编辑&#x2F;etc&#x2F;yum.repos.d&#x2F;底下repo文件。 123456[self_packages]name=self_packagesbaseurl=file:///develop/yumgpgcheck=1enabled=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7 (6)yum clean all(7)yum makecache 局域网yum源(1)安装nginx，准备nginx-1.10.2.tar.gz包，并解压（在10.170.200.6上安装） 1tar -zxvf nginx-1.10.2.tar.gz (2)进入nginx目录 cd nginx-1.10.2(3)安装编译nginx 1./configure &amp;&amp; make &amp;&amp; make install (4)编辑nginx.conf，配置nginx，nginx路径为&#x2F;usr&#x2F;local&#x2F;nginx，如需更改nginx默认的80端口，只需更改listen即可。location &#x2F; {root &#x2F;develop&#x2F;yum&#x2F;;autoindex on;}(5)启动nginx服务 12345cd /usr/local/nginx/sbin/./nginx./nginx -s stop./nginx -s quit./nginx -s reload 备注：.&#x2F;nginx -s quit:此方式停止步骤是待nginx进程处理任务完毕进行停止。.&#x2F;nginx -s stop:此方式相当于先查出nginx进程id再使用kill命令强制杀掉进程。.&#x2F;nginx -s reload：当nginx的配置文件nginx.conf修改后，要想让配置生效需要重启nginx，使用.&#x2F;nginx -s reload不用先停止nginx再启动nginx即可将配置信息在nginx 生效。(6)查看nginx进程ps aux|grep nginx (7)配置nginx开机自启动切换到&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;目录，创建nginx.service文件 123456789101112131415161718vim nginx.service# cd /lib/systemd/system/# vim nginx.service文件内容如下：[Unit]Description=nginxAfter=network.target[Service]Type=forkingExecStart=/usr/local/nginx/sbin/nginxExecReload=/usr/local/nginx/sbin/nginx reloadExecStop=/usr/local/nginx/sbin/nginx quitPrivateTmp=true[Install]WantedBy=multi-user.target 退出并保存文件，执行systemctl enable nginx.service使nginx开机启动 123systemctl start nginx.service 启动nginxsystemctl stop nginx.service 结束nginxsystemctl restart nginx.service 重启nginx (8)准备ISO源，挂载CentOS-7-x86_64-Everything-1511.iso，把里面所有文件都拷贝到本地目录&#x2F;develop&#x2F;yum（自己创建）(9)再新建一个目录mkdir &#x2F;home&#x2F;mcloud&#x2F;centos-yum(10)将iso文件挂载到新建目录&#x2F;home&#x2F;mcloud&#x2F;centos-yum下,挂载成功后如图所示。 1mount CentOS-7-x86_64-Everything-1511.iso /home/mcloud/centos-yum (11)将挂载到&#x2F;home&#x2F;mcloud&#x2F;centos-yum下的ISO文件解压后文件中的Packages文件中的所有rpm包拷贝到&#x2F;develop&#x2F;yum下。 1cp -r /home/mcloud/centos-yum/Packages/* /develop/yum (12)需要提前安装createrepo 1yum install createrepo (13)createrepo &#x2F;develop&#x2F;yum 更新yum目录，会在&#x2F;develop&#x2F;yum底下生产一个repodata目录如果添加或者删除了个人的rpm包，不需要再次重新create，浪费时间，只需要createrepo –update &#x2F;develop&#x2F;yum就可以了。(14)编辑&#x2F;etc&#x2F;yum.repos.d&#x2F;底下repo文件。 123456[core-0]name=core-0baseurl=http://10.170.200.6:8088enabled=1gpgcheck=0priority=1 (15) 12yum clean allyum makecache","tags":[{"name":"yum源","slug":"yum源","permalink":"https://wangdj104.github.io/tags/yum%E6%BA%90/"}]},{"title":"Idap之二进制安装","date":"2019-08-29T15:14:59.000Z","path":"2019/08/29/Idap服务/","text":"第一章 LDAP认证在上图中，LDAP Client指各种需要身份认证的软件，例如Apache、Proftpd和Samba等。LDAP Sever指的是实现LDAP协议的软件，例如OpenLDAP等。LDAP Directory Services指的是OpenLDAP的数据存储，如关系型数据库（MySQL）或查询效率更高的嵌入式数据库（BerkeleyDB），甚至是平面文本数据库（—个txt的文本文件）。可见，OpenLDAP软件只是LDAP协议的一种实现形式，并不包括后台数据库存储。但在很多时候管理员经常将LDAP Server和DataStorage放在同一台服务器，这样就产生了人们通常所说的“LDAP数据库”。虽然后台数据库（backend）可以是多种多样，但LDAP协议还规定了数据的存储方式。LDAP数据库是树状结构的，与DNS类似，如下图所示：在上图中，以这种方式存储数据最大的一个好处就是查询速度快，LDAP数据库专门对读操作进行了优化， OpenLDAP配合Berkeley DB可使其读操作的效率得到很大提高。LDAP数据库的树状结构的另一个好处是便于分布式的管理。 第二章 Harbor用户模型用户模型Harbor登录用户有管理员和普通用户两种类型管理员模型： 123456789dn: uid=wangdj104,ou=admin,cn=devops-dev-gitlab,dc=jichen,dc=chinaunicomcn:: 546L6YGT5rGf #加密后的用户全名 此为王..employeetype: admin #标志为管理员mail: 1138523117@qq.com #邮箱必填objectclass: inetOrgPersonobjectclass: topsn:: 546L #加密后的用户姓 此为王uid: wangdj104userpassword: &#123;MD5&#125;DyoNF/7H+m7ndqjlKtr12w== #md5加密后的用户密码 此为111111 普通用户模型： 123456789dn: uid=zhangj1143,ou=users,cn=devops-dev-gitlab,dc=jichen,dc=chinaunicomcn:: 5byg5Yab #加密后的用户全名 此为张..employeetype: user #标志为普通用户mail: 23659874@qq.com #邮箱必填objectclass: inetOrgPersonobjectclass: topsn:: 5byg #加密后的用户姓 此为张uid: zhangj1143userpassword: &#123;MD5&#125;DyoNF/7H+m7ndqjlKtr12w== #md5加密后的用户密码 此为111111 用户模型的建立ldap环境搭建好后，登录界面化管理工具phpldapadmin后，显示如下。然后开始导入用户组和用户。1）增加基础dn（相当于集团） 12345dn: dc=jichen,dc=chinaunicomdc: jicheno: jichen #以上三项需和上图保持一致objectclass: dcObjectobjectclass: organization 2）导入组（cn,相当于分公司） 12345dn: cn=devops-dev- gitlab,dc=jichen,dc=chinaunicomcn: devops-dev-gitlab #设置的组名称gidnumber: 500 #可设其它值objectclass: posixGroupobjectclass: top 3）导入组织单元（ou,相当于部门） 123456789dn: ou=admin,cn=devops-dev-gitlab,dc=jichen,dc=chinaunicomobjectclass: organizationalUnitobjectclass: topou: admin #管理部门dn: ou=users,cn=devops-dev-gitlab,dc=jichen,dc=chinaunicomobjectclass: organizationalUnitobjectclass: topou: users #普通用户 4）导入员工 12345678910111213141516171819dn: uid=wangdj104,ou=admin,cn=devops-dev-gitlab,dc=jichen,dc=chinaunicomcn:: 546L6YGT5rGf #加密后的用户全名 此为王道江employeetype: admin #标志为管理员mail: 1138523117@qq.com #邮箱必填objectclass: inetOrgPersonobjectclass: topsn:: 546L #加密后的用户姓 此为王uid: wangdj104userpassword: &#123;MD5&#125;DyoNF/7H+m7ndqjlKtr12w== #md5加密后的用户密码 此为111111dn: uid=zhangj1143,ou=users,cn=devops-dev-gitlab,dc=jichen,dc=chinaunicomcn:: 5byg5Yab #加密后的用户全名 此为王道江employeetype: user #标志为普通用户mail: 23659874@qq.com #邮箱必填objectclass: inetOrgPersonobjectclass: topsn:: 5byg #加密后的用户姓 此为张uid: zhangj1143userpassword: &#123;MD5&#125;DyoNF/7H+m7ndqjlKtr12w== #md5加密后的用户密码 此为111111 用户权限的管理使用上述建立的用户登录harbor,页面如下：修改其权限为管理员，使用admin账号登录harbor再次登录wangdj104账户，可以有更多的权限 第三章 安装Berkeley DBBerkeley DB(BDB)是OpenLDAP后台数据库的默认配置，因此在安装OpenLDAP之前应先安装BDB安装包下载链接: https://pan.baidu.com/s/12TvaQRXADay2OvtiVOcLBg 提取码: khgw 1234cd db-5.3.28/build_unix/../dist/configure -prefix=/usr/local/BerkeleyDB(最好是装在此目录下，否则安装ldap时会报lib是一个目录的错误)makemake install 第四章 OpenLDAP安装linux搭建开源ldap服务器方法参考博文：https://blog.csdn.net/u010783533/article/details/88572697https://blog.csdn.net/developerinit/article/details/77170569https://blog.csdn.net/muriyue6/article/details/82845523https://www.cnblogs.com/MYSQLZOUQI/p/4840965.html Idap安装ldap是统一认证服务，它的优点是存储用户认证等不经常改变的信息，有清晰的组织结构。本文使用的是openldap-2.4.47.tgz。安装包下载链接：ftp://ftp.openldap.org/pub/OpenLDAP/openldap-release（1）开始前需设置编译参数，如下所示 123export CPPFLAGS=&quot;-I/usr/local/BerkeleyDB/include&quot;export LDFLAGS=&quot;-L/usr/local/BerkeleyDB/lib&quot;export LD_LIBRARY_PATH=&quot;/usr/local/BerkeleyDB/lib&quot; （2） 1./configure --prefix=/home/deployer/idap/ldap （3） 1make depend Make depend是一种makefile的规则，通过扫描各个目录下的所有C&#x2F;C++ 代码，从而判断出文件之间的依赖关系，如a.cc文件中调用了b.h(如以形势include&lt;b.h&gt;)，如果之后a.cc文件被改动，那 么只需要重新编译a.cc文件，不需要编译b.h文件。否则所有的文件都需要重新编译（4） 123makemake test（需要一段时间，可选操作）make install 安装完成后：etc目录显示的是配置文件，bin目录显示的ldap的客户端工具，sbin目录显示的是服务器相关执行文件，libexec下是ldap服务启动程序，.ldif是ldap特定的文件格式，这种格式的文件用于ldap数据的添加。 配置Idap（1）设置LDAP使用的Schema编辑slapd.conf 1vim /home/deployer/idap/ldap/etc/openldap 在文件中找到如下语句。 1include /home/deployer/idap/ldap/etc/schema/core.schema 在该语句的后面添加以下语句。 12345678include /home/deployer/idap/ldap/etc/openldap/schema/corba.schemainclude /home/deployer/idap/ldap/etc/openldap/schema/cosine.schemainclude /home/deployer/idap/ldap/etc/openldap/schema/dyngroup.schemainclude /home/deployer/idap/ldap/etc/openldap/schema/inetorgperson.schemainclude /home/deployer/idap/ldap/etc/openldap/schema/java.schemainclude /home/deployer/idap/ldap/etc/openldap/schema/misc.schemainclude /home/deployer/idap/ldap/etc/openldap/schema/nis.schemainclude /home/deployer/idap/ldap/etc/openldap/schema/openldap.schema (2)为目录树设置后缀在slapd.conf文件中找到如下语句:suffix “dc&#x3D;my-domain, dc&#x3D;com”将其改为如下内容。suffix “dc&#x3D;my-test, dc&#x3D;chinaunicom” (3)为LDAP服务器管理员设置DN在slapd.conf文件中找到如下语句:rootdn “cn&#x3D;Manager, dc&#x3D;my-domain, dc &#x3D; com”将其改为如下内容。rootdn “cn&#x3D;Manager, dc&#x3D;my-test, dc &#x3D; chinaunicom” (4)为LDAP服务器管理员设置口令 1）生成加密密码： 12345cd /home/deployer/idap/ldap/sbin[root@localhost sbin] # ./slappasswdNew password:Re-enter new password:&#123;SSHA&#125;YARtaZb0fjq3B2gJuflulDS7vezL7O1p 2)设置管理员密码在slapd.conf文件中找到如下语句：rootpw secret将其改为如下内容。rootpw {SSHA}YARtaZb0fjq3B2gJuflulDS7vezL7O1p(5)初始化OpenLDAP（可选） 12[root@instance-0pk09gjj /]# cd /home/deployer/idap/ldap/var/openldap-data/[root@instance-0pk09gjj openldap-data]# cp DB_CONFIG.example DB_CONFIG (6)启动LDAP服务器要启动LDAP服务器，只需执行以下命令。 1nohup /home/deployer/idap/ldap/libexec/slapd -f /home/deployer/idap/ldap/etc/openldap/slapd.conf -d 255 &amp; 然后执行以下命令确保sland进程启动成功。执行结果如下所示，显示“|-slapd”表示LDAP服务器已经成功启动。 12[root@localhost ~]# pstree|grep &quot;slapd&quot; |-slapd -----&#123;slapd&#125;/ (7)创建一个ldif文件 123456789101112[root@localhost bin]# cd /home/deployer/idap/ldap/etc/openldap[root@localhost bin]# vim /home/deployer/idap/ldap/etc/openldap/example.ldif[root@localhost bin]# cat /home/deployer/idap/ldap/etc/openldap/example.ldifdn:dc=my-test,dc=chinaunicomobjectclass:dcObjectobjectclass:organizationdc:my-testo:my-test,Inc.dn:ou=Harbor,dc=my-test,dc=chinaunicomobjectclass:organizationalUnitou:Harbor (8) 添加添加ldif文件 1/home/deployer/idap/ldap/bin/ldapadd -x -D &#x27;cn=Manager,dc=my-test,dc=chinaunicom&#x27; -W -f /home/deployer/idap/ldap/etc/openldap/example.ldif 第五章 phpLDAPadmin安装配置安装博客：http://blog.sina.com.cn/s/blog_681e31a20102vqdc.html安装包下载链接：https://pan.baidu.com/s/1fEu-CENRJ5ja8knHQzyuZw 提取码: fgij 安装Apache 下载apache版本httpd-2.2.26.tar.gz，并下载相关的支持组件包 apr-1.5.0.tar.gz, apr-util-1.5.3.tar.gz（1）解压apache及相关组件包源码 123# tar -zxvf httpd-2.2.26.tar.gz# tar -zxvf apr-1.5.0.tar.gz# tar -zxvf apr-util-1.5.3.tar.gz （2）安装apr 1234# cd apr-1.5.0# ./configure --prefix=/usr/local/apr# make# make install （3）安装apr-util 1234# cd apr-util-1.5.3# ./configure --prefix=/usr/local/apr-util --with-apr=/usr/local/apr# make# make install （4）安装apache 12345#cd httpd-2.2.26# ./configure --prefix=/usr/local/apache --enable-so --enable-rewrite --with-apr=/usr/local/apr --with-apr-util=/usr/local/apr-util//安装到指定目录，so开启动态模块支持，rewrite开启地址重写。# make;#make install 安装php5(1) 安装libxml2获取php支持组件libxml2-2.7.8.tar.gz安装如下： 12345#tar -zxvf libxml2-2.7.8.tar.gz#cd libxml2-2.7.8#./configure --prefix=/usr/local/libxml2#make#make install 安装成功以后，在&#x2F;usr&#x2F;local&#x2F;libxml2&#x2F;目录下将生成bin、include、lib、man和share五个目录。在后面安装PHP5源代码包的配置时，会通过在configure命令的选项中加上”–with-libxml-dir&#x3D;&#x2F;usr&#x2F; local&#x2F;libxml2”选项，用于指定安装libxml2库文件的位置。(2) 安装php5 12345678# tar -zxvf php-5.6.0.tar.gz# cd php-5.6.0# ./configure --prefix=/usr/local/php5 \\ --with-apxs2=/usr/local/apache/bin/apxs \\ --with-ldap=/home/deployer/idap/ldap \\ --with-config-file-path=/usr/local/php5 \\ --with-libxml-dir=/usr/local/libxml2 \\ --with-gettext &#x2F;&#x2F;指定安装目录，跟apache融合，指定配置文件位置，启用ldap支持（必须的），启用 gettext支持（必须要启用的支持）注：出现以下信息则配置环境成功（.&#x2F;configure），才可继续输入安装命令make和make install。 1234 make make install cp php.ini-development /usr/local/php5/php.ini//把配置文件拷贝过去,编辑它，在适当位置添加include_path = /usr/local/php5/lib/php 配置Apache(1) 配置apache主文件httpd.conf有三个地方需要修改 1vim /usr/local/apache/conf/httpd.conf (2) 配置apache支持php 1vim /usr/local/apache/conf/httpd.conf 在AddType application&#x2F;x-gzip .gz .tgz 下添加如下内容在DirectoryIndex index.html 后添加如下内容 测试Apche +PHP安装是否成功启动apache，命令行输入 12cd /usr/local/apache/bin./apachectl start 在浏览器输入IP ： http://192.168.117.132，显示如下则成功安装apache 安装配置phpLDAPadmin(1)安装phpldapadmin 1234tar zxvf phpldapadmin-1.2.3.tgzmv phpldapadmin-1.2.3 /usr/local/phpldapadmincd /usr/local/phpldapadmin/configcp config.php.example config.php openldapadmin的安装很简单，只需要解压，然后移动到相应的目录就可以了。默认情况下phpldapadmin自带了一个示例配置文件config.php.example，对该文件稍做修改，就可以使用了。（2）配置config.conf 1vim config.php 注：上面的array（’’）里面需要跟之前oepnldap的配置文件slapd.conf里的suffix的值一致注：登陆的账号和密码设置为空，则网页弹出需要手动输入（账号为slapd.conf的rootdn，密码是rootpw）；若想绑定账号，则绑定密码项绑定与否都可以(3)建立phpldapadmin虚拟目录及设置用户认证。在httpd.conf文件末添加下图所示内容，建立虚拟目录设置admin密码 123 mkdir -p /etc/apach2 cd /etc/apach2/usr/local/apache/bin/htpasswd -c php_ldap_admin_pwd admin (4)配置phpLDAPadmin使用中文1)编辑文件&#x2F;usr&#x2F;local&#x2F;phpldapadmin&#x2F;config&#x2F;config.php，修改语句：&#x2F;&#x2F; $config-&gt;custom-&gt;appearance[‘language’] &#x3D; ‘auto’;将“&#x2F;&#x2F;”注释符号去处，并将语句改为：$config-&gt; custom-&gt;appearance[‘language’] &#x3D; ‘zh_CN’;2)使用下列的命令转换phpLDAPadmin语言文件的编码。 12iconv -f gbk -t utf8 /usr/local/phpldapadmin/locale/zh_CN/LC_MESSAGES/messages.mo &gt; /usr/local/phpldapadmin/locale/zh_CN/LC_MESSAGES/messages.new.pomsgfmt -o /usr/local/phpldapadmin/locale/zh_CN/LC_MESSAGES/messages.mo /usr/ local/phpldapadmin/locale/zh_CN/LC_MESSAGES/messages.new.po (5)启动phpldapadmin保存上述httpd.conf文件，重启Apache服务 12cd /usr/local/apache/bin./apachectl restart 浏览器输入：http://192.21.11.132/phpldapadmin/进行访问。如下图注：在建立phpldapadmin虚拟目录及设置用户认证后，在浏览器打开phpldapadmin出现下图错误，phpldapadmin&#x2F;lib&#x2F;functions.php 是下载安装包的文件，不应有错。在官网查了下（http://sourceforge.net/u/nihilisticz/phpldapadmin/ci/7e53dab990748c546b79f0610c3a7a58431e9ebc/ ），原来是软件版本更新，几个文件都需要修改，照着上面依次修改了lib&#x2F;PageRender.php、lib&#x2F;ds_ldap.php、 lib&#x2F;ds_ldap.php 、lib&#x2F;functions.php 、lib\\TemplateRender.php 里面的函数及相应代码，最后运行成功。 第六章 Harbor集成LDAP默认配置文件在harbor.cfg，我们可以先不添加配置，直接在harbor web界面进行配置，也可两者同时设置。 LDAP账号生成Openldap是一个统一的账户管理工具，只进行用户的管理，不进行权限的管理，权限管理在各自的组件中进行。搭建phpldapadmin界面化管理工具，创建组、组织单元、账户：1） 创建一个组点击dc&#x3D;my-test,dc&#x3D;chinaunicom-》创建一个子条目-》默认-》posixGroup-》RDN选择cn,cn框填写组名-》创建对象，本例为devops-dev-harbor;2） 创建一个组织单元ou点击cn&#x3D;devops-dev-harbor-》创建一个子条目-》默认-》organizationalUnit-》RDN选择ou,ou框填写单元名-》创建对象3） 创建一个用户点击ou&#x3D;users-》创建一个子条目-》默认-》inetOrgPerson-》RDN选择User Name(uid)-》cn框填写用户名，sn框填写中文名，设置密码，User Namevim harbor.cfg再harbor界面设置认证方式为ldap另外，harbor1.6版本，还可以进行组的管理。然后点击组管理。新增组配置完成后，即可使用创建的账户登录harbor注意：初次使用ldap的账户登录harbor后，认证模式就不可更改了。并且在用户管理中，可以看到刚刚登录的ldap用户（admin用户下） 权限设置Openldap创建的用户是普通的用户，若想将该用户设置成管理员，需要登录harbor的超级管理员（admin）进行设置。普通用户的页面如下： 第七章 Sonarqube集成ldap参考博客https://blog.csdn.net/neufeng2010/article/details/99655942https://blog.csdn.net/BeauXie/article/details/81157330https://blog.csdn.net/qq_21816375/article/details/80787993 安装启动sonarqube服务安装包下载地址：https://www.sonarqube.org/downloads/。版本要求7.2.1以上 修改sonar配置文件sonar.properties 1$ vim /home/deployer/sonarqube-7.2.1/conf/sonar.properties 修改如下(请将具体信息按照自己的LDAP服务器信息进行修改): 12345678910111213141516171819# LDAP CONFIGURATION# Enable the LDAP featuresonar.security.realm=LDAP //取消注释# Set to true when connecting to a LDAP server using a case-insensitive setup.# sonar.authenticator.downcase=true# URL of the LDAP server. Note that if you are using ldaps, then you should install the server certificate into the Java truststore.ldap.url=ldap://192.168.117.132:389 //取消注释，填写实际ldap服务器地址# Bind DN is the username of an LDAP user to connect (or bind) with. Leave this blank for anonymous access to the LDAP directory (optional)ldap.bindDn=cn=Manager,dc=my-test,dc=chinaunicom //取消注释，填写实际ldap服务器基础DN# Bind Password is the password of the user to connect with. Leave this blank for anonymous access to the LDAP directory (optional)ldap.bindPassword=111111 //取消注释，填写实际ldap服务器基础DN密码# Possible values: simple | CRAM-MD5 | DIGEST-MD5 | GSSAPI See http://java.sun.com/products/jndi/tutorial/ldap/security/auth.html (default: simple)ldap.authentication=simple //取消注释 启动并查看启动日志.&#x2F;sonar.sh console（可以实时查看sonarqube的启动日志）如果报错 123456789101112131415Running SonarQube...wrapper | --&gt; Wrapper Started as Consolewrapper | Launching a JVM...jvm 1 | Wrapper (Version 3.2.3) http://wrapper.tanukisoftware.orgjvm 1 | Copyright 1999-2006 Tanuki Software, Inc. All Rights Reserved.jvm 1 |jvm 1 | 2019.08.19 07:44:17 INFO app[][o.s.a.AppFileSystem] Cleaning or creating temp directory /home/deployer/sonarqube-7.2.1/tempjvm 1 | 2019.08.19 07:44:17 INFO app[][o.s.a.es.EsSettings] Elasticsearch listening on /192.168.117.130:44172jvm 1 | 2019.08.19 07:44:17 INFO app[][o.s.a.p.ProcessLauncherImpl] Launch process[[key=&#x27;es&#x27;, ipcIndex=1, logFilenamePrefix=es]] from [/home/deployer/sonarqube-7.2.1/elasticsearch]: /home/deployer/sonarqube-7.2.1/elasticsearch/bin/elasticsearch -Epath.conf=/home/deployer/sonarqube-7.2.1/temp/conf/esjvm 1 | 2019.08.19 07:44:17 INFO app[][o.s.a.SchedulerImpl] Waiting for Elasticsearch to be up and runningjvm 1 | 2019.08.19 07:44:17 INFO app[][o.e.p.PluginsService] no modules loadedjvm 1 | 2019.08.19 07:44:17 INFO app[][o.e.p.PluginsService] loaded plugin [org.elasticsearch.transport.Netty4Plugin]jvm 1 | 2019.08.19 07:44:19 WARN app[][o.s.a.p.AbstractProcessMonitor] Process exited with exit value [es]: 1jvm 1 | 2019.08.19 07:44:19 INFO app[][o.s.a.SchedulerImpl] Process [es] is stoppedjvm 1 | 2019.08.19 07:44:19 INFO app[][o.s.a.SchedulerImpl] SonarQube is stopped 出现上述错误原因是不能用root用户启动es创建sonarUser用户并赋权 12345678[root@i-vzdytl5t linux-x86-64]# adduser sonarUser[root@i-vzdytl5t linux-x86-64]# passwd sonarUserChanging password for user sonarUser.New password:BAD PASSWORD: The password is shorter than 8 charactersRetype new password:passwd: all authentication tokens updated successfully.[root@i-vzdytl5t linux-x86-64]# chown -R sonarUser:sonarUser sonarqube-7.2.1 再次启动sonarqube，若出现下图错误删除sonarqube-7.2.1&#x2F;temp目录下的所有文件即可如果出现下述错误切换到root用户，编辑limits.conf添加如下内容 123vim /etc/security/limits.conf* soft nofile 65536* hard nofile 65536 切换到root用户，编辑sysctl.conf 123vim /etc/sysctl.conf在文件最后添加一行vm.max_map_count=262144 重新启动即可，出现下图即为成功安装了集成LDAP的sonarqube打开sonar网址，输入LDAP中的用户名和密码后点击登录 汉化点击界面上的Adminstration,然后选择Marketplace，在Plugins一栏，搜索Chinese Pack，然后点击insatll进行安装，如下所示:安裝完成以后，点击页面上Restart按钮，重启服务器，如下图所示:等待一会儿，重启完毕以后，会自动跳转到登录界面。输入admin&#x2F;admin登录以后，便会看到汉化成功:至此，SonarQube整合LDAP完成 第八章 Gitlab集成ldap修改gitlab.rb文件如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445gitlab_rails[&#x27;ldap_enabled&#x27;] = true###! **remember to close this block with &#x27;EOS&#x27; below**gitlab_rails[&#x27;ldap_servers&#x27;] = YAML.load &lt;&lt;-&#x27;EOS&#x27; main: # &#x27;main&#x27; is the GitLab &#x27;provider ID&#x27; of this LDAP server label: &#x27;LDAP&#x27; host: &#x27;_your_ldap_server&#x27; port: 389 uid: &#x27;uid&#x27; bind_dn: &#x27;cn=Manager,dc=jichen,dc=chinaunicom&#x27; password: &#x27;111111&#x27; encryption: &#x27;plain&#x27;# &quot;start_tls&quot; or &quot;simple_tls&quot; or &quot;plain&quot;# verify_certificates: true# active_directory: true allow_username_or_email_login: false# lowercase_usernames: false# block_auto_created_users: false base: &#x27;cn=devops-dev-gitlab,dc=jichen,dc=chinaunicom&#x27;# user_filter: &#x27;&#x27;# ## EE only# group_base: &#x27;&#x27;# admin_group: &#x27;&#x27;# sync_ssh_keys: false## secondary: # &#x27;secondary&#x27; is the GitLab &#x27;provider ID&#x27; of second LDAP server# label: &#x27;LDAP&#x27;# host: &#x27;_your_ldap_server&#x27;# port: 389# uid: &#x27;sAMAccountName&#x27;# bind_dn: &#x27;_the_full_dn_of_the_user_you_will_bind_with&#x27;# password: &#x27;_the_password_of_the_bind_user&#x27;# encryption: &#x27;plain&#x27; # &quot;start_tls&quot; or &quot;simple_tls&quot; or &quot;plain&quot;# verify_certificates: true# active_directory: true# allow_username_or_email_login: false# lowercase_usernames: false# block_auto_created_users: false# base: &#x27;&#x27;# user_filter: &#x27;&#x27;# ## EE only# group_base: &#x27;&#x27;# admin_group: &#x27;&#x27;# sync_ssh_keys: false EOS","tags":[{"name":"Idap","slug":"Idap","permalink":"https://wangdj104.github.io/tags/Idap/"}]},{"title":"Pipeline script 自动化测试流程配置","date":"2019-08-07T14:32:04.000Z","path":"2019/08/07/Pipeline-script-自动化测试流程配置/","text":"操作系统windows 基础配置Git安装配置下载地址：https://git-scm.com/downloads安装（推荐）：https://git-scm.com/book/zh/v1/ Jdk安装配置下载地址：https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html安装（推荐）：https://www.cnblogs.com/tiankong101/p/4226559.html Tomcat安装配置下载地址：https://tomcat.apache.org/download-90.cgi安装（推荐）：解压到指定目录下即可 Robot framework 配置1)Python2.7安装配置下载地址：https://www.python.org/downloads/ 2)ride安装配置下载安装（推荐）：https://www.cnblogs.com/yinrw/p/5837828.html Jenkins 配置下载地址：https://jenkins.io/download/下载jenkins.war,放在tomcat下，启动tomcat配置（推荐）：https://www.cnblogs.com/c9999/p/6399367.html 在jenkins-master节点的jenkins上安装Robot Framework插件如果在线安装插件报下图错误，请尝试离线安装插件下载地址：http://mirrors.jenkins-ci.org/plugins/robot/latest/ jenkins-slave-windows工具安装安装谷歌浏览器安装git客户端安装Gpg4win jenkins-master上windows从节点的配置参考 https://www.jianshu.com/p/20124cdee8e9 https://www.jianshu.com/p/ef8d3109ac5f http://blog.sina.com.cn/s/blog_13cc013b50102wiau.html https://www.jianshu.com/p/1d0eb3fff364配置环境变量将从节点上的环境变量Path框中的值复制粘贴到Node Properties中的新增环境变量中 1%SystemRoot%\\system32;%SystemRoot%;%SystemRoot%\\System32\\Wbem;%SYSTEMROOT%\\System32\\WindowsPowerShell\\v1.0;C:\\Python27;C:\\Python27\\Scripts;C:\\station\\Git\\cmd;%JAVA_HOME%\\bin;%JAVA_HOME%\\jre\\bin;C:\\station\\Gpg4win\\..\\GnuPG\\bin;C:\\Google\\Chrome\\Application;C:\\Windows\\System32 修改slave-agent.jnlp下载地址Jenkins -&gt; ”系统管理” -&gt; “系统设置”，如下，找到“Jenkins Location” -&gt; “Jenkins URL”,把其中的URL地址改成jenkins-master的访问网址点击Launch下载slave-agent.jnlp后,复制到jenkins-slave-windows工作目录下，启动即可 点击launch 安装该文件， 环境变量配置如图所示C:\\station\\Python27;C:\\station\\Python27\\Scripts;C:\\Program Files (x86)\\Google\\Chrome\\Application;C:\\Windows\\System32;启动方式没有java web start解决办法1：打开”系统管理”——“Configure Global Security”2：启用安全3：随机选取如图所示：系统管理全局安全配置代理是否制定端口，根据自己的实际情况 Jenkins master&#x2F;slave 主从机制配置Master主机系统：linuxSlave 机系统：windows&#x2F;linux  master主机与slave机之间的端口问题 解决办法 制定slave端口 参考文档 https://stackoverflow.com/questions/17472291/jenkins-slave-port-number-for-firewallTCP49187 - Fixed jnlp port8080 - jenkins http port 在Jenkins中环境变量有：• 主机中的系统环境变量• Master&#x2F;Slave节点设置的环境变量• Job执行时的环境变量（http://ip:port/jenkins/env-vars.html/、参数化构建时的参数也会被设置为环境变量、一些插件提供的环境变量）其中，如果环境变量名称相同，后者会覆盖前者 在使用Jenkins的过程中，多次遇到Jenkins job中无法获取Slave上的环境变量的情况，以pipeline script 构建失败居多，出现问题如图所示： 构建pipeline 查看此处的环境变量是否是slave机的环境变量解决方法：• 使用绝对路径的命令• 在Jenkins的job中设置环境变量参数• 在Jenkins的节点配置中设置环境变量当然，个人感觉其中最友好的方式是在Jenkins的节点配置中设置环境变量注意：要确定jenkins 系统管理系统设置全局属性环境变量是否做了配置，如果做了配置，slave节点设置的环境变量就会被覆盖Master主机 Slave节点节点属性环境变量 Pipeline script stage 里设置环境变量env.PATH &#x3D; “。。。” 环境变量生效顺序既然上面有那么多的地方可以设置环境变量，当多个地方都有定义的时候，全局环境变量 &lt; Slave 配置环境变量 &lt; Job 参数 &lt; Job injected 环境变量一般不Override系统变量，也就是说不重复定义系统内置的变量，否则可能出现不可预知的问题。s3cmd 配置下载地址：https://s3tools.org/download安装（推荐）：https://tecadmin.net/setup-s3cmd-in-windows/ https://www.digitalocean.com/docs/spaces/resources/s3cmd/安装s3cmd ,并运行 python s3cmd –configure 配置信息Enter new values or accept defaults in brackets with Enter.Refer to user manual for detailed description of all options.Access key and Secret key are your identifiers for Amazon S3. Leave them empty for using the env variables.Access Key []:必填（ceph 申请）Secret Key []:必填（ceph 申请）Default Region [US]:Use “s3.amazonaws.com” for S3 Endpoint and not modify it to the target Amazon S3.S3 Endpoint [s3.amazonaws.com]: 必填 远程ceph ip地址:port(如有)Use “%(bucket)s.s3.amazonaws.com” to the target Amazon S3. “%(bucket)s” and “%(location)s” vars can be used if the target S3 system supports dns based buckets.DNS-style bucket+hostname:port template for accessing a bucket []: devops（必填）Encryption password is used to protect your files from readingby unauthorized persons while in transfer to S3Encryption password:Path to GPG program []:When using secure HTTPS protocol all communication with Amazon S3servers is protected from 3rd party eavesdropping. This method isslower than plain HTTP, and can only be proxied with Python 2.7 or newerUse HTTPS protocol []: NoOn some networks all internet access must go through a HTTP proxy.Try setting it here if you can’t connect to S3 directlyHTTP Proxy server name:New settings: Access Key: EXAMPLES7UQOTHDTF3GK4 Secret Key: b8e1ec97b97bff326955375c5example Default Region: US S3 Endpoint: nyc3.digitaloceanspaces.com DNS-style bucket+hostname:port template for accessing a bucket: %(bucket)s.nyc3.digitaloceanspaces.com Encryption password: secure_password Path to GPG program: &#x2F;usr&#x2F;bin&#x2F;gpg Use HTTPS protocol: True HTTP Proxy server name: HTTP Proxy server port: 0Test access with supplied credentials? [Y&#x2F;n] YPlease wait, attempting to list all buckets…Success. Your access key and secret key worked fine :-)Now verifying that encryption works…Success. Encryption and decryption worked fine :-)Save settings? [y&#x2F;N] YConfiguration saved to ‘生产配置文件地址’Windows配置文件 s3cmd.iniLinux配置文件 .s3cfg运行页面","tags":[{"name":"Ride","slug":"Ride","permalink":"https://wangdj104.github.io/tags/Ride/"}]},{"title":"Ceph文件系统","date":"2019-07-29T16:00:13.000Z","path":"2019/07/30/Ceph文件系统/","text":"1)disable iptables&amp;selinux 12345678910111213echo -n &quot;正在配置iptables防火墙……&quot;systemctl stop firewalld &gt; /dev/null 2&gt;&amp;1systemctl disable firewalld &gt; /dev/null 2&gt;&amp;1if [ $? -eq 0 ];thenecho -n &quot;Iptables防火墙初始化完毕！&quot;fiecho -n &quot;正在关闭SELinux……&quot;setenforce 0 &gt; /dev/null 2&gt;&amp;1sed -i &#x27;/^SELINUX=/s/=.*/=disabled/&#x27; /etc/selinux/configif [ $? -eq 0 ];then echo -n &quot;SELinux初始化完毕！&quot;fi set hostname as ceph1234HOSTNAME=cephhostnamectl set-hostname cephIP=`ip route |grep src|grep metric|awk -F&quot; &quot; &#x27;&#123; print $9 &#125;&#x27;`echo &quot;$IP $HOSTNAME&quot; &gt;&gt;/etc/hosts install epel.repo1234567891011121314151617181920212223242526272829yum install -y epel-releasecat &lt;&lt;EOF &gt; /etc/yum.repos.d/ceph.repo[Ceph]name=Ceph packages for $basearchbaseurl=http://mirrors.163.com/ceph/rpm-jewel/el7/x86_64enabled=1gpgcheck=1type=rpm-mdgpgkey=http://mirrors.163.com/ceph/keys/release.ascpriority=1[Ceph-noarch]name=Ceph noarch packagesbaseurl=http://mirrors.163.com/ceph/rpm-jewel/el7/noarchenabled=1gpgcheck=1type=rpm-mdgpgkey=http://mirrors.163.com/ceph/keys/release.ascpriority=1[ceph-source]name=Ceph source packagesbaseurl=http://mirrors.163.com/ceph/rpm-jewel/el7/SRPMSenabled=1gpgcheck=1type=rpm-mdgpgkey=http://mirrors.163.com/ceph/keys/release.ascpriority=1EOF 4)update system &amp; install ceph-deploy1yum update -y &amp;&amp;yum clean all &amp;&amp;yum -y install ceph-deploy 设置本机密匙12 ssh-keygen -t rsa -P &quot;&quot; -f ~/.ssh/id_rsacat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys 6)ceph服务初始化123yum clean all &amp;&amp;yum -y install ceph-deploymkdir /etc/ceph &amp;&amp;cd /etc/cephceph-deploy new ceph 7)修改配置文件123cp ceph.conf ceph.conf.baksed -i &#x27;s@^$@osd_pool_default_size = 1@g&#x27; ceph.confecho &quot;mon_pg_warn_max_per_osd = 1000&quot; &gt;&gt; /etc/ceph/ceph.conf 8)安装ceph1ceph-deploy install ceph 9)创建monitor服务12ceph-deploy mon create cephceph-deploy gatherkeys ceph 准备osd123456789mkfs.xfs /dev/sdbmkdir -p /var/local/osdmount /dev/sdb /var/local/osd/chown -R ceph:ceph /var/local/osd* #创建osdceph-deploy osd prepare ceph:/var/local/osd #激活osdceph-deploy osd activate ceph:/var/local/osdchown -R ceph:ceph /var/local/osd* #有些同学可能会忘记配置目录权限引起激活osd失败 #查看状态：###ceph-deploy osd list ceph 11）修改配置文件权限12ceph-deploy admin cephchmod +r /etc/ceph/* 12）部署mds服务12ceph-deploy mds create cephceph mds stat 13）创建ceph文件系统12345ceph fs lsceph osd pool create cephfs_data 128ceph osd pool create cephfs_metadata 128ceph fs new cephfs cephfs_metadata cephfs_dataceph fs ls 14）挂载Ceph文件系统123456mkdir /cephyum install -y ceph-fuseIP=`ip route |grep src|grep metric|awk -F&quot; &quot; &#x27;&#123; print $9 &#125;&#x27;`ceph-fuse -m $IP:7480/ /ceph#ceph-fuse -m $IP:6789/ /cephdf -Th 15）查看ceph状态12345ceph mon statceph osd statceph osd tree #显示crush图ceph pg statset password for root@localhost = password(&#x27;S@Aj#XrpS114y!9$&#x27;); #设置mysql密码 ##部署ceph对象存储-bucket1)安装ceph-radosgw 1yum install ceph-radosgw 2)部署rgw 1ceph-deploy rgw create ceph #需在/etc/ceph目录下执行 3)如果要修改为80端口，可修改配置文件 重启 12345 vim /etc/ceph/ceph.conf [client.rgw.ceph-node1]rgw_frontends = &quot;civetweb port=80&quot;sudo systemctl restart ceph-radosgw@rgw.ceph.servicenetstat -tnlp |grep 7480 4)创建池 1234567891011121314151617181920212223wget https://raw.githubusercontent.com/aishangwei/ceph-demo/master/ceph-deploy/rgw/poolwget https://raw.githubusercontent.com/aishangwei/ceph-demo/master/ceph-deploy/rgw/create_pool.shcat create_pool.sh#!/bin/bashPG_NUM=250PGP_NUM=250SIZE=3for i in `cat /etc/ceph/pool` do ceph osd pool create $i $PG_NUM ceph osd pool set $i size $SIZE donefor i in `cat /etc/ceph/pool` do ceph osd pool set $i pgp_num $PGP_NUM donechmod +x create_pool.sh./create_pool.sh 5)测试是否能访问ceph集群 123ls -l /var/lib/ceph/cp /var/lib/ceph/radosgw/ceph-rgw.ceph/keyring ./ceph -s -k keyring --name client.rgw.ceph 6)使用S3 API访问ceph对象存储创建radosgw用户 1radosgw-admin user create --uid=radosgw --display-name=&quot;radosgw&quot; 7)安装s3cmd客户端 1234567yum install -y s3cmds3cmd --configure //注意该Access Key填写radosgw的access_key Secret Key同样编辑s3配置文件：cat .s3cfg创建桶并放入文件s3cmd mb s3://devopss3cmd lss3cmd put /etc/hosts s3://devops 8)使用Swift API访问ceph对象存储 1234567radosgw-admin subuser create --uid=radosgw --subuser=radosgw:swift --access=fullyum install python-pip -ypip install --upgrade python-swiftclientswift -A http://ceph:7480/auth/1.0 -U radosgw:swift -K 6Dz8f8ubJDNhr57jV5tM0xeFSzBxXN1MzK59qdMh listswift -A http://ceph:7480/auth/1.0 -U radosgw:swift -K 6Dz8f8ubJDNhr57jV5tM0xeFSzBxXN1MzK59qdMh listswift -A http://ceph:7480/auth/1.0 -U radosgw:swift -K 6Dz8f8ubJDNhr57jV5tM0xeFSzBxXN1MzK59qdMh post second-bucketswift -A http://ceph:7480/auth/1.0 -U radosgw:swift -K 6Dz8f8ubJDNhr57jV5tM0xeFSzBxXN1MzK59qdMh list 9)最后验证 1234s3cmd ls2019-02-15 07:45 s3://devops2019-02-15 08:18 s3://second-buckets3cmd ls s3://devops/20190619/","tags":[{"name":"ceph","slug":"ceph","permalink":"https://wangdj104.github.io/tags/ceph/"}]},{"title":"LVM磁盘扩容及磁盘挂载","date":"2019-07-29T06:13:25.000Z","path":"2019/07/29/LVM磁盘扩容及磁盘挂载/","text":"LVM是逻辑盘卷管理（Logical Volume Manager）的简称，他是磁盘管理的另一种工具，目前基本上所有操作系统均支持，LVM是建立在硬盘和分区之上的一个逻辑层，来提高磁盘分区管理的灵活性。 磁盘扩容• PV（Phsical Volume，物理卷），PV是VG的组成部分，有分区构成，多块盘的时候，可以把一块盘格式化成一个主分区，然后用这个分区做成一个PV，只有一块盘的时候，可以这块盘的某一个分区做成一个PV，实际上一个PV就一个分区。• VG（Volume Group， 卷组），有若干个PV组成，作用就是将PV组成到以前，然后再重新划分空间。• LV（Logical Volume，逻辑卷），LV就是从VG中划分出来的卷，LV的使用要比PV灵活的多，可以在空间不够的情况下，增加空间。(1) 创建磁盘分区 1sudo fdisk /dev/sda 选择 n 创建分区 （默认值）选择 t 修改分区类型 8e （note:8e为Linux LVM）选择 w 写入分区表 (2) 请求操作系统重新加载分区表 1sudo partprobe (3) 创建物理卷PV 1sudo pvcreate /dev/sda3 (4) 扩展卷组VG 123sudo vgscansudo vgextend VolGroup00 /dev/sda3Sudo vgdisplay (5) 扩展逻辑卷LV 123sudo lvextend -l +100%free /dev/VolGroup00/LogVol01或者sudo lvextend –L +50G /dev/VolGroup00/LogVol01sudo lvdisplay /dev/VolGroup00/LogVol01 (6) 扩展文件系统容量 1sudo xfs_growfs /dev/VolGroup00/LogVol01 磁盘挂载(1) 查看操作系统有几块硬盘 1sudo fdisk -l (2) 创建pv 12sudo pvcreate /dev/sdbsudo pvdisplay (3) 创建vg 12sudo vgcreate vg_k8s /dev/sdbsudo vgdisplay vg_k8s (4) 创建lv，并挂载 创建lv12sudo lvcreate -n lv_k8s -l 100%FREE vg_k8ssudo lvscan 在根目录下创建目录k8s1sudo mkdir /k8s 3）对逻辑卷进行格式化：1sudo mkfs -t xfs -n ftype=1 /dev/vg_k8s/lv_k8s 将逻辑卷&#x2F;dev&#x2F;vg_k8s&#x2F;lv_k8s挂载到&#x2F;k8s目录下:12sudo mount /dev/vg_k8s/lv_k8s /k8ssudo df -h 5)修改&#x2F;etc&#x2F;fstab，添加挂载信息，实现开机自动挂载1sudo vi /etc/fstab 重新挂载1sudo mount -a 重启服务器生效 注意：创建pv时，可能出现&#x2F;dev&#x2F;sdb明明存在并且没被使用，但却无法使用&#x2F;dev&#x2F;sdb创建pv。解决办法：执行 dd if&#x3D;&#x2F;dev&#x2F;urandom of&#x3D;&#x2F;dev&#x2F;sdb bs&#x3D;512 count&#x3D;64 然后尝试创建pv成功。具体原因见 http://blog.sina.com.cn/s/blog_701300bc0100xmr4.html 修改逻辑盘文件系统类型及重新挂载(1)df -Th 查看的是file system, 也就是文件系统层的磁盘大小df查看的是文件系统层的磁盘大小,-T 文件系统类型,-h 方便阅读方式显示(2)mount 查看挂载(3)umount &amp;&amp; mount(4)fdisk -l 查看当前的磁盘分区信息(5)lsblk 查看的是block device,也就是逻辑磁盘大小。(6)修改文件系统格式 1mkfs -t xfs -n ftype=1 -f /dev/mapper/VG4736-lv_32 (7)重新挂载 1mount /dev/mapper/VG4736-lv_32 /data01 (8)lsblk(9)","tags":[{"name":"LVM","slug":"LVM","permalink":"https://wangdj104.github.io/tags/LVM/"}]},{"title":"Glusterfs服务","date":"2019-07-21T14:33:12.000Z","path":"2019/07/21/Glusterfs服务/","text":"安装包下载链接: https://pan.baidu.com/s/1-HLP6AbAdsy5-v82CBZNIQ 提取码: pkde本文部署glusterfs集群在四台机器192.168.117.129 gfs129192.168.117.130 gfs130192.168.117.131 gfs131192.168.117.132 gfs132 GlusterFs离线安装(需在四台机器上操作)123456rpm -Uvh xz-libs-5.2.2-1.el7.x86_64.rpm xz-5.2.2-1.el7.x86_64.rpm xz-libs-5.2.2-1.el7.i686.rpm --force --nodepsrpm -ihv glusterfs-4.0.2-1.el7.x86_64.rpm glusterfs-libs-4.0.2-1.el7.x86_64.rpm --force --nodepsrpm -ihv glusterfs-fuse-4.0.2-1.el7.x86_64.rpm glusterfs-client-xlators-4.0.2-1.el7.x86_64.rpm glusterfs-cli-4.0.2-1.el7.x86_64.rpm --force --nodepsrpm -ihv userspace-rcu-0.10.0-3.el7.x86_64.rpm --force --nodepsrpm -ivh rpcbind-0.2.0-44.el7.x86_64.rpm libtirpc-0.2.4-0.10.el7.x86_64.rpm --force --nodepsrpm -ihv glusterfs-server-4.0.2-1.el7.x86_64.rpm glusterfs-api-4.0.2-1.el7.x86_64.rpm --force --nodeps 启动 Glusterfs12systemctl start glusterd.servicesystemctl enable glusterd.service 添加hosts文件12345[root@localhost ~]# vim /etc/hosts192.168.117.129 gfs129192.168.117.130 gfs130192.168.117.131 gfs131192.168.117.132 gfs132 【说明】添加hosts文件这一步不是必需。如果使用IP地址，之后添加节点时也必需使用IP地址。如果使用hosts文件，则4个节点都必需要添加相同的hosts文件 添加GlusterFS集群节点【说明】本小节操作在任意节点操作即可,且进行操作前要先检查每台机器的防火墙是否关闭。 12345678[root@gfs129 ~]# gluster peer probe gfs129peer probe: success. Probe on localhost not needed[root@gfs129 ~]# gluster peer probe gfs130peer probe: success.[root@gfs129 ~]# gluster peer probe gfs131peer probe: success.[root@gfs129 ~]# gluster peer probe gfs132peer probe: success. 【说明】移除节点gluster peer detach gfs132 查看GlusterFS集群状态（任意节点）1234567891011121314[root@gfs129 ~]# gluster peer statusNumber of Peers: 3Hostname: gfs130Uuid: f5af69ef-545a-4e34-be4a-4b3ab10b7caaState: Peer in Cluster (Connected)Hostname: gfs131Uuid: 239e6236-ec2f-4b75-a39e-57eb8bd7144eState: Peer in Cluster (Connected)Hostname: gfs132Uuid: 631f8255-06d7-45f3-ade3-da6eef88c908State: Peer in Cluster (Connected) 查看GlusterFS volume12[root@gfs129 ~]# gluster volume infoNo volumes present 创建Glusterfs所需磁盘（1）查看磁盘情况fdisk -l（2）添加sdb磁盘关机，菜单栏找到虚拟机—设置，点击硬盘—添加，按步骤走，重新开机就有sdb了 （3）新建分区 123456789101112131415161718192021222324252627[root@gfs129 ~]# fdisk /dev/sdbWelcome to fdisk (util-linux 2.23.2).Changes will remain in memory only, until you decide to write them.Be careful before using the write command.Device does not contain a recognized partition tableBuilding a new DOS disklabel with disk identifier 0x51a8f067.Command (m for help): #输入n回车,n是“new”新建分区的意思Partition type: p primary (0 primary, 0 extended, 4 free) e extendedSelect (default p): #默认值Using default response pPartition number (1-4, default 1): #默认值First sector (2048-8388607, default 2048): #默认值Using default value 2048Last sector, +sectors or +size&#123;K,M,G&#125; (2048-8388607, default 8388607): #默认值Using default value 8388607Partition 1 of type Linux and of size 4 GiB is setCommand (m for help): w # &quot;write&quot;并回车，意思是对刚才的结果进行保存The partition table has been altered!Calling ioctl() to re-read partition table.Syncing disks. 执行完后，效果如下图：（4）格式化分区为XFS 1mkfs.xfs -i size=512 /dev/sdb1 （5）挂载： 12345mkdir -p /glusterfs-data/data01/mount /dev/sdb1 /glusterfs-data/data01/ vim /etc/fstab /dev/sdb1 /glusterfs-data/data01 xfs defaults 0 0 注意：所有节点都需创建Glusterfs所需磁盘及glusterfs-data数据目录 创建GlusterFS volume（master节点）分布式复制模式（组合型）, 最少需要4台服务器才能创建。创建分布式复制卷需指定卷类型为replica（复制卷）（否则默认为分布式卷），卷类型后边参数是副本数量。Transport指定传输类型为tcp。传输类型后的brick server数量需是副本数量的倍数，且&gt;&#x3D;2倍。当副本数量与brick server数量不等且符合倍数关系时，即是分布式复制卷 12[root@gfs129 glusterfs-data]# gluster volume create devops-volume replica 2 transport tcp gfs129:/glusterfs-data/data01/ gfs130:/glusterfs-data/data01/ gfs131:/glusterfs-data/data01/ gfs132:/glusterfs-data/data01/ forcevolume create: devops-volume: success: please start the volume to access data 查看GlusterFS volume(任意节点)123456789101112131415161718[root@gfs130 ~]# gluster volume infoVolume Name: devops-volumeType: Distributed-ReplicateVolume ID: 14573cf5-bd1d-4ce0-82b9-b44f13db60b3Status: CreatedSnapshot Count: 0Number of Bricks: 2 x 2 = 4Transport-type: tcpBricks:Brick1: gfs129:/glusterfs-data/data01Brick2: gfs130:/glusterfs-data/data01Brick3: gfs131:/glusterfs-data/data01Brick4: gfs132:/glusterfs-data/data01Options Reconfigured:transport.address-family: inetnfs.disable: onperformance.client-io-threads: off 启动GlusterFS volume12[root@gfs129 glusterfs-data]# gluster volume start devops-volumevolume start: devops-volume: success 再次查看GlusterFS volume(任意节点)123456789101112131415161718[root@gfs130 /]# gluster volume infoVolume Name: devops-volumeType: Distributed-ReplicateVolume ID: 14573cf5-bd1d-4ce0-82b9-b44f13db60b3Status: Started #与前一次查看，状态有所改变Snapshot Count: 0Number of Bricks: 2 x 2 = 4Transport-type: tcpBricks:Brick1: gfs129:/glusterfs-data/data01Brick2: gfs130:/glusterfs-data/data01Brick3: gfs131:/glusterfs-data/data01Brick4: gfs132:/glusterfs-data/data01Options Reconfigured:transport.address-family: inetnfs.disable: onperformance.client-io-threads: off 客户端挂载GlusterFS如果GlusterFS集群初始化使用了主机&#x2F;etc&#x2F;hosts中的信息，请在客户端也添加对应的信息。否则挂载过程会报错。这里就是需要在harbor所在的服务器上把hosts文件添加成跟glusterfs上一致，并且也需要在这台服务器上装一个glusterfs的客户端。安装方式同1中Glusterfs离线安装。 123[root@localhost /]# mkdir -p /gfsdata01volume[root@localhost /]# mount -t glusterfs gfs130:devops-volume /gfsdata01volume注意：gfs130:devops-volume中也可为129,131,132 挂载成功后，效果如下： 设置开机自动挂载12[root@localhost harbor]# vim /etc/fstabgfs129:devops-volume /gfsdata01volume glusterfs defaults 0 0 Glusterfs的常用命令参考自 https://blog.csdn.net/lincoln_2012/article/details/52201227 服务器节点123gluster peer status //查看所有节点信息，显示时不包括本节点gluster peer probe NODE-NAME //添加节点gluster peer detach NODE-NAME //移除节点，需要提前将该节点上的brick移除 glusterd服务123/etc/init.d/glusterd start //启动glusterd服务/etc/init.d/glusterd stop //关闭glusterd服务/etc/init.d/glusterd status //查看glusterd服务 创建卷12345678910111213141516171819202122232425262728293031323334353637383940&lt;1&gt;复制卷语法： gluster volume create NEW-VOLNAME [replica COUNT] [transport tcp | rdma | tcp, rdma] NEW-BRICK示例1：gluster volume create test-volume replica 2 transport tcp server1:/exp1/brick server2:/exp2/brick&lt;2&gt;条带卷语法：gluster volume create NEW-VOLNAME [stripe COUNT] [transport tcp | rdma | tcp, rdma] NEW-BRICK...示例：gluster volume create test-volume stripe 2 transport tcp server1:/exp1/brick server2:/exp2/brick&lt;3&gt;分布式卷语法： gluster volume create NEW-VOLNAME [transport tcp | rdma | tcp, rdma] NEW-BRICK示例1：gluster volume create test-volume server1:/exp1/brick server2:/exp2/brick示例2：gluster volume create test-volume transport rdma server1:/exp1/brick server2:/exp2/brick server3:/exp3/brick server4:/exp4/brick&lt;4&gt;分布式复制卷语法： gluster volume create NEW-VOLNAME [replica COUNT] [transport tcp | rdma | tcp, rdma] NEW-BRICK...示例： gluster volume create test-volume replica 2 transport tcp server1:/exp1/brick server2:/exp2/brick server3:/exp3/brick server4:/exp4/brick&lt;5&gt;分布式条带卷语法：gluster volume create NEW-VOLNAME [stripe COUNT] [transport tcp | rdma | tcp, rdma] NEW-BRICK...示例：gluster volume create test-volume stripe 2 transport tcp server1:/exp1/brick server2:/exp2/brick server3:/exp3/brick server4:/exp4/brick&lt;6&gt;条带复制卷语法：gluster volume create NEW-VOLNAME [stripe COUNT] [replica COUNT] [transport tcp | rdma | tcp, rdma] NEW-BRICK...示例：gluster volume create test-volume stripe 2 replica 2 transport tcp server1:/exp1/brick server2:/exp2/brick server3:/exp3/brick server4:/exp4/brick 启动卷1gluster volume start test-volume 停止卷1gluster volume stop test-volume 删除卷1gluster volume delete test-volume //先停止卷后才能删除 查看卷1234gluster volume list /*列出集群中的所有卷*/gluster volume info [all] /*查看集群中的卷信息*/gluster volume status [all] /*查看集群中的卷状态*/gluster volume status [detail| clients | mem | inode | fd] 配置卷1gluster volume set &lt;VOLNAME&gt; &lt;OPTION&gt; &lt;PARAMETER&gt; 扩展卷1gluster volume add-brick &lt;VOLNAME&gt; &lt;NEW-BRICK&gt; 注意，如果是复制卷或者条带卷，则每次添加的Brick数必须是replica或者stripe的整数倍。 收缩卷123456先将数据迁移到其它可用的Brick，迁移结束后才将该Brick移除：# gluster volume remove-brick start在执行了start之后，可以使用status命令查看移除进度：# gluster volume remove-brick status不进行数据迁移，直接删除该Brick：# gluster volume remove-brick commit 注意，如果是复制卷或者条带卷，则每次移除的Brick数必须是replica或者stripe的整数倍。 迁移卷12345678910使用start命令开始进行迁移：# gluster volume replace-brick start在数据迁移过程中，可以使用pause命令暂停迁移：# gluster volume replace-brick pause在数据迁移过程中，可以使用abort命令终止迁移：# gluster volume replace-brick abort在数据迁移过程中，可以使用status命令查看迁移进度：# gluster volume replace-brick status在数据迁移结束后，执行commit命令来进行Brick替换：# gluster volume replace-brick commit 重新均衡卷123456不迁移数据：# gluster volume rebalance lay-outstart# gluster volume rebalance start# gluster volume rebalance startforce# gluster volume rebalance status# gluster volume rebalance stop 添加Brick1# gluster volume add-brick test-volume 192.168.1.&#123;151,152&#125;:/mnt/brick2 删除Brick1234567若是副本卷，则移除的Bricks数是replica的整数倍#gluster volume remove-brick test-volume 192.168.1.&#123;151,152&#125;:/mnt/brick2 start在执行开始移除之后，可以使用status命令进行移除状态查看。#gluster volume remove-brick test-volume 192.168.1.&#123;151,152&#125;:/mnt/brick2 status使用commit命令执行Brick移除，则不会进行数据迁移而直接删除Brick，符合不需要数据迁移的用户需求。#gluster volume remove-brick test-volume 192.168.1.&#123;151,152&#125;:/mnt/brick2 commit 替换Brick任务：把192.168.1.151:&#x2F;mnt&#x2F;brick0 替换为192.168.1.151:&#x2F;mnt&#x2F;brick2 1234567891011121314151617181920212223&lt;1&gt;开始替换#gluster volume replace-brick test-volume 192.168.1.:/mnt/brick0 ..152:/mnt/brick2 start异常信息：volume replace-brick: failed: /data/share2 or a prefix of it is already part of a volume说明 /mnt/brick2 曾经是一个Brick。具体解决方法# rm -rf /mnt/brick2/.glusterfs# setfattr -x trusted.glusterfs.volume-id /mnt/brick2# setfattr -x trusted.gfid /mnt/brick2//如上，执行replcace-brick卷替换启动命令，使用start启动命令后，开始将原始Brick的数据迁移到即将需要替换的Brick上。&lt;2&gt;查看是否替换完#gluster volume replace-brick test-volume 192.168.1.151:/mnt/brick0 ..152:/mnt/brick2 status&lt;3&gt;在数据迁移的过程中，可以执行abort命令终止Brick替换。#gluster volume replace-brick test-volume 192.168.1.151:/mnt/brick0 ..152:/mnt/brick2 abort&lt;4&gt;在数据迁移结束之后，执行commit命令结束任务，则进行Brick替换。使用volume info命令可以查看到Brick已经被替换。#gluster volume replace-brick test-volume 192.168.1.151:/mnt/brick0 .152:/mnt/brick2 commit# 此时我们再往 /sf/data/vs/gfs/rep2上添加数据的话，数据会同步到 192.168.1.152:/mnt/brick0和192.168.1.152:/mnt/brick2上。而不会同步到192.168.1.151:/mnt/brick0 上。 文件系统扩展属性获取文件扩展属性 12getfattr -d -m . -e hex filenamegetfattr -d -m &quot;trusted.afr.*&quot; -e hex filename 日志文件路径相关日志，在客户端机器的&#x2F;var&#x2F;log&#x2F;glusterfs&#x2F;目录下，可根据需要查看；如&#x2F;var&#x2F;log&#x2F;glusterfs&#x2F;brick&#x2F;下是各brick创建的日志；如&#x2F;var&#x2F;log&#x2F;glusterfs&#x2F;cmd_history.log是命令执行记录日志；如&#x2F;var&#x2F;log&#x2F;glusterfs&#x2F;glusterd.log是glusterd守护进程日志。 Glusterfs容灾能力研究glusterfs集群节点关机重启(单节点)关机的机器没有设置开机自动挂载,关机重启后，挂载点消失 需手动重新挂载 1mount /dev/sdb1 /glusterfs-data/data01/ 若重新挂载后,出现关机重启后的机器的brick不正常，则在glusterfs集群节点重启数据卷即可： 12gluster volume stop devops-volumegluster volume start devops-volume 然后重新进入glusterfs目录，发现在关机期间，客户端新建的文件也同步过来了，但是和同一个副本集下的另一个副本的文件扩展属性有差别，如下（关机重启的是gfs131机器）：如果属性值如上，则可用gfs132的bcv2.txt文件去恢复gfs131的bcv2.txt文件。 【说明】如果gfs131和gfs132都正常的情况下，客户端新往这两台机器写入数据（bcv3.txt），那么bcv3.txt的扩展属性都与上图1类似，没有trusted.afr.dirty属性值，两台机器重启时扩展属性值不变。证明此种情况下，两台机器的bcv3.txt数据是正常的。 如果gfs131正常，gfs132离线，客户端新往这两台机器写入数据（bcv4.txt），那么，重启gfs132后，该文件的扩展属性与上图1类似，没有trusted.afr.dirty属性值。bcv4.txt的扩展属性在gfs131中与上图2类似，有trusted.afr.dirty属性值且为0x000000000000。 证明此种情况下，gfs131机器的bcv4.txt数据是正常的，gfs132机器的bcv4.txt数据可能是不可靠的。 glusterfs集群节点关机重启(一个副本集下的双节点)测试关闭了gfs131和gfs132，然后在客户端创建文件 1touch distributed-replica&#123;1..6&#125;.txt 出现下述问题： 此时，数据就会丢失且无法恢复，应尽量避免此情况发生。 glusterfs集群状态检查可以通过下述命令，对glusterfs集群进行健康状态的检查:gluster volume status devops-volumegluster volume info下图即为正常","tags":[{"name":"Glusterfs","slug":"Glusterfs","permalink":"https://wangdj104.github.io/tags/Glusterfs/"}]},{"title":"Rsyncd服务","date":"2019-07-17T11:52:30.000Z","path":"2019/07/17/Rsyncd服务/","text":"Gitlab主备数据备份安装包下载链接: https://pan.baidu.com/s/1s1nPjzM9w9M8e0V4EqcZog 提取码: gv5wmaster-129（1）安装 1yum localinstall *.rpm （2）)创建用户名和密码 12useradd forgitlab 创建用户forgitlabpasswd forgitlab 给已创建的用户forgitlab设置密码为pass123 （3）修改配置文件 123456789101112131415161718192021222324252627282930vim /etc/rsyncd.conf#设置rsync运行权限为rootuid=root#设置rsync运行权限为rootgid=root#最大连接数max connections=3#默认为true，修改为no，增加对目录文件软连接的备份use chroot=no#日志文件位置，启动rsync后自动产生这个文件，无需提前创建log file=/var/log/rsyncd.log#pid文件的存放位置pid file=/var/run/rsyncd.pid#支持max connections参数的锁文件lock file=/var/run/rsyncd.lock#用户认证配置文件，里面保存用户名称和密码 需要创建（可选）secrets file=/etc/rsync.pass#允许进行数据同步的客户端IP地址，可以设置多个，用英文状态下逗号隔开,可设置所有hosts allow= *#设置rsync服务端文件为读写权限read only = no#不显示rsync服务端资源列表list = no[forgitlab]#需要备份的源主机数据目录路径path = /data/gitlab/data/git-data#执行数据同步的用户名，可以设置多个，用英文状态下逗号隔开 可选配置auth users = forgitlab (4)创建认证文件如果在rsyncd服务中定义了可选配置，则需创建认证文件。 123[root@localhost ~]# vim /etc/rsync.passforgitlab:pass123chmod 600 /etc/rsync.pass (5)启动Rsyncd服务 12[root@localhost ~]# systemctl start rsyncd[root@localhost ~]# systemctl enable rsyncd (6)开启rsyncd服务端口 123[root@localhost ~]# firewall-cmd --permanent --add-port=873/tcp[root@localhost ~]# firewall-cmd --permanent --add-port=873/udp[root@localhost ~]# firewall-cmd --reload slave-130 master备节点（1）安装 1yum localinstall *.rpm (2)创建认证文件 123[root@localhost ~]# vim /etc/rsync.passpass123chmod 600 /etc/rsync.pass (3)手动测试 1[root@localhost ~]# rsync -avzrtlp --progress --delete --password-file=/etc/rsync.pass forgitlab@192.168.117.129::forgitlab /data/gitlab/data/git-data (4)自动执行 12[root@localhost ~]# crontab -e*/5 * * * * rsync -avzrtlp --progress --delete --password-file=/etc/rsync.pass forgitlab@192.168.117.129::forgitlab /data/gitlab/data/git-data 【说明1】每5分钟同步一次。【说明2】如果出现目录可以同步，文本文件类型的文件不能同步,请检查SELinux是否关闭1、临时关闭：输入命令setenforce 0，重启系统后还会开启。2、永久关闭：输入命令vi &#x2F;etc&#x2F;selinux&#x2F;config，将SELINUX&#x3D;enforcing改为SELINUX&#x3D;disabled，然后保存退出。 Jenkins主备数据备份安装包下载链接: https://pan.baidu.com/s/1s1nPjzM9w9M8e0V4EqcZog 提取码: gv5wmaster-129（1）安装 1yum localinstall *.rpm （2）)创建用户名和密码 12useradd forjenkinspasswd forjenkins 给已创建的用户forjenkins设置密码为pass123 （3）修改配置文件 123456789101112131415161718192021222324252627282930vim /etc/rsyncd.conf#设置rsync运行权限为rootuid=root#设置rsync运行权限为rootgid=root#最大连接数max connections=3#默认为true，修改为no，增加对目录文件软连接的备份use chroot=no#日志文件位置，启动rsync后自动产生这个文件，无需提前创建log file=/var/log/rsyncd.log#pid文件的存放位置pid file=/var/run/rsyncd.pid#支持max connections参数的锁文件lock file=/var/run/rsyncd.lock#用户认证配置文件，里面保存用户名称和密码 需要创建（可选）secrets file=/etc/rsync.pass#允许进行数据同步的客户端IP地址，可以设置多个，用英文状态下逗号隔开,可设置所有hosts allow= *#设置rsync服务端文件为读写权限read only = no#不显示rsync服务端资源列表list = no[forjenkins]#需要备份的源主机数据目录路径path = /home/jenkins/work#执行数据同步的用户名，可以设置多个，用英文状态下逗号隔开 可选配置auth users = forjenkins (4)创建认证文件如果在rsyncd服务中定义了可选配置，则需创建认证文件。 123[root@localhost ~]# vim /etc/rsync.passforjenkins:pass123chmod 600 /etc/rsync.pass (5)启动Rsyncd服务 12[root@localhost ~]# systemctl start rsyncd[root@localhost ~]# systemctl enable rsyncd (6)开启rsyncd服务端口 123[root@localhost ~]# firewall-cmd --permanent --add-port=873/tcp[root@localhost ~]# firewall-cmd --permanent --add-port=873/udp[root@localhost ~]# firewall-cmd --reload master-130 master备节点（1）安装 1yum localinstall *.rpm (2)创建认证文件 123[root@localhost ~]# vim /etc/rsync.passpass123chmod 600 /etc/rsync.pass (3)手动测试 1[root@localhost ~]# rsync -vzrtp --progress --delete --password-file=/etc/rsync.pass forjenkins@192.168.117.129::forjenkins /home/jenkins/work (4)自动执行 12[root@localhost ~]# crontab -e*/5 * * * * rsync -vzrtp --progress --delete --password-file=/etc/rsync.pass forjenkins@192.168.117.129::forjenkins /home/jenkins/work 【说明1】每5分钟同步一次。【说明2】如果出现目录可以同步，文本文件类型的文件不能同步,请检查SELinux是否关闭1、临时关闭：输入命令setenforce 0，重启系统后还会开启。2、永久关闭：输入命令vi &#x2F;etc&#x2F;selinux&#x2F;config，将SELINUX&#x3D;enforcing改为SELINUX&#x3D;disabled，然后保存退出。 harbor主备数据备份安装包下载链接: https://pan.baidu.com/s/1s1nPjzM9w9M8e0V4EqcZog 提取码: gv5wmaster-129（1）安装 1yum localinstall *.rpm （2）)创建用户名和密码 12useradd forgitlab 创建用户forgitlabpasswd forgitlab 给已创建的用户forgitlab设置密码为pass123 （3）修改配置文件 123456789101112131415161718192021222324252627282930vim /etc/rsyncd.conf#设置rsync运行权限为rootuid=root#设置rsync运行权限为rootgid=root#最大连接数max connections=3#默认为true，修改为no，增加对目录文件软连接的备份use chroot=no#日志文件位置，启动rsync后自动产生这个文件，无需提前创建log file=/var/log/rsyncd.log#pid文件的存放位置pid file=/var/run/rsyncd.pid#支持max connections参数的锁文件lock file=/var/run/rsyncd.lock#用户认证配置文件，里面保存用户名称和密码 需要创建（可选）secrets file=/etc/rsync.pass#允许进行数据同步的客户端IP地址，可以设置多个，用英文状态下逗号隔开,可设置所有hosts allow= *#设置rsync服务端文件为读写权限read only = no#不显示rsync服务端资源列表list = no[forharbor]#需要备份的源主机数据目录路径path = /data/registry#执行数据同步的用户名，可以设置多个，用英文状态下逗号隔开 可选配置auth users = forgitlab (4)创建认证文件如果在rsyncd服务中定义了可选配置，则需创建认证文件。 123[root@localhost ~]# vim /etc/rsync.passforgitlab:pass123chmod 600 /etc/rsync.pass (5)启动Rsyncd服务 12[root@localhost ~]# systemctl start rsyncd[root@localhost ~]# systemctl enable rsyncd (6)开启rsyncd服务端口 123[root@localhost ~]# firewall-cmd --permanent --add-port=873/tcp[root@localhost ~]# firewall-cmd --permanent --add-port=873/udp[root@localhost ~]# firewall-cmd --reload slave-130 master备节点（1）安装 1yum localinstall *.rpm (2)创建认证文件 123456 [root@localhost ~]# vim /etc/rsync.pass pass123 chmod 600 /etc/rsync.pass(3)手动测试``` bash [root@localhost ~]# rsync -avzrtlp --progress --delete --password-file=/etc/rsync.pass forharbor@192.168.117.129::forharbor /data/registry (4)自动执行 12[root@localhost ~]# crontab -e*/5 * * * * rsync -avzrtlp --progress --delete --password-file=/etc/rsync.pass forgitlab@192.168.117.129::forgitlab /data/registry 【说明1】每5分钟同步一次。【说明2】如果出现目录可以同步，文本文件类型的文件不能同步,请检查SELinux是否关闭1、临时关闭：输入命令setenforce 0，重启系统后还会开启。2、永久关闭：输入命令vi &#x2F;etc&#x2F;selinux&#x2F;config，将SELINUX&#x3D;enforcing改为SELINUX&#x3D;disabled，然后保存退出。selinux是 2.6 版本的 Linux 系统内核中提供的强制访问控制(MAC）系统。算是内置的安全系统，防火墙什么的应该算是外配的。","tags":[{"name":"Rsyncd","slug":"Rsyncd","permalink":"https://wangdj104.github.io/tags/Rsyncd/"}]},{"title":"jenkins高可用部署文档","date":"2019-07-15T02:49:04.000Z","path":"2019/07/15/jenkins高可用部署文档/","text":"本文目标:配置jenkins的一主一从（ 192.168.117.129 MASTER (jenkins master),192.168.117.130 BACKUP (jenkins master),192.168.117.131 (jenkins slave) )。 部署jenkin在三台机器上按照jenkins部署文档，部署jenkins。 安装keepalived服务Jenkins-master (1)为keepalived开启转发 1234[root@localhost ~]# vim /etc/sysctl.conf net.ipv4.ip_forward = 1 net.ipv4.ip_nonlocal_bind = 1[root@localhost ~]# systemctl restart keepalived (2)修改keepalived的配置文件 123456789101112131415161718192021222324252627282930vim /etc/keepalived/keepalived.confvrrp_script chk_apiserver &#123; script &quot;/usr/bin/curl -k http://192.168.117.129:8088 --connect-timeout 3&quot; interval 10 weight -10 fall 3 rise 2&#125;vrrp_instance VI_1 &#123; state MASTER interface ens33 virtual_router_id 51 mcast_src_ip 192.168.117.129 priority 101 authentication &#123; auth_type PASS auth_pass gitlab &#125; track_script &#123; chk_apiserver &#125; virtual_ipaddress &#123; 192.168.117.208 &#125; notify_master &quot;/home/jenkins/start.sh&quot; notify_backup &quot;/home/jenkins/stop.sh&quot;&#125; (3)重启keepalived 1systemctl restart keepalived Jenkins-slave按照上述步骤配置keepalived,只需要把其中的ip换成slave机器的ip,priority值要比master小。 安装rsync安装包下载链接: https://pan.baidu.com/s/1s1nPjzM9w9M8e0V4EqcZog 提取码: gv5w master-129（1）安装 1yum localinstall *.rpm （2）)创建用户名和密码 12useradd forjenkinspasswd forjenkins 给已创建的用户forjenkins设置密码为pass123 （3）修改配置文件 123456789101112131415161718192021222324252627282930vim /etc/rsyncd.conf#设置rsync运行权限为rootuid=root#设置rsync运行权限为rootgid=root#最大连接数max connections=3#默认为true，修改为no，增加对目录文件软连接的备份use chroot=no#日志文件位置，启动rsync后自动产生这个文件，无需提前创建log file=/var/log/rsyncd.log#pid文件的存放位置pid file=/var/run/rsyncd.pid#支持max connections参数的锁文件lock file=/var/run/rsyncd.lock#用户认证配置文件，里面保存用户名称和密码 需要创建（可选）secrets file=/etc/rsync.pass#允许进行数据同步的客户端IP地址，可以设置多个，用英文状态下逗号隔开,可设置所有hosts allow= *#设置rsync服务端文件为读写权限read only = no#不显示rsync服务端资源列表list = no[forjenkins]#需要备份的源主机数据目录路径path = /home/jenkins/work#执行数据同步的用户名，可以设置多个，用英文状态下逗号隔开 可选配置auth users = forjenkins (4)创建认证文件如果在rsyncd服务中定义了可选配置，则需创建认证文件。 123[root@localhost ~]# vim /etc/rsync.passforjenkins:pass123chmod 600 /etc/rsync.pass (5)启动Rsyncd服务 12[root@localhost ~]# systemctl start rsyncd[root@localhost ~]# systemctl enable rsyncd (6)开启rsyncd服务端口 123[root@localhost ~]# firewall-cmd --permanent --add-port=873/tcp[root@localhost ~]# firewall-cmd --permanent --add-port=873/udp[root@localhost ~]# firewall-cmd --reload master-130 master备节点（1）安装 1yum localinstall *.rpm (2)创建认证文件 123[root@localhost ~]# vim /etc/rsync.passpass123chmod 600 /etc/rsync.pass (3)手动测试 1[root@localhost ~]# rsync -vzrtp --progress --delete --password-file=/etc/rsync.pass forjenkins@192.168.117.129::forjenkins /home/jenkins/work (4)自动执行 12[root@localhost ~]# crontab -e*/5 * * * * rsync -avzrtlp --progress --delete --password-file=/etc/rsync.pass forjenkins@192.168.117.129::forjenkins /home/jenkins/work 【说明1】每5分钟同步一次。【说明2】如果出现目录可以同步，文本文件类型的文件不能同步,请检查SELinux是否关闭1、临时关闭：输入命令setenforce 0，重启系统后还会开启。2、永久关闭：输入命令vi &#x2F;etc&#x2F;selinux&#x2F;config，将SELINUX&#x3D;enforcing改为SELINUX&#x3D;disabled，然后保存退出。","tags":[{"name":"Jenkins","slug":"Jenkins","permalink":"https://wangdj104.github.io/tags/Jenkins/"}]},{"title":"jenkins部署文档","date":"2019-07-15T02:48:48.000Z","path":"2019/07/15/jenkins部署文档/","text":"安装包下载链接: https://pan.baidu.com/s/1gNy0EfrHXzapwuEXlGfi8Q 提取码: er4p(1)上传到&#x2F;home目录,解压 1unzip jenkins.zip (2)查看本机的8080端口是否被占用，如果被占用，修改端口 1vim start.sh (3)启动 1./start.sh","tags":[{"name":"Jenkins","slug":"Jenkins","permalink":"https://wangdj104.github.io/tags/Jenkins/"}]},{"title":"Harbor高可用部署文档","date":"2019-07-10T12:30:46.000Z","path":"2019/07/10/Harbor高可用部署文档/","text":"本文目标:配置harbor一主二从（master 192.168.117.129,slave 192.168.117.130,192.168.117.131）,其中harbor-master外挂一主两从的数据库mysql、redis。 部署docker安装包下载链接：链接: https://pan.baidu.com/s/1dqi0IQlILkfQpjmDqxTktw 提取码: p3h31）将docker1.12.6的rpm安装包上传到&#x2F;home目录2）本地安装docker 1234cd /home/docker1.12.6yum localinstall *.rpmsystemctl start dockersystemctl enable docker #设置开机自启动 如果 yum localinstall *.rpm显示缺少相关依赖，可重新配置一个本地yum源。也可使用rpm -ivh *.rpm –nodeps –force强制安装，但尽量不要使用，因为这样方式安装的docker可能有问题，导致装harbor的时候报错。配置yum源参考：https://blog.csdn.net/shida_csdn/article/details/78477202 部署harbor（1）将harbor安装包上传到&#x2F;home,解压（2）根据实际情况修改harbor.cfg(master外挂mysql，从暂时不挂) 1vim harbor.cfg 123456789101112131415161718192021222324252627282930313233343536填写VIP或FQDN。请不要使用 localhost 或 127.0.0.1 类似的地址。Hostname = 192.168.117.129使用https协议请在`ssl_cert`和`ssl_cert_key`添加相应的证书。默认http协议。ui_url_protocol = http设置job service数量，建议50。请注意job会消耗网络、CPU和磁盘的资源max_job_workers = 50设置harbor管理员密码harbor_admin_password = Harbor12345关闭帐号注册功能。设置为off。self_registration = off设置外部数据库主机地址db_host = 192.168.117.129设置外部数据库root帐号密码db_password = 123456设置外部数据库端口db_port = 3306设置外部数据库管理员帐号db_user = root设置redis数据库地址(IP:port,权重,pwd)redis_url = 192.168.117.129:6379,1,redis-2019设置registry存储目录#registry_storage_provider can be: filesystem, s3, gcs, azure, etc.registry_storage_provider_name = filesystem#registry_storage_provider_config is a comma separated &quot;key: value&quot; pairs, e.g. &quot;key1: value, key2: value2&quot;.#Refer to https://docs.docker.com/registry/configuration/#storage for all available configuration.registry_storage_provider_config = 【说明】registry_storage_provider_name设置filesystem，必须创建&#x2F;data&#x2F;registry目录并将其所有者更改为10000:10000，因为harbor将以userID 10000和groupID 10000的形式运行。具体可参考官方文档。https://github.com/vmware/harbor/blob/master/docs/high_availability_installation_guide.md（3）在外挂的数据库中建立registry数据库，并导入harbor schema 12345mysql -uroot -p输入mysql登录密码CREATE database registry;use registry #选择要操作的数据库执行下述sql语句 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211 create table access ( access_id int NOT NULL AUTO_INCREMENT, access_code char(1), comment varchar (30), primary key (access_id));insert into access (access_code, comment) values(&#x27;M&#x27;, &#x27;Management access for project&#x27;),(&#x27;R&#x27;, &#x27;Read access for project&#x27;),(&#x27;W&#x27;, &#x27;Write access for project&#x27;),(&#x27;D&#x27;, &#x27;Delete access for project&#x27;),(&#x27;S&#x27;, &#x27;Search access for project&#x27;);create table role ( role_id int NOT NULL AUTO_INCREMENT, role_mask int DEFAULT 0 NOT NULL, role_code varchar(20), name varchar (20), primary key (role_id) );insert into role (role_code, name) values (&#x27;MDRWS&#x27;, &#x27;projectAdmin&#x27;), (&#x27;RWS&#x27;, &#x27;developer&#x27;), (&#x27;RS&#x27;, &#x27;guest&#x27;);create table user ( user_id int NOT NULL AUTO_INCREMENT, username varchar(255), email varchar(255), password varchar(40) NOT NULL, realname varchar (255) NOT NULL, comment varchar (30), deleted tinyint (1) DEFAULT 0 NOT NULL, reset_uuid varchar(40) DEFAULT NULL, salt varchar(40) DEFAULT NULL, sysadmin_flag tinyint (1), creation_time timestamp NOT NULL default CURRENT_TIMESTAMP, update_time timestamp NOT NULL default CURRENT_TIMESTAMP, primary key (user_id), UNIQUE (username), UNIQUE (email) ); insert into user (username, email, password, realname, comment, deleted, sysadmin_flag, creation_time, update_time) values (&#x27;admin&#x27;, &#x27;admin@example.com&#x27;, &#x27;&#x27;, &#x27;system admin&#x27;, &#x27;admin user&#x27;,0, 1, NOW(), NOW()),(&#x27;anonymous&#x27;,&#x27;anonymous@example.com&#x27;, &#x27;&#x27;, &#x27;anonymous user&#x27;, &#x27;anonymous user&#x27;, 1, 0, NOW(), NOW());create table project ( project_id int NOT NULL AUTO_INCREMENT, owner_id int NOT NULL, name varchar (255) NOT NULL, creation_time timestamp NOT NULL default CURRENT_TIMESTAMP, update_time timestamp NOT NULL default CURRENT_TIMESTAMP, deleted tinyint (1) DEFAULT 0 NOT NULL, primary key (project_id), FOREIGN KEY (owner_id) REFERENCES user(user_id), UNIQUE (name) );insert into project (owner_id, name, creation_time, update_time) values (1, &#x27;library&#x27;, NOW(), NOW());create table project_member ( project_id int NOT NULL, user_id int NOT NULL, role int NOT NULL, creation_time timestamp NOT NULL default CURRENT_TIMESTAMP, update_time timestamp NOT NULL default CURRENT_TIMESTAMP, PRIMARY KEY (project_id, user_id), FOREIGN KEY (role) REFERENCES role(role_id), FOREIGN KEY (project_id) REFERENCES project(project_id), FOREIGN KEY (user_id) REFERENCES user(user_id));insert into project_member (project_id, user_id, role, creation_time, update_time) values(1, 1, 1, NOW(), NOW());create table project_metadata ( id int NOT NULL AUTO_INCREMENT, project_id int NOT NULL, name varchar(255) NOT NULL, value varchar(255), creation_time timestamp default CURRENT_TIMESTAMP, update_time timestamp default CURRENT_TIMESTAMP on update CURRENT_TIMESTAMP, deleted tinyint (1) DEFAULT 0 NOT NULL, PRIMARY KEY (id), CONSTRAINT unique_project_id_and_name UNIQUE (project_id,name), FOREIGN KEY (project_id) REFERENCES project(project_id));insert into project_metadata (id, project_id, name, value, creation_time, update_time, deleted) values(1, 1, &#x27;public&#x27;, &#x27;true&#x27;, NOW(), NOW(), 0);create table access_log ( log_id int NOT NULL AUTO_INCREMENT, username varchar (255) NOT NULL, project_id int NOT NULL, repo_name varchar (256), repo_tag varchar (128), GUID varchar(64), operation varchar(20) NOT NULL, op_time timestamp NOT NULL default CURRENT_TIMESTAMP, primary key (log_id), INDEX pid_optime (project_id, op_time));create table repository ( repository_id int NOT NULL AUTO_INCREMENT, name varchar(255) NOT NULL, project_id int NOT NULL, description text, pull_count int DEFAULT 0 NOT NULL, star_count int DEFAULT 0 NOT NULL, creation_time timestamp default CURRENT_TIMESTAMP, update_time timestamp default CURRENT_TIMESTAMP on update CURRENT_TIMESTAMP, primary key (repository_id), UNIQUE (name));create table replication_policy ( id int NOT NULL AUTO_INCREMENT, name varchar(256), project_id int NOT NULL, target_id int NOT NULL, enabled tinyint(1) NOT NULL DEFAULT 1, description text, deleted tinyint (1) DEFAULT 0 NOT NULL, cron_str varchar(256), filters varchar(1024), replicate_deletion tinyint (1) DEFAULT 0 NOT NULL, start_time timestamp NULL, creation_time timestamp default CURRENT_TIMESTAMP, update_time timestamp default CURRENT_TIMESTAMP on update CURRENT_TIMESTAMP, PRIMARY KEY (id));create table replication_target ( id int NOT NULL AUTO_INCREMENT, name varchar(64), url varchar(64), username varchar(255), password varchar(128), target_type tinyint(1) NOT NULL DEFAULT 0, insecure tinyint(1) NOT NULL DEFAULT 0, creation_time timestamp default CURRENT_TIMESTAMP, update_time timestamp default CURRENT_TIMESTAMP on update CURRENT_TIMESTAMP, PRIMARY KEY (id) ); create table replication_job ( id int NOT NULL AUTO_INCREMENT, status varchar(64) NOT NULL, policy_id int NOT NULL, repository varchar(256) NOT NULL, operation varchar(64) NOT NULL, tags varchar(16384), creation_time timestamp default CURRENT_TIMESTAMP, update_time timestamp default CURRENT_TIMESTAMP on update CURRENT_TIMESTAMP, PRIMARY KEY (id), INDEX policy (policy_id), INDEX poid_uptime (policy_id, update_time) );create table replication_immediate_trigger ( id int NOT NULL AUTO_INCREMENT, policy_id int NOT NULL, namespace varchar(256) NOT NULL, on_push tinyint(1) NOT NULL DEFAULT 0, on_deletion tinyint(1) NOT NULL DEFAULT 0, creation_time timestamp default CURRENT_TIMESTAMP, update_time timestamp default CURRENT_TIMESTAMP on update CURRENT_TIMESTAMP, PRIMARY KEY (id) );create table img_scan_job ( id int NOT NULL AUTO_INCREMENT, status varchar(64) NOT NULL, repository varchar(256) NOT NULL, tag varchar(128) NOT NULL, digest varchar(128), creation_time timestamp default CURRENT_TIMESTAMP, update_time timestamp default CURRENT_TIMESTAMP on update CURRENT_TIMESTAMP, PRIMARY KEY (id));create table img_scan_overview ( id int NOT NULL AUTO_INCREMENT, image_digest varchar(128) NOT NULL, scan_job_id int NOT NULL, severity int NOT NULL default 0, components_overview varchar(2048), details_key varchar(128), creation_time timestamp default CURRENT_TIMESTAMP, update_time timestamp default CURRENT_TIMESTAMP on update CURRENT_TIMESTAMP, PRIMARY KEY(id), UNIQUE(image_digest));create table clair_vuln_timestamp ( id int NOT NULL AUTO_INCREMENT, namespace varchar(128) NOT NULL, last_update timestamp NOT NULL, PRIMARY KEY(id), UNIQUE(namespace));create table properties ( id int NOT NULL AUTO_INCREMENT, k varchar(64) NOT NULL, v varchar(128) NOT NULL, PRIMARY KEY(id), UNIQUE (k));CREATE TABLE IF NOT EXISTS `alembic_version` (`version_num` varchar(32) NOT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8;insert into alembic_version values (&#x27;1.4.0&#x27;); 启动harbor 1234cd /home/harbor./prepare./install --hadocker ps 安装keepalived服务Keepalived是一个免费开源的，用C编写的类似于layer3, 4 &amp; 7交换机制软件，具备我们平时说的第3层、第4层和第7层交换机的功能。主要提供loadbalancing（负载均衡）和high-availability（高可用）功能，负载均衡实现需要依赖Linux的虚拟服务内核模块（ipvs），而高可用是通过VRRP协议实现多台机器之间的故障转移服务。keepalived安装包下载链接: https://pan.baidu.com/s/13kW_Bz6RGSo4ewZ68BwCVA 提取码: 6ktn i)设置每台机器的虚拟ip 1ifconfig ens33:1 192.168.117.208 broadcast 192.168.117.129 netmask 255.255.255.0 up 将上述命令写在&#x2F;etc&#x2F;rc.local里进行开机自动设置执行ifconfig查看是否生效ii)修改keepalived的配置文件 12yum localinstall *.rpm或rpm -ivh *.rpm --nodeps --forcevim /etc/keepalived/keepalived.conf 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061! Configuration File for keepalivedglobal_defs &#123; router_id harborlb #更改内容&#125;vrrp_sync_groups VG1 &#123; group &#123; VI_1 &#125;&#125;vrrp_instance VI_1 &#123; state MASTER #谁被备份 interface ens33 #网卡的名称 virtual_router_id 51 #同一集群ID应该一致 priority 100 #优先级，数字越高优先级越高，决定主备 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.117.208 #虚拟IP，与现有IP应不冲突 &#125; track_interface &#123; ens33 &#125; notify_master &quot;/etc/keepalived/redis_master.sh 192.168.117.129 6379&quot; notify_backup &quot;/etc/keepalived/redis_backup.sh 192.168.117.129 6379&quot;&#125;virtual_server 192.168.117.208 80 &#123; delay_loop 15 lb_algo rr lb_kind DR protocol TCP nat_mask 255.255.255.0 persistence_timeout 10 real_server 192.168.117.129 80 &#123; weight 10 MISC_CHECK &#123; misc_path &quot;/usr/local/bin/check.sh 192.168.117.129&quot; misc_timeout 5 &#125; &#125; real_server 192.168.117.130 80 &#123; weight 10 MISC_CHECK &#123; misc_path &quot;/usr/local/bin/check.sh 192.168.117.130&quot; misc_timeout 5 &#125; &#125; real_server 192.168.117.131 80 &#123; weight 10 MISC_CHECK &#123; misc_path &quot;/usr/local/bin/check.sh 192.168.117.131&quot; misc_timeout 5 &#125; &#125;&#125; iii)设置harbor检查脚本 1vim /usr/local/bin/check.sh 1234567891011121314151617181920212223242526272829303132333435363738394041424344#!/bin/bashset -e#get protocol#LOG=/var/log/keepalived_check.lognodeip=$1nodeaddress=&quot;http://$&#123;nodeip&#125;&quot;http_code=`curl -s -o /dev/null -w &quot;%&#123;http_code&#125;&quot; $&#123;nodeaddress&#125;`if [ $http_code == 200 ] ; then protocol=&quot;http&quot;elif [ $http_code == 301 ]then protocol=&quot;https&quot;else# echo &quot;`date +&quot;%Y-%m-%d %H:%M:%S&quot;` $1, CHECK_CODE=$http_code&quot; &gt;&gt; $LOG exit 1fisysteminfo=`curl -k -o - -s $&#123;protocol&#125;://$&#123;nodeip&#125;/api/systeminfo`echo $systeminfo | grep &quot;registry_url&quot;if [ $? != 0 ] ; then exit 1fi#TODO need to check Clair, but currently Clair status api is unreachable from LB.# echo $systeminfo | grep &quot;with_clair&quot; | grep &quot;true&quot;# if [ $? == 0 ] ; then# clair is enabled# do some clair check# else# clair is disabled# fi#check top apihttp_code=`curl -k -s -o /dev/null -w &quot;%&#123;http_code&#125;\\n&quot; $&#123;protocol&#125;://$&#123;nodeip&#125;/api/repositories/top`set +eif [ $http_code == 200 ] ; then exit 0else exit 1fi 为检查脚本添加执行权限 1chmod +x /usr/local/bin/check.sh iv)为keepalived开启转发 123456[root@localhost ~]# vim /etc/sysctl.conf net.ipv4.ip_forward = 1 net.ipv4.ip_nonlocal_bind = 1[root@localhost ~]# sysctl -p[root@localhost ~]# systemctl restart keepalived （5）验证","tags":[{"name":"Harbor","slug":"Harbor","permalink":"https://wangdj104.github.io/tags/Harbor/"}]},{"title":"Gitlab高可用部署文档","date":"2019-07-10T12:30:29.000Z","path":"2019/07/10/Gitlab高可用部署文档/","text":"本文目标:配置gitlab一主二从（master 192.168.117.129,slave 192.168.117.130,192.168.117.131）,其中gitlab-master外挂一主两从的数据库postgresql,gitlab-slave130挂载postgresql-130,gitlab-slave131挂载postgresql-131。 在三台机器上部署gitlab下载安装包链接: https://pan.baidu.com/s/1geCvvWl54kp_5AJ6O8bJ3w 提取码: tser其中gitlab-ce-zh110104.tar为gitlab-11.1.4,gitlab-ce-zh100604.tar为gitlab-10.6.4(1)将安装包上传到&#x2F;home目录，并加载镜像 12docker load -i gitlab-ce-zh110104.tarvim /home/gitlab/docker-compose.yml #新建yml文件 12345678910111213141516171819202122232425262728version: &#x27;2&#x27;services: gitlab: image: &#x27;twang2218/gitlab-ce-zh:11.1.4&#x27; restart: unless-stopped hostname: &#x27;192.168.117.129&#x27; environment: TZ: &#x27;Asia/Shanghai&#x27; GITLAB_OMNIBUS_CONFIG: | external_url &#x27;http://192.168.117.129&#x27; gitlab_rails[&#x27;time_zone&#x27;] = &#x27;Asia/Shanghai&#x27; gitlab_rails[&#x27;gitlab_shell_ssh_port&#x27;] = 54322 postgresql[&#x27;enable&#x27;] = false gitlab_rails[&#x27;db_adapter&#x27;] = &quot;postgresql&quot; gitlab_rails[&#x27;db_encoding&#x27;] = &quot;utf8&quot; gitlab_rails[&#x27;db_database&#x27;] = &quot;gitlabtest&quot; gitlab_rails[&#x27;db_username&#x27;] = &quot;postgres&quot; gitlab_rails[&#x27;db_password&#x27;] = &quot;postgres&quot; gitlab_rails[&#x27;db_host&#x27;] = &quot;192.168.117.129&quot; gitlab_rails[&#x27;db_port&#x27;] = 5432 ports: - &#x27;8080:80&#x27; - &#x27;843:443&#x27; - &#x27;54322:22&#x27; volumes: - &#x27;/data/gitlab/config:/etc/gitlab&#x27; - &#x27;/data/gitlab/logs:/var/log/gitlab&#x27; - &#x27;/data/gitlab/config/gitlab/data:/var/opt/gitlab&#x27; 注意：上述postgresql挂载分别为三台机器上各自的postgresql 1docker-compose up -d 安装keepalived服务Gitlab-master(1)为keepalived开启转发 1234[root@localhost ~]# vim /etc/sysctl.conf net.ipv4.ip_forward = 1 net.ipv4.ip_nonlocal_bind = 1[root@localhost ~]# systemctl restart keepalived (2)修改keepalived的配置文件 12345678910111213141516171819202122 vim /etc/keepalived/keepalived.confvrrp_script chk_gitlab&#123; script &quot;/etc/keepalived/check-gitlab.sh&quot; interval 2 &#125;vrrp_instance VI_1 &#123; state MASTER interface ens33 virtual_router_id 51 priority 101 authentication &#123; auth_type PASS auth_pass gitlab &#125; track_script &#123; chk_gitlab &#125; virtual_ipaddress &#123; 192.168.117.208 &#125;&#125; (3)创建检测脚本 1234567#!/bin/bashreturn_code=`curl -s -w &quot;%&#123;http_code&#125;&quot; -o /dev/null http://192.168.117.129`if [[ $return_code -ne 200 ]]; thensystemctl stop keepalivedfi (4)重启keepalived 1systemctl restart keepalived Gitlab-slave按照上述步骤配置keepalived,只需要把其中的ip换成slave机器的ip,priority值要比master小。 安装Rsyncd服务安装包下载链接: https://pan.baidu.com/s/1s1nPjzM9w9M8e0V4EqcZog 提取码: gv5wmaster-129（1）安装 1yum localinstall *.rpm （2）)创建用户名和密码 12useradd forgitlab 创建用户forgitlabpasswd forgitlab 给已创建的用户forgitlab设置密码为pass123 （3）修改配置文件 123456789101112131415161718192021222324252627282930vim /etc/rsyncd.conf#设置rsync运行权限为rootuid=root#设置rsync运行权限为rootgid=root#最大连接数max connections=3#默认为true，修改为no，增加对目录文件软连接的备份use chroot=no#日志文件位置，启动rsync后自动产生这个文件，无需提前创建log file=/var/log/rsyncd.log#pid文件的存放位置pid file=/var/run/rsyncd.pid#支持max connections参数的锁文件lock file=/var/run/rsyncd.lock#用户认证配置文件，里面保存用户名称和密码 需要创建（可选）secrets file=/etc/rsync.pass#允许进行数据同步的客户端IP地址，可以设置多个，用英文状态下逗号隔开,可设置所有hosts allow= *#设置rsync服务端文件为读写权限read only = no#不显示rsync服务端资源列表list = no[forgitlab]#需要备份的源主机数据目录路径path = /data/gitlab/data/git-data#执行数据同步的用户名，可以设置多个，用英文状态下逗号隔开 可选配置auth users = forgitlab (4)创建认证文件如果在rsyncd服务中定义了可选配置，则需创建认证文件。 123[root@localhost ~]# vim /etc/rsync.passforgitlab:pass123chmod 600 /etc/rsync.pass (5)启动Rsyncd服务 12[root@localhost ~]# systemctl start rsyncd[root@localhost ~]# systemctl enable rsyncd (6)开启rsyncd服务端口 123[root@localhost ~]# firewall-cmd --permanent --add-port=873/tcp[root@localhost ~]# firewall-cmd --permanent --add-port=873/udp[root@localhost ~]# firewall-cmd --reload slave-130 master备节点（1）安装 1yum localinstall *.rpm (2)创建认证文件 123456 [root@localhost ~]# vim /etc/rsync.pass pass123 chmod 600 /etc/rsync.pass(3)手动测试``` bash [root@localhost ~]# rsync -avzrtlp --progress --delete --password-file=/etc/rsync.pass forgitlab@192.168.117.129::forgitlab /data/gitlab/data/git-data (4)自动执行 12[root@localhost ~]# crontab -e*/5 * * * * rsync -avzrtlp --progress --delete --password-file=/etc/rsync.pass forgitlab@192.168.117.129::forgitlab /data/gitlab/data/git-data 【说明1】每5分钟同步一次。【说明2】如果出现目录可以同步，文本文件类型的文件不能同步,请检查SELinux是否关闭1、临时关闭：输入命令setenforce 0，重启系统后还会开启。2、永久关闭：输入命令vi &#x2F;etc&#x2F;selinux&#x2F;config，将SELINUX&#x3D;enforcing改为SELINUX&#x3D;disabled，然后保存退出。","tags":[{"name":"Gitlab","slug":"Gitlab","permalink":"https://wangdj104.github.io/tags/Gitlab/"}]},{"title":"Postgresql高可用部署文档","date":"2019-07-10T12:29:25.000Z","path":"2019/07/10/Postgresql高可用部署文档/","text":"由于Redhat或CentOS等发行版本自带postgresql数据库版本为9.2，由于gitlab 9+版本以上需要 postgresq 9.6版本，因此本文档安装(postgresql-10.1)。PostgreSQL是一个功能强大,开源对象关系型数据库系统。配置一主二从（master 192.168.117.129,slave 192.168.117.130,192.168.117.131） 在这三台机器上分别安装Postgresql按照Postgresql部署文档在这三台机器上安装Postgresql 配置PostgreSQL Master节点i)设置replication用户 123456su - postgrescd /usr/local/pgsql/bin./psqlCREATE ROLE repluser WITH REPLICATION PASSWORD &#x27;postgres&#x27; LOGIN;\\qexit ii)停止postgresql服务 1systemctl stop postgresql iii)修改postgresql配置，内容如下 123456vim /usr/local/pgsql/data/postgresql.confwal_level = hot_standbymax_wal_senders = 5wal_keep_segments = 32archive_mode = onarchive_command = &#x27;cp %p /usr/local/pgsql/archive/%f&#x27; iv)创建归档目录 12mkdir /usr/local/pgsql/archivechown -R postgres:postgres /usr/local/pgsql/archive v)允许replication连接数据库 1vim /usr/local/pgsql/data/pg_hba.conf 添加如下内容: 123host replication repluser 192.168.117.0/24 md5host all repluser 192.168.117.0/24 trusthost all all 0.0.0.0/0 md5 vi)启动postgresql数据库服务 1systemctl start postgresql 配置PostgreSQL Slave节点i)停止postgresql数据库 1systemctl stop postgresql ii)清空数据目录 1rm -rf /usr/local/pgsql/data/* iii)初始化备分数据库 1/usr/local/pgsql/bin/pg_basebackup -h 192.168.117.129 -D /usr/local/pgsql/data -P -U repluser -X stream iv)修改配置 1vim /usr/local/pgsql/data/postgresql.conf 修改项为：hot_standby &#x3D; onv)创建恢复配置 123456 cp /usr/local/pgsql/share/recovery.conf.sample /usr/local/pgsql/data/recovery.conf vim /usr/local/pgsql/data/recovery.conf standby_mode = &#x27;on&#x27;primary_conninfo = &#x27;host=92.168.117.129 port=5432 user=repluser password=postgres&#x27; trigger_file = &#x27;/usr/local/pgsql/trigger&#x27; restore_command = &#x27;cp /usr/local/pgsql/archive/%f %p&#x27; vi)启动数据库 1systemctl restart postgresql 若果用上述命令,不报错,但查不到postgresql的进程,则试着使用下述命令 1service postgresql start 若出现下述问题 1chown -R postgres:postgres /usr/local/pgsql/data/ 重新执行即可查看到postgresql进程（ps:不知道为什么systemctl start postgresql和service postgresql start不能起到相同的效果） 测试–主库查看 1234su - postgrescd /usr/local/pgsql/bin./psqlselect client_addr,sync_state from pg_stat_replication; 1postgres=# select * from pg_stat_replication; –主库创建一个数据库 123postgres=# create database test;CREATE DATABASEpostgres=# \\l // 查看当前所有数据库 –查看从库中是否已经同步 1234su - postgrescd /usr/local/pgsql/bin./psqlpostgres=# \\l Linux 服务管理两种方式service和systemctl比较i)service命令service命令其实是去&#x2F;etc&#x2F;init.d目录下，去执行相关程序。其中脚本需要我们自己编写。ii)systemctl命令systemd是Linux系统最新的初始化系统(init),作用是提高系统的启动速度，尽可能启动较少的进程，尽可能更多进程并发启动。systemd对应的进程管理命令是systemctl1)systemctl命令兼容了service即systemctl也会去&#x2F;etc&#x2F;init.d目录下，查看，执行相关程序2)systemctl命令管理systemd的资源Unitsystemd的Unit放在目录&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system(Centos)或&#x2F;etc&#x2F;systemd&#x2F;system(Ubuntu)","tags":[{"name":"Postgresql","slug":"Postgresql","permalink":"https://wangdj104.github.io/tags/Postgresql/"}]},{"title":"Redis高可用部署文档","date":"2019-07-10T12:28:39.000Z","path":"2019/07/10/Redis高可用部署文档/","text":"配置一主二从（master 192.168.117.129,slave 192.168.117.130,192.168.117.131） 在三台机器上分别安装redisi)下载安装包 1wget http://download.redis.io/releases/redis-4.0.6.tar.gz ii)上传并解压到&#x2F;home下 1tar -xzvf redis-4.0.6.tar.gz iii)编译安装 123cd /home/redis-4.0.6make MALLOC=libcmake install iv)修改redis配置文件 12echo &quot;&quot;&gt;redis.confvim redis.conf master(192.168.117.129)修改如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#bind 0.0.0.0protected-mode yesport 6379tcp-backlog 511timeout 0tcp-keepalive 300daemonize yessupervised systemdpidfile &quot;/var/run/redis_6379.pid&quot;loglevel noticelogfile &quot;/home/redis-4.0.6/redis-master.log&quot;databases 16save 900 1save 300 10save 60 10000stop-writes-on-bgsave-error yesrdbcompression yesrdbchecksum yesdbfilename &quot;dump.rdb&quot;dir &quot;/home/redis-4.0.6&quot;masterauth &quot;redis-2019&quot;slave-serve-stale-data yesslave-read-only norepl-diskless-sync norepl-diskless-sync-delay 5repl-disable-tcp-nodelay noslave-priority 100requirepass redis-2019appendonly noappendfilename &quot;appendonly.aof&quot;appendfsync everysecno-appendfsync-on-rewrite noauto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mbaof-load-truncated yeslua-time-limit 5000slowlog-log-slower-than 10000slowlog-max-len 128latency-monitor-threshold 0notify-keyspace-events &quot;&quot;hash-max-ziplist-entries 512hash-max-ziplist-value 64list-max-ziplist-size -2list-compress-depth 0set-max-intset-entries 512zset-max-ziplist-entries 128zset-max-ziplist-value 64hll-sparse-max-bytes 3000activerehashing yesclient-output-buffer-limit normal 0 0 0client-output-buffer-limit slave 256mb 64mb 60client-output-buffer-limit pubsub 32mb 8mb 60hz 10aof-rewrite-incremental-fsync yesmaxclients 2000 slave(192.168.117.130,192.168.117.131)修改如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#bind 0.0.0.0protected-mode yesport 6379tcp-backlog 511timeout 0tcp-keepalive 300daemonize yessupervised systemdpidfile /var/run/redis_6379.pidloglevel noticelogfile &quot;/home/redis-4.0.6/redis-slave.log&quot;databases 16save 900 1save 300 10save 60 10000stop-writes-on-bgsave-error yesrdbcompression yesrdbchecksum yesdbfilename dump.rdbdir /home/redis-4.0.6/slaveof 192.168.117.129 6379 #主数据库ip和端口masterauth redis-2019 #主数据库密码slave-serve-stale-data yesslave-read-only yesrepl-diskless-sync norepl-diskless-sync-delay 5repl-disable-tcp-nodelay noslave-priority 100requirepass redis-2019appendonly noappendfilename &quot;appendonly.aof&quot;appendfsync everysecno-appendfsync-on-rewrite noauto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mbaof-load-truncated yeslua-time-limit 5000slowlog-log-slower-than 10000slowlog-max-len 128latency-monitor-threshold 0notify-keyspace-events &quot;&quot;hash-max-ziplist-entries 512hash-max-ziplist-value 64list-max-ziplist-size -2list-compress-depth 0set-max-intset-entries 512zset-max-ziplist-entries 128zset-max-ziplist-value 64hll-sparse-max-bytes 3000activerehashing yesclient-output-buffer-limit normal 0 0 0client-output-buffer-limit slave 256mb 64mb 60client-output-buffer-limit pubsub 32mb 8mb 60hz 10aof-rewrite-incremental-fsync yes v)创建redis用户组和用户 1234groupadd redisuseradd -r -g redis redischown -R redis:redis /home/redis-4.0.6chmod -R 770 /home/redis-4.0.6 vi)配置Redis的systemd服务,同时在master和slave配置 12345678910111213141516171819vim /etc/systemd/system/redis.service[Unit]Description=Redis In-Memory Data Store[Service]User=redisGroup=redisType=forkingExecStart=/home/redis-4.0.6/src/redis-server /home/redis-4.0.6/redis.confExecStop=/home/redis-4.0.6/src/redis-cli -h 127.0.0.1 -p 6379 shutdownRestart=always[Install]WantedBy=multi-user.target vii)启动redis服务 123systemctl enable redis.servicesystemctl start redis.service viii)配置防火墙redis端口 12firewall-cmd --permanent --add-port=6739/tcpfirewall-cmd --reload 安装keepalived服务1)安装keepalived服务的主机都需要配置端口转发 123[root@localhost ~]# echo &quot;net.ipv4.ip_forward = 1&quot; &gt;&gt; /etc/sysctl.conf[root@localhost ~]# sysctl -pnet.ipv4.ip_forward = 1 2)Redis-Master节点安装keepalived服务i)keepalived安装包下载链接: https://pan.baidu.com/s/13kW_Bz6RGSo4ewZ68BwCVA 提取码: 6ktnii)修改keepalived的配置文件 12yum localinstall *.rpm或rpm -ivh *.rpm --nodeps --forcevim /etc/keepalived/keepalived.conf 添加如下内容 123456789101112131415161718192021vrrp_script chk_redis &#123; script &quot;/etc/keepalived/redis_check.sh&quot; interval 2&#125;vrrp_instance VI_1 &#123; state MASTER interface eth0 virtual_router_id 51 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.117.208/32 &#125; notify_master &quot;/etc/keepalived/redis_master.sh 192.168.117.129 6379&quot; notify_backup &quot;/etc/keepalived/redis_backup.sh 192.168.117.129 6379&quot;&#125; iii)增加redis_master.sh文件 12345678910111213141516171819202122vim /etc/keepalived/redis_master.sh#!/bin/bashREDISCLI=&quot;/home/redis-4.0.6/src/redis-cli -a redis-2019&quot;LOGFILE=&quot;/etc/keepalived/keepalived-redis-state.log&quot;pid=$$host=$1port=$2echo &quot;`date +&#x27;%Y-%m-%d %H:%M:%S&#x27;`|$pid|state:[slaver]&quot; &gt;&gt; $LOGFILE 2&gt;&amp;1echo &quot;`date +&#x27;%Y-%m-%d %H:%M:%S&#x27;`|$pid|state:[slaver] Run &#x27;SLAVEOF $host $port&#x27;&quot; &gt;&gt; $LOGFILE 2&gt;&amp;1$REDISCLI SLAVEOF $host $port&gt;&gt; $LOGFILE 2&gt;&amp;1echo &quot;`date +&#x27;%Y-%m-%d %H:%M:%S&#x27;`|$pid|state:[slaver] wait 10 sec for data sync from old master&quot; &gt;&gt; $LOGFILE 2&gt;&amp;1sleep 10echo &quot;`date +&#x27;%Y-%m-%d %H:%M:%S&#x27;`|$pid|state:[slaver] data rsync from old mater ok...&quot; &gt;&gt; $LOGFILE 2&gt;&amp;1echo &quot;`date +&#x27;%Y-%m-%d %H:%M:%S&#x27;`|$pid|state:[master] Run slaveof no one,close master/slave&quot; &gt;&gt; $LOGFILE 2&gt;&amp;1$REDISCLI SLAVEOF NO ONE &gt;&gt; $LOGFILE 2&gt;&amp;1echo &quot;`date +&#x27;%Y-%m-%d %H:%M:%S&#x27;`|$pid|state:[master] wait other slave connect....&quot; &gt;&gt; $LOGFILE 2&gt;&amp;1 iv)增加redis_backup.sh文件 123456789101112131415vim /etc/keepalived/redis_backup.sh#!/bin/bashREDISCLI=&quot;/home/redis-4.0.6/src/redis-cli -a redis-2019&quot;LOGFILE=&quot;/etc/keepalived/keepalived-redis-state.log&quot;pid=$$host=$1port=$2echo &quot;`date +&#x27;%Y-%m-%d %H:%M:%S&#x27;`|$pid|state:[master] Being slave state...&quot; &gt;&gt; $LOGFILE 2&gt;&amp;1echo &quot;`date +&#x27;%Y-%m-%d %H:%M:%S&#x27;`|$pid|state:[master] wait 10 sec for data sync from old master&quot; &gt;&gt; $LOGFILE 2&gt;&amp;1sleep 10echo &quot;`date +&#x27;%Y-%m-%d %H:%M:%S&#x27;`|$pid|state:[master] data rsync from old mater ok...&quot; &gt;&gt; $LOGFILE 2&gt;&amp;1echo &quot;`date +&#x27;%Y-%m-%d %H:%M:%S&#x27;`|$pid|state:[slaver] Run &#x27;SLAVEOF $host $port&#x27;&quot; &gt;&gt; $LOGFILE 2&gt;&amp;1$REDISCLI SLAVEOF $host $port &gt;&gt; $LOGFILE 2&gt;&amp;1echo &quot;`date +&#x27;%Y-%m-%d %H:%M:%S&#x27;`|$pid|state:[slaver] slave connect to $host ok...&quot; &gt;&gt; $LOGFILE 2&gt;&amp;1 3)Redis-Slave节点安装keepalived服务i)修改keepalived的配置文件 12yum localinstall *.rpm或rpm -ivh *.rpm --nodeps --forcevim /etc/keepalived/keepalived.conf 添加如下内容 12345678910111213141516171819202122232425vrrp_script chk_redis &#123; script &quot;/etc/keepalived/redis_check.sh&quot; interval 5 timeout 2 fall 3&#125;vrrp_instance VI_1 &#123; state BACKUP interface ens33 virtual_router_id 52 priority 90 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.117.209/32 &#125; notify_master &quot;/etc/keepalived/redis_master.sh 192.168.117.130 6379&quot; notify_backup &quot;/etc/keepalived/redis_backup.sh 192.168.117.130 6379&quot;&#125; iii)增加redis_master.sh文件 123456789101112131415161718192021vim /etc/keepalived/redis_master.sh#!/bin/bashREDISCLI=&quot;/home/redis-4.0.6/src/redis-cli -a redis-2019&quot;LOGFILE=&quot;/etc/keepalived/keepalived-redis-state.log&quot;pid=$$host=$1port=$2echo &quot;`date +&#x27;%Y-%m-%d %H:%M:%S&#x27;`|$pid|state:[slaver]&quot; &gt;&gt; $LOGFILE 2&gt;&amp;1echo &quot;`date +&#x27;%Y-%m-%d %H:%M:%S&#x27;`|$pid|state:[slaver] Run &#x27;SLAVEOF $host $port&#x27;&quot; &gt;&gt; $LOGFILE 2&gt;&amp;1$REDISCLI SLAVEOF $host $port&gt;&gt; $LOGFILE 2&gt;&amp;1echo &quot;`date +&#x27;%Y-%m-%d %H:%M:%S&#x27;`|$pid|state:[slaver] wait 10 sec for data sync from old master&quot; &gt;&gt; $LOGFILE 2&gt;&amp;1sleep 10echo &quot;`date +&#x27;%Y-%m-%d %H:%M:%S&#x27;`|$pid|state:[slaver] data rsync from old mater ok...&quot; &gt;&gt; $LOGFILE 2&gt;&amp;1echo &quot;`date +&#x27;%Y-%m-%d %H:%M:%S&#x27;`|$pid|state:[master] Run slaveof no one,close master/slave&quot; &gt;&gt; $LOGFILE 2&gt;&amp;1$REDISCLI SLAVEOF NO ONE &gt;&gt; $LOGFILE 2&gt;&amp;1echo &quot;`date +&#x27;%Y-%m-%d %H:%M:%S&#x27;`|$pid|state:[master] wait other slave connect....&quot; &gt;&gt; $LOGFILE 2&gt;&amp;1 iv)增加redis_backup.sh文件 12345678910111213141516vim /etc/keepalived/redis_backup.sh#!/bin/bashREDISCLI=&quot;/home/redis-4.0.6/src/redis-cli -a redis-2019&quot;LOGFILE=&quot;/etc/keepalived/keepalived-redis-state.log&quot;pid=$$host=$1port=$2echo &quot;`date +&#x27;%Y-%m-%d %H:%M:%S&#x27;`|$pid|state:[master] Being slave state...&quot; &gt;&gt; $LOGFILE 2&gt;&amp;1echo &quot;`date +&#x27;%Y-%m-%d %H:%M:%S&#x27;`|$pid|state:[master] wait 10 sec for data sync from old master&quot; &gt;&gt; $LOGFILE 2&gt;&amp;1sleep 10echo &quot;`date +&#x27;%Y-%m-%d %H:%M:%S&#x27;`|$pid|state:[master] data rsync from old mater ok...&quot; &gt;&gt; $LOGFILE 2&gt;&amp;1echo &quot;`date +&#x27;%Y-%m-%d %H:%M:%S&#x27;`|$pid|state:[slaver] Run &#x27;SLAVEOF $host $port&#x27;&quot; &gt;&gt; $LOGFILE 2&gt;&amp;1$REDISCLI SLAVEOF $host $port &gt;&gt; $LOGFILE 2&gt;&amp;1echo &quot;`date +&#x27;%Y-%m-%d %H:%M:%S&#x27;`|$pid|state:[slaver] slave connect to $host ok...&quot; &gt;&gt; $LOGFILE 2&gt;&amp;1 4）验证i)使用redis-cli连接主数据库并输入密码登录 123[root@Master bin]# /home/redis-4.0.6/src/redis-cli -p 6379127.0.0.1:6379&gt; auth redis-2019OK 查看主数据库info属性: 1127.0.0.1:6379&gt; info ii)使用redis-cli连接从数据库并输入密码登录 123[root@Master bin]# /home/redis-4.0.6/src/redis-cli -p 6379127.0.0.1:6379&gt; auth redis-2019OK 查看从数据库info属性： 1127.0.0.1:6379&gt; info iii)测试如果上述两步都成功，在主数据库录入一条数据，在从库中查询主库插入数据 12127.0.0.1:6379&gt; set num 13ok 从库中查询 12127.0.0.1:6379&gt; get num&quot;13&quot; 同步成功5)配置防火墙上vrrp协议 1234567[root@localhost ~]# firewall-cmd --direct --permanent --add-rule ipv4 filter INPUT 0 \\ --in-interface enp0s8 --destination 224.0.0.18 --protocol vrrp -j ACCEPT[root@localhost ~]# firewall-cmd --direct --permanent --add-rule ipv4 filter OUTPUT 0 \\ --out-interface enp0s8 --destination 224.0.0.18 --protocol vrrp -j ACCEPT[root@localhost ~]# firewall-cmd –reload 6)Redis运维(主备切换) Redis主备服务切换是自动完成。切换至redis slave时，redis进入只读模式。","tags":[{"name":"Redis","slug":"Redis","permalink":"https://wangdj104.github.io/tags/Redis/"}]},{"title":"mysql高可用部署文档","date":"2019-07-09T07:31:26.000Z","path":"2019/07/09/Mysql高可用部署文档/","text":"配置mysql主从同步(一主两从)（Master：192.168.117.129，Slave1：192.168.117.130，Slave2:192.168.117.131） 在master库创建授权账户（以192.168.117.130为例，131同理）1234mysql -u root -pPassword: 输入密码GRANT REPLICATION SLAVE ON *.* TO &#x27;slave1&#x27;@&#x27;192.168.117.130&#x27; IDENTIFIED BY &#x27;slave1&#x27;;GRANT REPLICATION SLAVE ON *.* TO &#x27;slave2&#x27;@&#x27;192.168.117.131&#x27; IDENTIFIED BY &#x27;slave2&#x27;; 修改Master配置：i) 编辑&#x2F;etc&#x2F;my.cnf配置文件123[mysqld] #在mysqld下面添加如下两行log-bin = mysql-bin #slave会基于此log-bin来做replicationserver-id = 129 #master的标示，唯一ID，一般采用IP最后一段 ii)重启Master数据库1service mysql restart iii)查看状态1show master status; 修改Slave配置：（以192.168.117.130为例，131同理）编辑&#x2F;etc&#x2F;my.cnf文件1234[mysqld]log-bin=mysql-bin #slave会基于此log-bin来做replicationserver-id=130 #master的标示，唯一ID，一般采用IP最后一段slave-skip-errors=all #跳过所有错误 保存退出重新启动mysql服务1service mysql restart 分别进入两台机器的Slave mysql控制台，执行下述语句（以192.168.117.130为例，131同理）12345678mysql -uroot -pstop slave;change master to master_host=&#x27;192.168.117.129&#x27;,master_user=&#x27;root&#x27;,master_password=&#x27;123456&#x27;,master_log_file=&#x27;mysql-bin.000005&#x27;,master_log_pos=154;start slave; 分别查看从库状态：1show slave status\\G 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960mysql&gt; show slave status\\G*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 192.168.117.129 Master_User: root Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000005 Read_Master_Log_Pos: 154 Relay_Log_File: localhost-relay-bin.000002 Relay_Log_Pos: 320 Relay_Master_Log_File: mysql-bin.000005 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 154 Relay_Log_Space: 531 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 129 Master_UUID: 278b0fa2-a25b-11e9-8ccf-000c296bc078 Master_Info_File: /home/mysql/data/master.info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: Auto_Position: 0 Replicate_Rewrite_DB: Channel_Name: Master_TLS_Version:1 row in set (0.00 sec) 出现以下内容表示slave配置成功Slave_IO_Running: YesSlave_SQL_Running: Yes5)通过主库查看从库状态 1show slave hosts; 6) 测试主从同步(创建操作在master进行)i)创建库（也可不创建,在原有的库上直接建表） 1CREATE database test_mysql; 查看已创建的库： 1show databases; ii) 创建表 12345use test_mysql;create table python_one( id int(11), name varchar(255) ); iii) 查看已创建的表 1show tables; iv) 插入数据 1INSERT INTO python_one(id) value(1); v) 查看数据 1SELECT * FROM python_one; vi) 分别在两个从库上面查看数据看是否同步成功 12use test_mysql;SELECT * FROM python_one; MySQL运维（主从切换）当MySQL主库问题时，需要进行主从切换。 1）将原有从库切换为主库 12345678910111213 [root@localhost ~]# mysql -u root -p Password: 输入密码 mysql&gt; stop slave; mysql&gt; reset master; mysql&gt; reset slave all; mysql&gt; show master status \\G*************************** 1. row *************************** File: mysql-bin.000001 Position: 154 Binlog_Do_DB: Binlog_Ignore_DB: Executed_Gtid_Set: 1 row in set (0.01 sec) 在master库创建授权账户 12GRANT REPLICATION SLAVE ON *.* TO &#x27;slave1&#x27;@&#x27;192.168.117.129&#x27; IDENTIFIED BY &#x27;slave1&#x27;;GRANT REPLICATION SLAVE ON *.* TO &#x27;slave2&#x27;@&#x27;192.168.117.131&#x27; IDENTIFIED BY &#x27;slave2&#x27;; 2）将原有主库切换为从库 123456789mysql&gt; reset master;mysql&gt; reset slave all;mysql&gt; change master to master_host=&#x27;192.168.117.130&#x27;, master_user=&#x27;root&#x27;, master_password=&#x27;123456&#x27;, master_log_file=&#x27;mysql-bin.000001&#x27;, master_log_pos=154; start slave;mysql&gt; SHOW SLAVE STATUS\\G; 3）将原有从库切换为从库 123456789mysql&gt; stop slave;mysql&gt; reset slave all;mysql&gt; change master to master_host=&#x27;192.168.117.130&#x27;, master_user=&#x27;root&#x27;, master_password=&#x27;123456&#x27;, master_log_file=&#x27;mysql-bin.000001&#x27;, master_log_pos=154; start slave;mysql&gt; SHOW SLAVE STATUS\\G; mysql密码的重置1)关闭mysql 1systemctl stop mysqld 2)修改mysql配置文件 123vim /etc/my.cnf #增加如下内容[mysqld]skip-grant-tables 作用是登录mysql的时候跳过密码验证3)启动mysql服务: 1systemctl start mysqld 4)修改密码 1234567891011[root@mytestlnx02 ~]# mysql -u root mysql&gt; use mysql; Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; update mysql.user set authentication_string=password(&#x27;root&#x27;) where user=&#x27;root&#x27;;Query OK, 1 row affected, 1 warning (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 1mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec)mysql&gt; exit 重启mysql服务先将之前加在配置文件里面的2句代码注释或删除掉，然后重启mysql服务，就可以使用刚刚设置的密码登录了。12345[root@mytestlnx02 ~]# systemctl restart mysqld[root@mytestlnx02 ~]#[root@mytestlnx02 ~]# mysql -u root -pEnter password:Welcome to the MySQL monitor. Commands end with ; or \\g. 磁盘挂载（待验证）1）查看磁盘情况 fdisk -l2）fdisk &#x2F;dev&#x2F;vdb # 对vdb硬盘进行分区3）将硬盘格式化为xfs文件系统 mkfs -t xfs &#x2F;dev&#x2F;vdb14）mkfs -t xfs &#x2F;dev&#x2F;vdb15）mkdir 挂载点6）mount &#x2F;dev&#x2F;vdb1 挂载点7）执行以下命令，查看挂载结果。df -TH8）设置启动时自动挂载磁盘 blkid &#x2F;dev&#x2F;vdb1 粘贴“&#x2F;dev&#x2F;vdb1”的UUID。9）通过vim编辑 &#x2F;etc&#x2F;fstab10）UUID&#x3D;f5c5c392-4704-4475-9abc-f6a2e049f2ea 挂载目录 xfs defaults 0 0参考文档 https://www.cnblogs.com/stulzq/p/7610100.html","tags":[]},{"title":"代码提交随笔","date":"2019-07-09T06:57:29.000Z","path":"2019/07/09/代码提交随记/","text":"git回退单个文件1）进入到文件所在文件目录，或者能找到文件的路径 1$ git log MainActivity.java 2）回退到指定的版本 1$ git reset a4e215234aa4927c85693dca7b68e9976948a35e MainActivity.java 3）提交到本地参考，注意不需要git add。 1$ git commit -m &quot;revert old file because yjl commmit have a bug&quot; 4）更新到工作目录 1$ git checkout MainActivity.java eclipse文件比较1）比较同一版本,两个文件之间的代码差异选中两个文件。右键选择Compare With ，再选择Each Other即可2）比较一个文件不同版本之间的差异选中文件–右键选择team–选择显示资源历史记录 – 然后从历史记录中选择需要比较的版本（两个文件）– 右键选择 Compare with Each Other 即可","tags":[]},{"title":"Redis安装文档","date":"2019-06-28T00:43:55.000Z","path":"2019/06/28/Redis部署文档/","text":"下载安装包1wget http://download.redis.io/releases/redis-4.0.6.tar.gz 上传并解压到&#x2F;home下1tar -xzvf redis-4.0.6.tar.gz 编译安装123cd /home/redis-4.0.6make MALLOC=libcmake install 修改redis配置文件123vim /home/redis-4.0.6/redis.conf #修改配置文件daemonize no 改为 daemonize yes #后台运行# requirepass foobared 改为 requirepass 123456 #设置redis密码 以后台进程方式启动redis启动12cd /home/redis-4.0.6/src./redis-server ../redis.conf 添加到开机自启(将redis注册成服务).)在&#x2F;etc目录下新建redis目录: 1mkdir redis ..)将&#x2F;home&#x2F;redis-4.0.6&#x2F;redis.conf文件复制一份到&#x2F;etc&#x2F;redis目录下，并命名为6379.conf 1cp /home/redis-4.0.6/redis.conf /etc/redis/6379.conf ..)将redis的启动脚本复制一份放到&#x2F;etc&#x2F;init.d目录下: 1cp /home/redis-4.0.6/utils/redis_init_script /etc/init.d/redisd …)先切换到&#x2F;etc&#x2F;init.d目录下,然后执行自启命令 1chkconfig redisd on 若出现service redisd does not support chkconfig，则证明redisd不支持chkconfig解决方法：使用vim编辑redisd文件，在第二行加入如下两行注释，保存退出 123vim /etc/init.d/redisd# chkconfig: 2345 90 10# description: Redis is a persistent key-value database 注释的意思是，redis服务必须在运行级2，3，4，5下被启动或关闭，启动的优先级是90，关闭的优先级是10。再次执行开机自启命令，成功现在可以直接已服务的形式启动和关闭redis了：service redisd start 或者 service redisd stop 可能出现的问题1）Redis服务器设置密码后，使用service redis stop 会出现以下信息： 12345678[root@localhost ~]# service redis stopStopping ...(error) NOAUTH Authentication required.Waiting for Redis to shutdown ...Waiting for Redis to shutdown ...Waiting for Redis to shutdown ...Waiting for Redis to shutdown ...Waiting for Redis to shutdown ... 此时就需要修改redisd服务脚本，加入如下所示的红色授权信息即可： 1vim /etc/init.d/redisd 2）通过redis客户端连接redis服务时,出现无法连接redis服务器的问题redis默认只能localhost登录，所以需要开启远程登录。在redis的配置文件redis.conf中,找到bind 127.0.0.1注释掉cp &#x2F;home&#x2F;redis-4.0.6&#x2F;redis.conf &#x2F;etc&#x2F;redis&#x2F;6379.conf说明注释掉bind localhost 局域网内的所有计算机都能访问。bind localhost 只能本机访问,局域网内计算机不能访问。bind 局域网IP 只能局域网内IP的机器访问, 本机无法访问。","tags":[{"name":"Redis","slug":"Redis","permalink":"https://wangdj104.github.io/tags/Redis/"}]},{"title":"Mysql部署文档","date":"2019-06-27T15:29:47.000Z","path":"2019/06/27/Mysql部署文档/","text":"安装mysql服务mysql数据库安装包下载链接：链接: https://pan.baidu.com/s/1OqSnqRXJ951V6Gn7P-IFQQ 提取码: wini参考：https://www.cnblogs.com/javabg/p/9951852.html1)上传二进制包到&#x2F;home 解压 1tar -zxvf mysql-5.7.23-linux-glibc2.12-x86_64.tar.gz 2)更改解压缩后的文件夹名称 1mv mysql-5.7.23-linux-glibc2.12-x86_64/ /home/mysql 3)创建存放数据库数据的文件夹&#x2F;home&#x2F;mysql&#x2F;data 1mkdir /home/mysql/data 4)创建mysql用户组和mysql用户 12groupadd mysqluseradd -r -g mysql mysql 5)mysql目录授权给mysql组和mysql用户 123cd /homechown -R mysql:mysql mysqlchmod -R 750 mysql 6)修改配置文件：vim &#x2F;etc&#x2F;my.cnf 1234567891011121314151617181920212223242526272829[mysqld]basedir=/home/mysqldatadir=/home/mysql/dataport = 3306socket=/tmp/mysql.socksymbolic-links=0log-error=/var/log/mysqld.logpid-file=/tmp/mysqld/mysqld.pid sql_mode=&#x27;STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION&#x27;[client]default-character-set=utf8[mysql]default-character-set=utf8[mysqld]log-bin=mysql-binbinlog-format=ROWserver_id=1max_connections=1000init_connect=&#x27;SET collation_connection = utf8_unicode_ci&#x27;init_connect=&#x27;SET NAMES utf8&#x27;character-set-server=utf8collation-server=utf8_unicode_ciskip-character-set-client-handshake 7)创建文件&#x2F;tmp&#x2F;mysql.sock：设置用户组及用户，授权 1234cd /tmptouch mysql.sockchown mysql:mysql mysql.sockchmod 755 mysql.sock 8)创建文件&#x2F;tmp&#x2F;mysqld&#x2F;mysqld.pid 1234567mkdir mysqldcd mysqldtouch mysqld.pidcd ..chown -R mysql:mysql mysqldcd mysqldchmod 755 mysqld.pid 9）创建文件&#x2F;var&#x2F;log&#x2F;mysqld.log 1234touch /var/log/mysqld.logchown -R mysql:mysql /var/log/mysqld.logcd /var/logchmod 755 mysqld.log 10)初始化和安全启动数据库 1234cd /home/mysql/bin./mysqld --user=mysql --basedir=/home/mysql \\--datadir=/home/mysql/data --initializenohup ./mysqld_safe --user=mysql &amp; 11)修改数据库初始默认密码a)默认密码在mysqld.log日志里, 找到后保存到安全的地方: 12cat /var/log/mysqld.log或 grep &#x27;temporary password&#x27; /var/log/mysqld.log b)登录mysql: 12cd /home/mysql/bin/./mysql -uroot -p 拷贝或者输入mysqld.log中获得的默认密码，即可进入mysql命令客户端c)假设密码修改为123456 1set password=password(&quot;123456&quot;); 12)设置远程登录权限（进入数据库后操作） 1grant all privileges on *.* to &#x27;root&#x27;@&#x27;%&#x27; identified by &#x27;123456&#x27;; 13)设置开机自启动.)把support-files&#x2F;mysql.server 拷贝为&#x2F;etc&#x2F;init.d&#x2F;mysql： 1cp -a /home/mysql/support-files/mysql.server /etc/init.d/mysql ..)查看是否拷贝成功 12cd /etc/init.d/ll …)查看mysql服务是否在服务配置中,若没有，则把mysql注册为开机启动的服务，然后在进行查看 123chkconfig --list mysqlchkconfig --add mysqlchkconfig --list mysql ….)验证是否注册成服务 12service mysql startservice mysql stop …..)创建快捷方式,服务启动后，直接运行mysql -u root -p即可登录，不需要进入到对应的目录。 1ln -s /home/mysql/bin/mysql /usr/bin 配置防火墙MySQL端口12firewall-cmd --add-service=mysql --permanentfirewall-cmd --reload","tags":[{"name":"mysql","slug":"mysql","permalink":"https://wangdj104.github.io/tags/mysql/"}]},{"title":"Harbor部署文档","date":"2019-06-27T12:30:05.000Z","path":"2019/06/27/Harbor部署文档/","text":"安装dockera)docker17.12.0-ce下载链接: https://pan.baidu.com/s/1oxAwdjcLxt91Fh2YdZB5dQ 提取码: 34d1包含下述相关依赖包rpm包：b)启动docker所需的docker-compose下载链接：https://pan.baidu.com/s/1WBMMLn2g15daXE0JDr0wMg 提取码: 6rz31)进入a)所下的rpm包文件夹中，执行下述命令 1rpm -ivh *.rpm --nodeps --force –nodeps就是安装时不检查依赖关系，比如你这个rpm需要A，但是你没装A，这样你的包就装不上，用了–nodeps你就能装上了。–force就是强制安装，比如你装过这个rpm的版本1，如果你想装这个rpm的版本2，就需要用–force强制安装2)修改docker-compose权限,并移动到&#x2F;usr&#x2F;bin目录下 12chmod +x docker-composemv docker-compose /usr/bin 3)在&#x2F;etc目录下创建docker文件夹，并在docker文件夹下编写daemon.json文件 12345678mkdir /etc/dockervim /etc/docker/daemon.jsoncat /etc/docker/daemon.json&#123; &quot;registry-mirrors&quot;:[&quot;https://registry.docker-cn.com&quot;], &quot;insecure-registries&quot;:[&quot;0.0.0.0/0&quot;]&#125; 4)启动docker,并设置开机自启动 12systemctl start dockersystemctl enable docker 5)验证,输入docker info,若出现下图所示,则安装成功 安装harbor安装包下载链接： https://pan.baidu.com/s/1HSgc0gaK_40LBhouERC3cg 提取码: gftp1)解压 1tar -zxvf harbor-offline-installer-v1.4.0.tgz 解压后的文件夹里包含下图所示文件2)修改harbor配置文件 1vim harbor.cfg 3)启动harbor 12./prepare 更新一下配置文件./install 4)验证查看启动的容器的健康状态： 1docker ps 本机登录harbor仓库： 1docker login harbor仓库IP harbor外挂mysql、redis数据库1)首先需要在mysql客户端新建一个registry数据库,并执行下述sql语句： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211create table access ( access_id int NOT NULL AUTO_INCREMENT, access_code char(1), comment varchar (30), primary key (access_id));insert into access (access_code, comment) values(&#x27;M&#x27;, &#x27;Management access for project&#x27;),(&#x27;R&#x27;, &#x27;Read access for project&#x27;),(&#x27;W&#x27;, &#x27;Write access for project&#x27;),(&#x27;D&#x27;, &#x27;Delete access for project&#x27;),(&#x27;S&#x27;, &#x27;Search access for project&#x27;);create table role ( role_id int NOT NULL AUTO_INCREMENT, role_mask int DEFAULT 0 NOT NULL, role_code varchar(20), name varchar (20), primary key (role_id) );insert into role (role_code, name) values (&#x27;MDRWS&#x27;, &#x27;projectAdmin&#x27;), (&#x27;RWS&#x27;, &#x27;developer&#x27;), (&#x27;RS&#x27;, &#x27;guest&#x27;);create table user ( user_id int NOT NULL AUTO_INCREMENT, username varchar(255), email varchar(255), password varchar(40) NOT NULL, realname varchar (255) NOT NULL, comment varchar (30), deleted tinyint (1) DEFAULT 0 NOT NULL, reset_uuid varchar(40) DEFAULT NULL, salt varchar(40) DEFAULT NULL, sysadmin_flag tinyint (1), creation_time timestamp NOT NULL default CURRENT_TIMESTAMP, update_time timestamp NOT NULL default CURRENT_TIMESTAMP, primary key (user_id), UNIQUE (username), UNIQUE (email) ); insert into user (username, email, password, realname, comment, deleted, sysadmin_flag, creation_time, update_time) values (&#x27;admin&#x27;, &#x27;admin@example.com&#x27;, &#x27;&#x27;, &#x27;system admin&#x27;, &#x27;admin user&#x27;,0, 1, NOW(), NOW()),(&#x27;anonymous&#x27;,&#x27;anonymous@example.com&#x27;, &#x27;&#x27;, &#x27;anonymous user&#x27;, &#x27;anonymous user&#x27;, 1, 0, NOW(), NOW());create table project ( project_id int NOT NULL AUTO_INCREMENT, owner_id int NOT NULL, name varchar (255) NOT NULL, creation_time timestamp NOT NULL default CURRENT_TIMESTAMP, update_time timestamp NOT NULL default CURRENT_TIMESTAMP, deleted tinyint (1) DEFAULT 0 NOT NULL, primary key (project_id), FOREIGN KEY (owner_id) REFERENCES user(user_id), UNIQUE (name) );insert into project (owner_id, name, creation_time, update_time) values (1, &#x27;library&#x27;, NOW(), NOW());create table project_member ( project_id int NOT NULL, user_id int NOT NULL, role int NOT NULL, creation_time timestamp NOT NULL default CURRENT_TIMESTAMP, update_time timestamp NOT NULL default CURRENT_TIMESTAMP, PRIMARY KEY (project_id, user_id), FOREIGN KEY (role) REFERENCES role(role_id), FOREIGN KEY (project_id) REFERENCES project(project_id), FOREIGN KEY (user_id) REFERENCES user(user_id));insert into project_member (project_id, user_id, role, creation_time, update_time) values(1, 1, 1, NOW(), NOW());create table project_metadata ( id int NOT NULL AUTO_INCREMENT, project_id int NOT NULL, name varchar(255) NOT NULL, value varchar(255), creation_time timestamp default CURRENT_TIMESTAMP, update_time timestamp default CURRENT_TIMESTAMP on update CURRENT_TIMESTAMP, deleted tinyint (1) DEFAULT 0 NOT NULL, PRIMARY KEY (id), CONSTRAINT unique_project_id_and_name UNIQUE (project_id,name), FOREIGN KEY (project_id) REFERENCES project(project_id));insert into project_metadata (id, project_id, name, value, creation_time, update_time, deleted) values(1, 1, &#x27;public&#x27;, &#x27;true&#x27;, NOW(), NOW(), 0);create table access_log ( log_id int NOT NULL AUTO_INCREMENT, username varchar (255) NOT NULL, project_id int NOT NULL, repo_name varchar (256), repo_tag varchar (128), GUID varchar(64), operation varchar(20) NOT NULL, op_time timestamp NOT NULL default CURRENT_TIMESTAMP, primary key (log_id), INDEX pid_optime (project_id, op_time));create table repository ( repository_id int NOT NULL AUTO_INCREMENT, name varchar(255) NOT NULL, project_id int NOT NULL, description text, pull_count int DEFAULT 0 NOT NULL, star_count int DEFAULT 0 NOT NULL, creation_time timestamp default CURRENT_TIMESTAMP, update_time timestamp default CURRENT_TIMESTAMP on update CURRENT_TIMESTAMP, primary key (repository_id), UNIQUE (name));create table replication_policy ( id int NOT NULL AUTO_INCREMENT, name varchar(256), project_id int NOT NULL, target_id int NOT NULL, enabled tinyint(1) NOT NULL DEFAULT 1, description text, deleted tinyint (1) DEFAULT 0 NOT NULL, cron_str varchar(256), filters varchar(1024), replicate_deletion tinyint (1) DEFAULT 0 NOT NULL, start_time timestamp NULL, creation_time timestamp default CURRENT_TIMESTAMP, update_time timestamp default CURRENT_TIMESTAMP on update CURRENT_TIMESTAMP, PRIMARY KEY (id));create table replication_target ( id int NOT NULL AUTO_INCREMENT, name varchar(64), url varchar(64), username varchar(255), password varchar(128), target_type tinyint(1) NOT NULL DEFAULT 0, insecure tinyint(1) NOT NULL DEFAULT 0, creation_time timestamp default CURRENT_TIMESTAMP, update_time timestamp default CURRENT_TIMESTAMP on update CURRENT_TIMESTAMP, PRIMARY KEY (id) ); create table replication_job ( id int NOT NULL AUTO_INCREMENT, status varchar(64) NOT NULL, policy_id int NOT NULL, repository varchar(256) NOT NULL, operation varchar(64) NOT NULL, tags varchar(16384), creation_time timestamp default CURRENT_TIMESTAMP, update_time timestamp default CURRENT_TIMESTAMP on update CURRENT_TIMESTAMP, PRIMARY KEY (id), INDEX policy (policy_id), INDEX poid_uptime (policy_id, update_time) );create table replication_immediate_trigger ( id int NOT NULL AUTO_INCREMENT, policy_id int NOT NULL, namespace varchar(256) NOT NULL, on_push tinyint(1) NOT NULL DEFAULT 0, on_deletion tinyint(1) NOT NULL DEFAULT 0, creation_time timestamp default CURRENT_TIMESTAMP, update_time timestamp default CURRENT_TIMESTAMP on update CURRENT_TIMESTAMP, PRIMARY KEY (id) );create table img_scan_job ( id int NOT NULL AUTO_INCREMENT, status varchar(64) NOT NULL, repository varchar(256) NOT NULL, tag varchar(128) NOT NULL, digest varchar(128), creation_time timestamp default CURRENT_TIMESTAMP, update_time timestamp default CURRENT_TIMESTAMP on update CURRENT_TIMESTAMP, PRIMARY KEY (id));create table img_scan_overview ( id int NOT NULL AUTO_INCREMENT, image_digest varchar(128) NOT NULL, scan_job_id int NOT NULL, severity int NOT NULL default 0, components_overview varchar(2048), details_key varchar(128), creation_time timestamp default CURRENT_TIMESTAMP, update_time timestamp default CURRENT_TIMESTAMP on update CURRENT_TIMESTAMP, PRIMARY KEY(id), UNIQUE(image_digest));create table clair_vuln_timestamp ( id int NOT NULL AUTO_INCREMENT, namespace varchar(128) NOT NULL, last_update timestamp NOT NULL, PRIMARY KEY(id), UNIQUE(namespace));create table properties ( id int NOT NULL AUTO_INCREMENT, k varchar(64) NOT NULL, v varchar(128) NOT NULL, PRIMARY KEY(id), UNIQUE (k));CREATE TABLE IF NOT EXISTS `alembic_version` (`version_num` varchar(32) NOT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8;insert into alembic_version values (&#x27;1.4.0&#x27;); 2)修改harbor配置文件 12vim harbor.cfg./db_host 搜索数据库配置项 3)重启harbor服务 123docker-compose down./prepare./install --ha 4)验证 harbor出现的问题（一） 当用dcoker ps查看harbor容器的状态时，发现为unheathly,此时用docker-compose down关闭出错先用systemctl restart docker 命令重启docker;若重启docker一直卡住不动，那就需要删除&#x2F;var&#x2F;lib下的docker目录（记住要先备份）;2)若执行删除命令无法删除docker的目录123[root@master shell]# rm -rf /var/lib/dockerrm: 无法删除&quot;/var/lib/docker/overlay/50f73bfd98368e3d9f47aac7d23ae12b514ade1c283b88013995b12a5f238860/merged&quot;: 设备或资源忙 3)这是因为网络共享挂载导致无法删除，首先找到挂载的位置，然后取消挂载后，就可以删除。123[root@master shell]# cat /proc/mounts |grep &quot;docker&quot;/dev/mapper/centos-root /var/lib/docker/overlay xfs rw,seclabel,relatime,attr2,inode64,noquota 0 0overlay /var/lib/docker/overlay/50f73bfd98368e3d9f47aac7d23ae12b514ade1c283b88013995b12a5f238860/merged overlay rw,seclabel,relatime,lowerdir=/var/lib/docker/overlay/a764e1d503861296ffe63f9f3e20ccd440b12abbaceb5ba49ddac8640b1aad96/root,upperdir=/var/lib/docker/overlay/50f73bfd98368e3d9f47aac7d23ae12b514ade1c283b88013995b12a5f238860/upper,workdir=/var/lib/docker/overlay/50f73bfd98368e3d9f47aac7d23ae12b514ade1c283b88013995b12a5f238860/work 0 0 4）取消挂载1umount /var/lib/docker/overlay/50f73bfd98368e3d9f47aac7d23ae12b514ade1c283b88013995b12a5f238860/merged 5）再次查看12[root@master shell]# cat /proc/mounts |grep &quot;docker&quot;/dev/mapper/centos-root /var/lib/docker/overlay xfs rw,seclabel,relatime,attr2,inode64,noquota 0 0 6）取消挂载1[root@master shell]# umount /var/lib/docker/overlay 7）现在删除一般可以正常删除。若cat &#x2F;proc&#x2F;mounts |grep “docker” 没有输出挂载信息,且提示&#x2F;var&#x2F;lib&#x2F;docker&#x2F;devicemapper不能删除，device or resource busy ,此时就需要重启机器,强制取消挂载8）重启docker1[root@master shell]# systemctl restart docker","tags":[{"name":"Harbor","slug":"Harbor","permalink":"https://wangdj104.github.io/tags/Harbor/"}]},{"title":"Gitlab安装文档","date":"2019-06-26T15:13:21.000Z","path":"2019/06/26/Gitlab部署文档/","text":"以镜像的方式部署gitlab,并外挂postgresql、redis数据库。环境为centos7(192.168.117.129),在192.168.117.129安装postgresql、redis数据库。(一)拉取镜像 1docker pull twang2218/gitlab-ce-zh:10.6.4 (二)启动在启动之前需要在postgresql中新建一个数据库gitlab-test1数据库1）以docker run 方式启动gitlab镜像,挂载的数据库,在容器的配置文件中添加: 123456789docker run -d \\ --hostname &#x27;192.168.117.129&#x27; \\ --publish 443:443 --publish 8080:80 --publish 2222:22 \\ --name dockercompose_gitlab_1 \\ --restart always \\ --volume &#x27;/gitlab/config:/etc/gitlab&#x27; \\ --volume &#x27;/gitlab/logs:/var/log/gitlab&#x27; \\ --volume &#x27;/gitlab/config/gitlab/data:/var/opt/gitlab&#x27; \\ twang2218/gitlab-ce-zh:10.6.4 2)以docker-compose 方式启动gitlab镜像: 1234567891011121314151617181920212223242526272829303132version: &#x27;2&#x27;services: gitlab: image: &#x27;twang2218/gitlab-ce-zh:10.6.4&#x27; restart: unless-stopped hostname: &#x27;192.168.117.129&#x27; environment: TZ: &#x27;Asia/Shanghai&#x27; GITLAB_OMNIBUS_CONFIG: | external_url &#x27;http://192.168.117.129&#x27; gitlab_rails[&#x27;time_zone&#x27;] = &#x27;Asia/Shanghai&#x27; gitlab_rails[&#x27;gitlab_shell_ssh_port&#x27;] = 54322 postgresql[&#x27;enable&#x27;] = false gitlab_rails[&#x27;db_adapter&#x27;] = &quot;postgresql&quot; gitlab_rails[&#x27;db_encoding&#x27;] = &quot;utf8&quot; gitlab_rails[&#x27;db_database&#x27;] = &quot;gitlab-test1&quot; gitlab_rails[&#x27;db_username&#x27;] = &quot;postgres&quot; gitlab_rails[&#x27;db_password&#x27;] = &quot;postgres&quot; gitlab_rails[&#x27;db_host&#x27;] = &quot;192.168.117.129&quot; gitlab_rails[&#x27;db_port&#x27;] = 5432 #redis[&#x27;enable&#x27;] = false #gitlab_rails[&#x27;redis_host&#x27;] = &quot;192.168.117.129&quot; #gitlab_rails[&#x27;redis_port&#x27;] = 6379 #gitlab_rails[&#x27;redis_password&#x27;] = &quot;123456&quot; ports: - &#x27;8080:80&#x27; - &#x27;441:443&#x27; - &#x27;54322:22&#x27; volumes: - &#x27;/gitlab/config:/etc/gitlab&#x27; - &#x27;/gitlab/logs:/var/log/gitlab&#x27; - &#x27;/gitlab/config/gitlab/data:/var/opt/gitlab&#x27; 以上两种启动方式,可能出现类似下面的错误： 12345678910There was an error running gitlab-ctl reconfigure:bash[migrate gitlab-rails database] (gitlab::database_migrations line 49) had an error: Mixlib::ShellOut::ShellCommandFailed: Expected process to exit with [0], but received &#x27;1&#x27;---- Begin output of &quot;bash&quot; &quot;/tmp/chef-script20190430-11860-4waqei&quot; ----STDOUT: bundler: failed to load command: rake (/opt/gitlab/embedded/bin/rake)Bundler::GemNotFound: Could not find mysql2-0.4.10 in any of the sources/opt/gitlab/embedded/lib/ruby/gems/2.4.0/gems/bundler- 1.16.2/lib/bundler/spec_set.rb:91:in `block in materialize&#x27;/opt/gitlab/embedded/lib/ruby/gems/2.4.0/gems/bundler- 1.16.2/lib/bundler/spec_set.rb:85:in `map!&#x27;STDERR:---- End output of &quot;bash&quot; &quot;/tmp/chef-script20190430-11860-4waqei&quot; ----Ran &quot;bash&quot; &quot;/tmp/chef-script20190430-11860-4waqei&quot; returned 1 解决方案：在192.168.117.129上安装postgresql的extension插件 12进入目录：/home/deployer/postgresql-10.1/contribmake &amp;&amp; make install (三)数据迁移 参考文档：https://www.cnblogs.com/wenwei-blog/p/6362829.html 1.迁移准备工作和思路:从a服务器迁移到b服务器,由于Gitlab自身的兼容性问题，高版本的Gitlab无法恢复低版本备份的数据,需要注意在b服务器部署和a服 务器一样版本的gitlab,部署好环境后开始备份和数据迁移。查看gitlab版本的命令: 1gitlab-rake gitlab:env:info(容器中运行) 2.备份原a服务器上的的数据 1gitlab-rake gitlab:backup:create RAILS_ENV=production(容器中运行) PS: 备份后的文件一般是位于&#x2F;var&#x2F;opt&#x2F;gitlab&#x2F;backups(容器中目录）下, 自动生成文件名,文件名如1481529483_gitlab_backup.tar3.将步骤2生成的tar文件拷贝到b服务器上相应的backups目录下4.在b服务器恢复数据将备份文件权限修改为777,不然可能恢复的时候会出现权限不够，不能解压的问题 12chmod 777 1502357536_2017_08_10_9.4.3_gitlab_backup.targitlab-rake gitlab:backup:restore RAILS_ENV=production BACKUP=1502357536_2017_08_10_9.4.3 PS：BACKUP的时间点必须与原服务器备份后的文件名一致 (四) gitlab升级并进行数据迁移如果要升级的gitlab和当前的gitlab不在同一台服务器上,按照3）进行数据的迁移,若在同一台服务器上，则直接按照下述命令操作即可： 修改docker-compose文件1234567891011121314151617181920212223242526272829303132version: &#x27;2&#x27;services: gitlab: image: &#x27;twang2218/gitlab-ce-zh:11.1.4&#x27; restart: unless-stopped hostname: &#x27;192.168.117.129&#x27; environment: TZ: &#x27;Asia/Shanghai&#x27; GITLAB_OMNIBUS_CONFIG: | external_url &#x27;http://192.168.117.129&#x27; gitlab_rails[&#x27;time_zone&#x27;] = &#x27;Asia/Shanghai&#x27; gitlab_rails[&#x27;gitlab_shell_ssh_port&#x27;] = 54322 postgresql[&#x27;enable&#x27;] = false gitlab_rails[&#x27;db_adapter&#x27;] = &quot;postgresql&quot; gitlab_rails[&#x27;db_encoding&#x27;] = &quot;utf8&quot; gitlab_rails[&#x27;db_database&#x27;] = &quot;gitlab-test1&quot; gitlab_rails[&#x27;db_username&#x27;] = &quot;postgres&quot; gitlab_rails[&#x27;db_password&#x27;] = &quot;postgres&quot; gitlab_rails[&#x27;db_host&#x27;] = &quot;192.168.117.129&quot; gitlab_rails[&#x27;db_port&#x27;] = 5432 #redis[&#x27;enable&#x27;] = false #gitlab_rails[&#x27;redis_host&#x27;] = &quot;192.168.117.129&quot; #gitlab_rails[&#x27;redis_port&#x27;] = 6379 #gitlab_rails[&#x27;redis_password&#x27;] = &quot;123456&quot; ports: - &#x27;8080:80&#x27; - &#x27;441:443&#x27; - &#x27;54322:22&#x27; volumes: - &#x27;/gitlab/config:/etc/gitlab&#x27; - &#x27;/gitlab/logs:/var/log/gitlab&#x27; - &#x27;/gitlab/config/gitlab/data:/var/opt/gitlab&#x27; 启动12docker-compose downdocker-compose up -d (五)调试如果容器启动不成功,可进入容器进行调试 1)查看并修改GitLab 配置。若postgresql在本机，则不设置此项，但须在数据库中新建一个数据库名为gitlabhq_production的数据库，若要外挂的postgresql数据库不在本机，则按下述步骤查看并修改相关配置: 1234567891011121314docker exec -t -i dockercompose_gitlab_1 bashvim /etc/gitlab/gitlab.rb在该文件中搜索 /db ,在这一栏修改下述语句，若果一致则不修改 gitlab_rails[&#x27;db_adapter&#x27;] = &quot;postgresql&quot; gitlab_rails[&#x27;db_encoding&#x27;] = &quot;utf8&quot; gitlab_rails[&#x27;db_database&#x27;] = &quot;gitlab-test1&quot; gitlab_rails[&#x27;db_username&#x27;] = &quot;postgres&quot; gitlab_rails[&#x27;db_password&#x27;] = &quot;postgres&quot; gitlab_rails[&#x27;db_host&#x27;] = &quot;192.168.117.129&quot; gitlab_rails[&#x27;db_port&#x27;] = 5432保存退出运行 gitlab-rake gitlab:check ,若不出错，即可进行下一步,若报错，解决问题重新配置gitlab: gitlab-ctl reconfigure 若不报错，则gitlab正常启动，并连接到postgresql数据库。注：每次删除容器，重新启动镜像时，需删除根目录下的/gitlab目录 2)如果gitlab启动的端口不是80,还需修改HTTP连接方式中的端口,该端口默认为80端口 12docker exec -t -i dockercompose_gitlab_1 bashvim /opt/gitlab/embedded/service/gitlab-rails/config/gitlab.yml 如上图所示,本文中的gitlab是以8080端口启动的，需修改为8080端口，执行下述命令 1gitlab-ctl restart","tags":[{"name":"gitlab","slug":"gitlab","permalink":"https://wangdj104.github.io/tags/gitlab/"}]},{"title":"Postgresql安装文档","date":"2019-06-26T08:47:58.000Z","path":"2019/06/26/Postgresql部署文档/","text":"PostgreSQL安装包官网下载地址：https://www.postgresql.org/ftp/source/v10.1/ (一) 安装 1）上传、解压 12将postgresql-10.1.tar.gz上传到/home下运行解压命令 tar -zxvf postgresql-10.1.tar.gz 2）检查当前系统环境能否安装PG 12 cd /home/postgresql-10.1/./configure --prefix=/usr/local/pgsql/ 若出现下述错误 1234configure: error: readline library not foundIf you have readline already installed, see config.log for details on thefailure. It is possible the compiler isn&#x27;t looking in the proper directory.Use --without-readline to disable readline support. 执行下述命令 1yum -y install -y readline-devel 若再次运行.&#x2F;configure命令，出现下述问题 1234configure: error: zlib library not foundIf you have zlib already installed, see config.log for details on thefailure. It is possible the compiler isn&#x27;t looking in the proper directory.Use --without-zlib to disable zlib support. 执行下述命令 1yum install zlib-devel 3）编译安装 1make &amp;&amp; make install 4)创建postgresql专有用户 1adduser postgres 5)创建postgresql数据目录，并赋予权限 12mkdir /usr/local/pgsql/datachown -R postgres:postgres /usr/local/pgsql/ (二)启动 1)切换用户 1su - postgres 2)进入bin目录 1cd /usr/local/pgsql/bin/ 3)初始化postgresql数据目录 1./initdb -D /usr/local/pgsql/data 4)启动数据库 1./pg_ctl start -D /usr/local/pgsql/data (三)新建数据库、登录数据库的用户和用户密码 1)创建数据库，在&#x2F;usr&#x2F;local&#x2F;pgsql&#x2F;bin&#x2F;目录下执行 1./createdb mydb 2）创建用户（如用户名为lin，密码为LinBug）有两种方式： 方式一：通过默认数据库创建 12先进入默认的 postgres 数据库: ./psql 默认数据库为postgresCREATE USER lin WITH PASSWORD &#x27;LinBug&#x27;; 方式二：直接执行命令，交互式创建 1./createuser -P lin 这会提示你输入新建用户的密码，重复输入密码后，创建成功 (四)访问数据库(以下操作确保是在&#x2F;usr&#x2F;local&#x2F;pgsql&#x2F;bin&#x2F;目录下) 12以默认用户名访问默认数据库（默认的用户名和数据库名都是postgres）：./psql以名为lin的角色登录名为mydb的数据库：./psql mydb -U lin (五)远程访问数据库设置 远程访问数据库的认证方式主要有很多方式，我只设置基于TCP&#x2F;IP连接的trust认证方式,需设置两个配置文件，(1)修改配置文件postgresql.conf 1vim /usr/local/pgsql/data/postgresql.conf 修改监听地址： 1#listen_addresses=’localhost’ 改为 listen_addresses=’*’ (2)修改配置文件pg_hba.conf： 1vim /usr/local/pgsql/data/pg_hba.conf 添加一条IP授权记录（如192.168.2.23），可以对一个网段授权 12# IPv4 myhost connections:host all all 192.168.117.0/24 trust 当然，可以设置所有网段IP可以访问： 12# IPv4 remote address connections:host all all 0.0.0.0/0 trust (六)设置postgresql数据库开机自启动 PostgreSQL的开机自启动脚本位于PostgreSQL源码目录的contrib&#x2F;start-scripts路径下，linux文件即为linux系统上的启动脚本。(1)将Linux文件复制到 &#x2F;etc&#x2F;init.d 目录下，并且将其重名为postgresql 1cp /home/postgresql-10.1/contrib/start-scripts/linux /etc/init.d/postgresql (2)进入&#x2F;etc&#x2F;init.d 目录下，修改postgresql文件 12cd /etc/init.d/vim postgresql 然后做以下修改：将prefix设置为postgresql的安装路径：&#x2F;usr&#x2F;local&#x2F;postgresql将PGDATA设置为postgresql的数据目录路径：&#x2F;usr&#x2F;local&#x2F;postgresql&#x2F;data将PGUSER设置为postgresql的用户：admin将PGLOG 设置为 postgresql 的数据目录的日志文件夹下：$PGDATA&#x2F;pg_log&#x2F;serverlog(3)添加到开机启动修改文件属性： 1chmod a+x postgresql 添加开机启动： 1chkconfig --add postgresql (4)测试postgresql的启停 1234systemctl start postgresqlsystemctl status postgresqlsystemctl stop postgresqlsystemctl status postgresql (5)远程Navicat连接测试(七)若要使用postgresql作为gitlab的外挂数据库，还需要安装postgresql的扩展插件 12进入目录：/home/deployer/postgresql-10.1/contribmake &amp;&amp; make install 注意：若出现下述问题，请检查防火墙是否开放5432端口权限 1234Unable to connect to server:could not connect to server: Connection timed out (0x0000274C/10060)Is the server running on host &quot;192.168.1.205&quot; and acceptingTCP/IP connections on port 5432","tags":[{"name":"postgresql","slug":"postgresql","permalink":"https://wangdj104.github.io/tags/postgresql/"}]},{"title":"Hello World","date":"2019-06-25T07:03:40.000Z","path":"2019/06/25/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post123456789101112部署流程：hexo cleanhexo ghexo d调试流程:hexo cleanhexo shttp://localhost:4000创建新的博客：hexo new &quot;博客名称&quot; 在\\hexo\\source\\_posts目录中生成创建新的页面：hexo new page &quot;页面名称&quot; 在\\hexo\\source目录中生成 1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[]},{"title":"在线考试系统部署文档","date":"2019-06-25T07:03:40.000Z","path":"2019/06/25/在线考试系统部署文档/","text":"主要介绍如何在本地运行项目，包括下载、导入、修改配置、运行项目 环境准备开发环境： jdk：1.8 MySQL：5.7+ redis rabbitMq node.js consul fastDfs（推荐基于docker安装，安装步骤网上很多） 开发工具：IntelliJ IDEA、WebStorm 项目下载 git clone下载spring-microservice-exam、spring-microservice-exam-ui、spring-microservice-exam-web：123`git clone https://gitee.com/wells2333/spring-microservice-exam.git``git clone https://gitee.com/wells2333/spring-microservice-exam-ui.git``git clone https://gitee.com/wells2333/spring-microservice-exam-web.git` 修改配置 修改spring-microservice-exam的config-service的&#x2F;config文件夹下各服务的配置，主要是数据库、redis和fastDfs，其它配置基本不用动 redis的IP和端口号: fastDfs的IP和端口号： 运行数据库初始化脚本：123`/doc/deploy/mysql/microservice-user.sql``/doc/deploy/mysql/microservice-exam.sql``/doc/deploy/mysql/microservice-auth.sql` 启动后端项目启动项目前要先确认consul、MySQL、redis、rabbitMq是否已经启动 按顺序启动： 1234561. config-service2. auth-service3. user-service4. exam-service5. gateway-service6. monitor-service 需要监控功能再启动： 内存不足的可以限制每个服务的内存：config-service可以分配64M（-Xmx64m -Xms64m）、其它服务分配128M（-Xmx128m -Xms128m） 启动前端项目分别在spring-microservice-exam-ui、spring-microservice-exam-web目录下，命令行运行： 12npm installnpm run dev WebStorm导入项目： 运行： spring-microservice-exam-web项目WebStorm导入操作类似 启动成功后访问： 前台：localhost:8080 后台：localhost:9527 默认账号： 管理员：admin&#x2F;123456 学生：student&#x2F;123456 教师：teacher&#x2F;123456 监控 名称 地址 rabbitMq监控 localhost:15672 spring boot admin服务监控 localhost:8085 zipkin链路跟踪 localhost:9411 consul localhost:8500","tags":[{"name":"microservice","slug":"microservice","permalink":"https://wangdj104.github.io/tags/microservice/"}]}]